{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABC\n",
    "import numpy as np\n",
    "from Models import LinearModel, SIRModel\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "def print_results(results):\n",
    "    used=[]\n",
    "    for x in returned_stats:\n",
    "        if x not in used:\n",
    "            used.append(x)\n",
    "            print(\"([{}],{})\".format(\",\".join([str(y) for y in x]),returned_stats.count(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics and ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary statistic $s(\\cdot)$ is a projection of high-dimensional data to a lower dimension space.\n",
    "$$ s:\\mathbb{R}^m\\to\\mathbb{R}^p\\text{ with }m>p $$\n",
    "Sets of summary statistics $S=\\{s_1,\\dots,s_K\\}$ are often applied to a data set, with each one capturing a different piece of information.\n",
    "$$ S:\\mathbb{R}^m\\to\\mathbb{R}^{\\sum p_i}\\text{ where }s_i:\\mathbb{R}^m\\to\\mathbb{R}^{p_i}\\text{ with }m>\\sum p_i $$\n",
    "It is common for datasets to be better represented by a matrix $\\mathbb{R}^{n\\times m}$ as they involve many observations, and each observation is multi-dimensional. In this case we want our summary statistics to map to dimensions $\\{p_1,\\dots,p_K\\}$ st $n\\times m\\ll \\sum p_i$.\n",
    "\n",
    "Ideally, summary statistics are able to compress a dataset $D$ without compromising the information held within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    " * [Motivation](#Motivation)\n",
    "   * [Motivating Example](#motivating_example)\n",
    " * [Summary Statistics and ABC](#Summary-Statistics-in-ABC)\n",
    " * [Common Summary Statistics](#Common-Summary-Statistics)\n",
    " * [Theoretically Optimal Summary Statistics](#Theoretically-Optimal-Summary-Statistics)\n",
    " * [Properties of Summary Statistics](#Properties-of-Summary-Statistics)\n",
    "   * [Sufficiency](#Sufficiency)\n",
    "   * [Approximation Sufficiency](#Approximate-Sufficiency)\n",
    "     * [Demonstration](#joyce_marjoram_demonstation)\n",
    " * [Summary Statistic Selection](#Summary-Statistic-Selection)\n",
    "     * [Joyce & Marjoram](#Joyce-&-Marjoram)\n",
    "     * [Minimising Entropy](#Minimising-Entropy)\n",
    "     * [Two-Step Minimising Entropy](#Two-Step-Minimising-Entropy)\n",
    "     * [(Semi-) Automatic ABC](#(Semi-)-Automatic-ABC)\n",
    "     * [Non-Linear Projections](#Non-Linear-Projections)\n",
    "     * [Neural Networks](#Neural-Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In recent years there has been an explosion in the amount of data available. This is due to reduction in costs of computational power and storage, commercial interests, changes in societal opinion towards data collection and many other factors.\n",
    "\n",
    "Larger datasets mean more computational power is required to run algorithms on them. Thus, being able to reduce the size of a dataset whilst still retaining most (if not all) of the meaningful information means the same analysis can be performed but require less resources.\n",
    "\n",
    "The study of summarising large, complex data sets is a problem of interest in many different fields.\n",
    "\n",
    "Approximate Bayesian Computation (ABC) methods are a class of algorithms for inferring model parameters given some observed data. ABC methods are computationally inefficient as they rely on simulations, thus they often require the use of summary statistics to be computationally tractable.\n",
    "\n",
    "<a id=\"motivating_example\"></a>\n",
    "**Motivating Example**\n",
    "\n",
    "Suppose you are given a dataset for the results of flipping a coin $X=\\{H,H,T,T,H,T,T,T,T,T\\}$ and you are tasked to determine whether the coin is biased or not. You may consider using two summary statistics:\n",
    " * The proportion of times heads occurs. $s_1(X)=3/10$.\n",
    " * The proportion of times tails occurs. $s_2(X)=7/10$.\n",
    "\n",
    "Now you analysis only involves two data points $(s_1(X),s_2(X))$ rather than $|X|=10$. This approach also has the advantage the number of data points being analysed is fixed, whereas the size of the dataset $X$ could be variable.\n",
    "\n",
    "This is obviously a very trivial example but it is easy to see how it can be extended to larger $X$s or multiple trials.\n",
    "\n",
    "(TODO - use how RONA data is reported to demonstrate why different summary statistics are important for different problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics in ABC\n",
    "See [ABC_Notebook](ABC_Notebook.ipynb) for details on ABC methods and its implementation.\n",
    "\n",
    "Approximate Bayesian Computation (ABC) methods are a class of algorithms for inferring model parameters given some observed data. ABC methods are computationally inefficient as they rely on simulations, thus they often require the use of summary statistics to be computationally tractable.\n",
    "\n",
    "ABC methods infer model parameters by generating an approximation of the posterior for each parameter, given some observed data from the true model. The better the approximation, the better the resulting model fit should be. Summary statistics influence the quality of this approximation as reducing dimensionality of data will always worsen the approximation. Our task is to choose summary statistics such that this loss of information (or at least make worthwhile improvements to the computational effiency).\n",
    "\n",
    "ABC methods involve comparing simulated data to observed data and determining how close the two sets are to each other. When these datasets have high-dimensionality it is less likely that they coincide due to the *curse of dimensionality*. Thus transforming these datasets with summary statistics makes it easier to compare them and should reduce the number of simulations required (or improve the quality of simulations kept).\n",
    "\n",
    "Greater reduction in dimensionality means greater reduction in computation requirements but also greater level of approximation, likely meaning the produced posterior is less representative.\n",
    "\n",
    "In ABC there are two sources of approximation-error: only requiring simulated data to be \"similar to\" rather than \"equal to\" the observed data; and comparing summaries of the simulated $S(X)$ and observed data $S(X_{obs})$, rather than comparing the raw simulated $X$ and observed data $X_{obs}$. An effective summary statistic should balance the trade-off of these two sources of error.\n",
    "\n",
    "Theoretical results show that the optimal summary statistics are the posterior means for each parameter. These cannot be calculated directly but can be estimated through simulation.\n",
    "\n",
    "In practice summary statistics often introduce unquantifiable bias (except by numerically comparing summary stats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Summary Statistics\n",
    "Most good summary statistics consider the following properties\n",
    " * Centre of the data (e.g. mean, median).\n",
    " * Spread of the data (e.g. variance).\n",
    " * Shape of the data (e.g. skew).\n",
    " * Interdependence of data-fields (e.g. correlation).\n",
    " * Quantiles of the data\n",
    "The reasons for this is explained below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretically Optimal Summary Statistics\n",
    "\n",
    "Which summary statistic is best for determining the centre of a posterior distribution depends on the loss measure being used\n",
    " * Quadratic Loss = Mean\n",
    " * Absolute Loss = Median\n",
    " * All-or-Nothing (identity) Loss = Mode\n",
    " \n",
    "**Proof** - *Mean minimises quadratic loss*\n",
    "\n",
    "Define $L(\\mathbf{y},x):=\\sum_{i=1}^n(y_i-x)^2$ to be the quadratic loss between elements of vector $\\mathbf{y}$ (some observed data) and target $x$. This definition can be expanded\n",
    "$$\\begin{array}{rcl}\n",
    "    L(\\mathbf{y},x)&=&\\sum_{i=1}^n(y_i-x)^2\\\\\n",
    "    &=&\\sum_{i=1}^n(y_i^2-2y_ix+x^2)\\\\\n",
    "    &=&\\sum_{i=1}^ny_i-2x\\sum_{i=1}^ny_i+\\sum_{i=1}^nx^2\\\\\n",
    "    &=&\\sum_{i=1}^ny_i-2x\\sum_{i=1}^ny_i+nx^2\n",
    "\\end{array}$$\n",
    "We want to find the value of $x$ which minimises of this expression. Consider the first & second derivatives wrt $x$\n",
    "$$\\begin{array}{rcl}\n",
    "    L'(\\mathbf{y},\\hat{x})&=&2n\\hat{x}-2\\sum_{i=1}^ny_i\\\\\n",
    "    L''(\\mathbf{y},\\hat{x})&=&2n\n",
    "\\end{array}$$\n",
    "Since $n\\in\\mathbb{N}$ then $2n>0$ and any solution to $L'=0$ will be a minimum.\n",
    "\n",
    "Setting $L'$ to zero and rearranging to find $\\hat{x}$ gives\n",
    "$$ \\hat{x}=\\frac1n\\sum_{i=1}^ny_i=\\bar{\\mathbf{y}} $$\n",
    "This shows that quadratic loss is minimised when the target value is the mean of the \"observed\" data set.\n",
    "\n",
    "*Comment*\n",
    "\n",
    "For some models using logarithms makes sense (namely, epidemic models). Logarithms can be incorporated into the summary statistic or the distance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Efficiency\n",
    "\n",
    "Generally, the performance of ABC-methods increases the more simulations they perform. Making implementations more computationally efficient decreases the time taken for each simulation and thus increases the number of simulations which can be completed in a fixed time-period (In practice, we specify how long an implementation runs for, not how many cycles). Choosing more computationally efficienct summary statistics will help with this.\n",
    "\n",
    "For a model which outputs $n\\times m$ data (ie $n$ readings each with $m$ features), most summary statistics have $O(mn)$ time-complexity theoretically (when being calculated for all features). However, in practice, there are intrinsic differences in the composition of each operation which affect their execution time:\n",
    " * Some summary stats require arithmetic computations (e.g. mean) whilst other comparision computations (e.g. min, max, median). \n",
    " * Some summary stats require the computation of other summary statistics first (e.g. variance and pearson correlation require mean; Pearson's skewness requires variance and mean).\n",
    "\n",
    "Some summary stats compare features and thus produce more than $m$ values (when applied to most combinations). For example, correlation between all features produces ${m\\choose2}$ values and thus runs in $O({m\\choose 2}n)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficiency\n",
    "A statistic is said to be \"sufficient\" for a model if no other statistic provides more information about the model's parameters. This idea can be extended to sets of statistics (i.e. summary statistics).\n",
    "\n",
    "Formally a set of statistics $S(\\cdot)=(s_1(\\cdot),\\dots,s_K(\\cdot))$ are said to be sufficient for the data $X\\sim f(\\cdot;\\theta)$ if\n",
    "\n",
    "$$ \\mathbb{P}(X|S(X))=\\mathbb{P}(X|S(X),\\theta)) $$\n",
    "\n",
    "This demonstrates that the summary statistics $S(X)$ capture all the information the data $X$ holds about the model parameters $\\theta$. The summary statistics $S(\\cdot)$ are a loss-less compresion of the data. (The likelihood of data $X$ given the summary statistic values $S(X)$ is independent of the model parameters $\\theta$).\n",
    "\n",
    "It is worth noting that if a set of statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ are sufficient for $X$ then any super-set $S\\supseteq S_{1:K-1}$ is also sufficient for $X$. This means that not only do we want to find a sufficient set of statistics, but also the smallest set (Minimum cardinality). Having a smaller set of statistics is desirable due to the computational efficiencies. (TODO proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Sufficiency\n",
    "In practice sufficient summary statistics do not always exist. Moreover, the Pitman–Koopman–Darmois Theorem states that sufficient statistics (whose dimension does not increase as sample size increase) only exist for models in the exponential family.\n",
    "\n",
    "<!-- The concept of approximate sufficiency is a generalisation of sufficiency which can be applied to any statistical model and involves finding sets of statistics which capture a large amount of information from the sample. -->\n",
    "\n",
    "Joyce & Marjoram introduce an approach to finding approximately sufficient statistics by building up a subset of statistics from a larger set. A statistic $S_K$ is added to the set $S_{1:K-1}=\\{S_1,\\dots,S_{K-1}\\}$  if it is deemed to significantly increase the information extracted from the data $X$.\n",
    "\n",
    "**Score $\\delta_{K}$**\n",
    "\n",
    "To determine the information gain from adding $S_K$ to the set $S$, first consider the following result from the product rule\n",
    "\n",
    "$$\\begin{array}{rrcl}\n",
    "&\\mathbb{P}(S_{1:K}(X)|\\theta)&=&\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\mathbb{P}(S_{1:K-1}|\\theta)\\\\\n",
    "\\implies&\\ln\\left\\{\\mathbb{P}(S_{1:K}(X)|\\theta)\\right\\}&=&\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}+\\ln\\left\\{\\mathbb{P}(S_{1:K-1}|\\theta)\\right\\}\\\\\n",
    "\\implies&\\ln\\left\\{\\mathbb{P}(S_{1:K}|\\theta)\\right\\}-\\ln\\left\\{\\mathbb{P}(S_{1:K-1}|\\theta)\\right\\}&=&\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}\n",
    "\\end{array}$$\n",
    "\n",
    "This shows that $\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}$ quantifies the information gained by adding $S_K$ to $S_{1:K-1}$. Thus, we want to find statistcs $S_K$ which maximise this quantity.\n",
    "\n",
    "Joyrce & Marjoram propose the *Score* metric $\\delta_{K}$, defined as\n",
    "$$ \\delta_{K}:=\\sup_\\theta\\left\\{\\ln\\left\\{\\mathbb{P}(S_K|S_{1:K-1},\\theta)\\right\\}\\right\\}-\\inf_\\theta\\left\\{\\ln\\left\\{\\mathbb{P}(S_K|S_{1:K-1},\\theta)\\right\\}\\right\\} $$\n",
    "\n",
    "**$\\varepsilon$-Approximate Sufficiency**\n",
    "\n",
    "A set of statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ is said to be $\\varepsilon$-Sufficient relative to a statistic $S_K$ if the score of $S_K$ relative to the set $S_{1:K-1}$ is no-greater than $\\varepsilon$.\n",
    "$$\\delta_K\\leq\\varepsilon$$\n",
    "Note that this is equivalent to the definition of sufficiency when $S_K=X$ (identity function) and $\\varepsilon=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistic Selection\n",
    "The most naive way to assess summary statistics is to visually inspect the plots when they are used.\n",
    "\n",
    "### Joyce & Marjoram\n",
    "Here are details on an approach, proposed by Joyce & Marjoram, to selecting summary statistics which uses the idea of approximate sufficiency.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Consider a large set of statistics $S:=\\{S_1,\\dots,S_N\\}$ and fix some value for $\\varepsilon\\in\\mathbb{R}^{\\geq0}$. Here is the algorithm proposed by Joyce & Marjoram for finding an $\\varepsilon$-sufficient subset $S^*$ of $S$, relative to the data set $X$.\n",
    "<ol>\n",
    "    <li>Define $S'=\\emptyset$</li>\n",
    "    <li>Calculate the score for statistic in $S_i\\in (S\\setminus S')$. See <a href=\"#approximating_score\">below</a>.</li>\n",
    "    <li>Identify the greatest score $\\delta_{max}:=\\max_{S_i\\in S}\\text{Score}(S_i,S')$.</li>\n",
    "    <li>Identify a statistic which attains the greatest score $S_{max}\\in\\text{argmax}_{S_i\\in S}\\text{Score}(S_i,S')$.</li>\n",
    "    <li>If $\\delta_\\text{max}>\\varepsilon$ then add $S_{max}$ to $S'$ and return to step 2. Otherwise, stop the algorithm and return $S'$</li>\n",
    "    $$S'=S'\\cup S_{max}$$\n",
    "    <li>When adding a new statistic, you should consider removing each summary statistic which has already been accepted so as to achieve the smallest set which is accepted.</li>\n",
    "</ol>\n",
    "\n",
    "**Variations**\n",
    "\n",
    "This algorithm is deterministic when a deterministic method is used in step 4. However it runs in $O(N^K)$ time when $K$ summary statistics are chosen. There are variations on this algorithm which aim to improve the run-time, inc.\n",
    " * Calculate the score for each statistic in $S$ sequentially and if one below $\\varepsilon$ then it is added to $S'$. (Rather than calculate score for all and chosing the maximum).\n",
    " * Remove $\\varepsilon$ and instead find the best subset of size $K$.\n",
    "\n",
    "**Implementation**\n",
    "<a id=\"approximating_score\"></a>\n",
    "\n",
    "The main difficulty in implementing the algorithm above is in calculating the score $\\delta_K$ for each statistic. Due to the nature of the problems which we are finding summary statistics for, we can only ever approximate the score $\\delta_K$.\n",
    "\n",
    "Joyce & Majoram replace the task of empirically estimating the score with the equivalent task of determining whether the odds-ratio differs from 1 more than would be expected from random noise.\n",
    "\n",
    "Suppose you have a set of already accepted statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ and are considering adding statistic $S_K$. Suppose a large set of parameters $\\Theta$ have been generated and let $\\Theta_{1:K-1},\\ \\Theta_{1:K}$ be the set of parameters where accepted when using $S_{1:K-1}$ and $S_{1:K}$ respectively.\n",
    "\n",
    "Define $N_{K-1}:=|\\Theta_{1:K-1}|,N_K:=|\\Theta_{1:K}|$\n",
    "\n",
    "<ol>\n",
    "    <li>Discretise the sets of accepted parameter $\\Theta_{1:K-1},\\Theta_{1:K}$ into $M$ bins. Let $C_{1:K-1},C_{1:K}$ be these two sets of counts.</li>\n",
    "    <li>Calculate the expected count for each bin $E=(N_K/N_{K-1})\\cdot C_{1:K-1}$. </li>\n",
    "    <li>Calculate the standard-variance for each bin $$sd=\\sqrt{E\\cdot\\frac{N_{K-1}-C_{1:K-1}}{N_{K-1}}}$$. </li>\n",
    "    <li>If any values in the counts from the proposed set $C_{1:K}$ is more than four standard deviations away from the expected counts then return that the proposed distribution is significantly different from the current (ie accepted $S_k$). Otherwise, accept don't accept the change in summary stats.</li>\n",
    "</ol>\n",
    "\n",
    "A big limitation of this approach is you can only determine whether two sets of summary stats produce notable different posteriors, and not whether one posterior is better than the other. This symmetry is an issue as it means it is as likely to replace a bad choice with a good choice as it is to replace a good choice with a bad choice.\n",
    "\n",
    "Moreover, only being able to compare and not rank sets of statistics means the algorithm above needs to be largely reworked in practice.\n",
    "\n",
    "**Demonstration**<a id=\"joyce_marjoram_demonstation\"></a>\n",
    "\n",
    "I have implemented the algorithm above as `ABC.joyce_marjoram()`. Here I demonstrate its usage by applying it to choosing a set of approximately sufficient summary statistics for a linear model $f(X)=1+10X+\\varepsilon$ where $\\varepsilon\\sim\\text{Normal}(0,30)$ and the start point is known. The only parameter which needs to be learnt is the gradient of the model. The mean difference gradient between consecutive data-points is a good estimator of the gradient of the model.\n",
    "\n",
    "I supply the algorithm with the following three summary statistics\n",
    " * $S_0$ - Mean gradient.\n",
    " * $S_1$ - $U\\sim\\text{Uniform}[0,6]$. This range was chosen so the variance is similar to $S_0$.\n",
    " * $S_2$ - $S_1\\cdot X$ where $X\\sim\\text{Uniform}[0,2]$\n",
    "\n",
    "I ran the algorithm 25 times, each time generating 10,000 samples using a uniform acceptance-kernel with bandwidth 1 and discretising the posterior into 10 bins. The results can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "lm=LinearModel(  # 1+10x\n",
    "    n_params=2,\n",
    "    params=[1,10],\n",
    "    n_vars=1,\n",
    "    n_obs=10,\n",
    "    x_obs=[[x] for x in range(10)],\n",
    "    noise=0\n",
    "    )\n",
    "lm.plot_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define summary statistics\n",
    "# Linear Model with known start point\n",
    "mean_grad = (lambda ys:[np.mean([ys[i+1][0]-ys[i][0] for i in range(len(ys)-1)])])\n",
    "rand=(lambda ys:[stats.uniform(0,6).rvs(1)[0]]) # variance set st similar number of samples accepted as mean_grad\n",
    "rand_grad = (lambda ys:[mean_grad(ys)[0]*stats.uniform(0,2).rvs(1)[0]])\n",
    "summary_stats=[mean_grad,rand,rand_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [[1], [1], [0], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0]]\n",
      "\n",
      "Results\n",
      "([1],11)\n",
      "([0],14)\n",
      "Wall time: 36min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Choose summary statistics for Linear Model with known start point\n",
    "param_bounds=[(1,1),(8,14)]\n",
    "lm_priors_intersect_known=[stats.uniform(1,0),stats.uniform(8,6)]\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats=ABC.joyce_marjoram(summary_stats,n_obs=10,y_obs=lm.observe(),fitting_model=lm.copy([1,1]),priors=lm_priors_intersect_known,param_bounds=param_bounds,n_samples=10000,n_bins=10,printing=False)\n",
    "    returned_stats.append(best_stats)\n",
    "\n",
    "print(\"\\n\\nResults\")\n",
    "print_results(returned_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "This implementation settles on the set $\\{S_0\\}$ in most cases, which is the best set of statistics from those provided. It is dissapoint that it picks the completely random noise $S_1$ on five occassions but this is likely due to the setup of the acceptance kernel so could be reduced with tuning (or more samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising Entropy\n",
    "\n",
    "In *On Optimal Selection of Summary Statisticsfor Approximate Bayesian Computation* Nunes and Balding present an algorithm which seeks to find summary statistics which minimise entropy of the posterior.\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "The Entropy $H(\\cdot)$ of a probability distribution $X$ is a measure of the information in that distribution. Moreover, it is an average of the inherent information each outcome from the distribution carries about the values of the paramaters of the distribution.\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "\\text{Discrete Distribution}&H(X)&:=&-\\sum_{x\\in\\mathcal{X}}\\mathbb{P}(X=x)\\cdot\\ln(\\mathbb{P}(X=x))\\\\\n",
    "\\text{Continuous Distribution}&H(X)&:=&-\\int_{\\mathcal{X}}f_X(x)\\ln(f_X(x))dx\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Estimating Entropy**\n",
    "\n",
    "There are several methods for estimating entropy, Nunes and Balding use the *$k^{th}$ Nearest-Neighbour Estimator of Entropy* as it performs well with multi-modal posteriors and is unbiased. They set $k=4$ (suggested by Singh et al. (2003)) due to its \"error properties\". Often the posterior is discretised to make calculation quicker.\n",
    "\n",
    "$$\\hat{H}=\\ln\\left(\\frac{\\pi^{p/2}}{\\Gamma(1+\\frac{p}2)}\\right)-\\frac{\\Gamma'(k)}{\\Gamma(k)}+\\ln(n)+\\frac{p}n\\sum_{i=1}^n\\ln R_{i,k}$$\n",
    "where\n",
    " * $p=|\\theta|$ - The number of parameters.\n",
    " * $R_{i,k}$ - The euclidean distance between the $i^{th}$ accepted parameter value and its $k^{th}$ nearest neighbour in the set of accepted parameters.\n",
    " * $n$ - Number of accepted parameters.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "A lower entropy value implies greater information per observation. In ABC we want to maximise the information gained per sample, thus minimising the entropy of the generated posterior is a worthwhile goal. This is the gist of the algorithm proposed by Nunes and Balding\n",
    "<ol>\n",
    "    <li>Define a large set of summary statistics $S$.</li>\n",
    "    <li>For each possible subset $S'\\subseteq S$: </li>\n",
    "    <ol>\n",
    "        <li>Run ABC-Rejection using $S'$.</li>\n",
    "        <li>Estimate the entropy of the generated posterior</li>\n",
    "    </ol>\n",
    "    <li>Return the set of statistics with the lowest estimated entropy.</li>\n",
    "</ol>\n",
    "\n",
    "**Extensions**\n",
    "\n",
    "It is significantly easier to use the \"best-samples\" version of ABC-Rejection so the results are not dependend on the choice of kernel and bandwidth.\n",
    "\n",
    "There are $2^{|S|}$ unique subsets of $S$, thus running the algorithm for all of these is inefficient (especially when $S$ has large cardinality). The simplest way to tackle this is only to consider subsets with cardinality less than some $n$. More compelx approaches involve use results from small subsets to determine whether to evaluate large subsets (pruning).\n",
    "\n",
    "This approach only uses the entropy score $\\hat{H}$ to find the best set of summary stats. Extensions could consider using it to rank them (or produce a probability distribution of them) and then a mixtures-model could be fitted with the summary statistics being chosen according to this ranking/distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_grad = (lambda ys:[np.mean([ys[i+1][0]-ys[i][0] for i in range(len(ys)-1)])])\n",
    "rand=(lambda ys:[stats.uniform(0,6).rvs(1)[0]]) # variance set st similar number of samples accepted as mean_grad\n",
    "rand_grad = (lambda ys:[mean_grad(ys)[0]*stats.uniform(0,2).rvs(1)[0]])\n",
    "summary_stats=[mean_grad,rand,rand_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_priors=[stats.uniform(0,6),stats.uniform(8,6)]\n",
    "lm_priors_intersect_known=[stats.uniform(1,0),stats.uniform(8,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "\n",
      "Results\n",
      "([0],25)\n",
      "Wall time: 23min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats,_=ABC.minimum_entropy(summary_stats=summary_stats,n_obs=10,y_obs=lm.observe(),fitting_model=lm.copy([1,1]),priors=lm_priors_intersect_known,printing=False)\n",
    "    returned_stats.append(best_stats)\n",
    "\n",
    "print(\"\\n\\nResults\")\n",
    "print_results(returned_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "Selects the best set of summary statistics in every scenario. Better than Joyce-Marjoram method of approximate sufficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [(0,), (0, 3, 4), (0, 3, 4), (0, 3, 4), (3, 4), (4,), (4,), (3, 4), (0,), (0, 3), (0, 3, 4), (0, 3), (0, 3), (4,), (0, 4), (0, 3), (0, 4), (0, 4), (0, 3), (0, 3), (0, 3, 4), (4,), (4,), (0, 3)]\n",
      "\n",
      "Results\n",
      "([0],2)\n",
      "([0,3,4],5)\n",
      "([3,4],2)\n",
      "([4],5)\n",
      "([0,3],7)\n",
      "([0,4],3)\n",
      "([3],1)\n",
      "Wall time: 1h 46min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# More summary stats\n",
    "mean_grad = (lambda ys:[np.mean([ys[i+1][0]-ys[i][0] for i in range(len(ys)-1)])])\n",
    "rand=(lambda ys:[stats.uniform(0,6).rvs(1)[0]]) # variance set st similar number of samples accepted as mean_grad\n",
    "first=(lambda ys:ys[0])\n",
    "first_last=(lambda ys:[ys[0][0],ys[-1][0]])\n",
    "summary_stats=[mean_grad,rand,rand_grad,first,first_last]\n",
    "\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats,_=ABC.minimum_entropy(summary_stats=summary_stats,n_obs=10,y_obs=lm.observe(),fitting_model=lm.copy([1,1]),priors=lm_priors,printing=False)\n",
    "    returned_stats.append(best_stats)\n",
    "\n",
    "print(\"\\n\\nResults\")\n",
    "print_results(returned_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "| `mean_grad` | `rand` | `rand_grad` | `first` | `first_last` |\n",
    "|-------------|--------|-------------|---------|--------------|\n",
    "| 68% | 0% | 0% | 60% | 60% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [(0, 3, 4), (0, 1, 2, 3), (1,), (0, 3), (1,), (0, 1, 2, 3), (0, 1, 2, 4), (0, 1, 2, 3), (0, 2, 3), (3,), (0, 1, 2), (0, 1, 3), (1, 2, 3, 4), (0, 1, 4), (0, 3), (1, 3, 4), (1, 3, 4), (0, 3), (2, 3, 4), (1, 3), (0, 1, 3, 4), (0, 2, 3), (1,), (0, 1, 4)]\n",
      "\n",
      "Results\n",
      "([0,3,4],1)\n",
      "([0,1,2,3],3)\n",
      "([1],4)\n",
      "([0,3],3)\n",
      "([0,1,2,4],1)\n",
      "([0,2,3],2)\n",
      "([3],1)\n",
      "([0,1,2],1)\n",
      "([0,1,3],1)\n",
      "([1,2,3,4],1)\n",
      "([0,1,4],2)\n",
      "([1,3,4],2)\n",
      "([2,3,4],1)\n",
      "([1,3],1)\n",
      "([0,1,3,4],1)\n",
      "Wall time: 2h 55min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SIR model\n",
    "sir_model=SIRModel(\n",
    "    params=[100000,100,1,.5],\n",
    "    n_obs=30,\n",
    "    x_obs=[[x] for x in range(30)],)\n",
    "sir_priors=[stats.uniform(100000,0),stats.uniform(100,0),stats.uniform(0,1.5),stats.uniform(0,2)]\n",
    "\n",
    "suscept_min_ss=(lambda ys:[ys[-1][0]])\n",
    "removed_peak_ss=(lambda ys:[ys[-1][2]])\n",
    "peak_infections_date_ss=(lambda ys:[1000*ys.index(max(ys,key=lambda y:y[1]))])\n",
    "peak_infections_value_ss=(lambda ys:[max(ys,key=lambda y:y[1])[1]])\n",
    "rand=(lambda ys:[stats.uniform(0,900).rvs(1)[0]])\n",
    "summary_stats=[removed_peak_ss,suscept_min_ss,peak_infections_date_ss,peak_infections_value_ss,rand]\n",
    "\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats,_=ABC.minimum_entropy(summary_stats=summary_stats,n_obs=30,y_obs=sir_model.observe(),fitting_model=sir_model.copy([1,1,1,1]),priors=sir_priors,n_samples=10000,n_accept=1000,printing=False)\n",
    "    returned_stats.append(best_stats)\n",
    "\n",
    "print(\"\\n\\nResults\")\n",
    "print_results(returned_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "| `removed_peak_ss` | `suscept_min_ss` | `peak_infections_date_ss` | `peak_infections_value_ss` | `rand` |\n",
    "|-------------------|------------------|---------------------------|----------------------------|--------|\n",
    "| 60% | 68%| 32% | 64% | 16% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Step Minimising Entropy\n",
    "\n",
    "Nunes and Balding suggest a second approach to summary stastistic selection which extends upon their minimum entropy approach. This approach attempts to increase the pool of observed data by using $S_{ME}$ to find well fitting parameter values and treating these as if they were true. Their approach then runs ABC-Rejection Sampling with each subset of statistic, again, but now uses the [MRSS](#mrss) measure to compare each subset.\n",
    "\n",
    "**RSS**\n",
    "\n",
    "*Residual Sum of Squares (RSS)* is a measure of fit between a set of sequences $\\{\\theta_{sim,1},\\dots,\\theta_{sim,n}\\}$ and some target sequence $\\theta_{obs}$. Smaller RSS values indicate a tighter fit.\n",
    "$$ RSS(\\pmb\\theta_{sim},\\theta_{obs}):=\\sqrt{\\frac1n\\sum_{i=1}^n||\\theta_{sim,i}-\\theta_{obs}\\|} $$\n",
    "<a id=\"mrss\"></a>\n",
    "\n",
    "RSS can be extended so different components of each sequence are weighted differently (I don't bother with that).\n",
    "\n",
    "*Mean Residual Sum of Squares (MRSS)* is an extension of RSS which measures how similar two sets of sequences $\\{\\theta_{sim,1},\\dots,\\theta_{sim,n}\\}$ \\& $\\{\\theta_{obs_1,},\\dots,\\theta_{obs,m}\\}$ are to each other. Smaller RSS values indicate a tighter fit.\n",
    "$$ MRSS(\\pmb\\theta_{sim},\\pmb\\theta_{obs}):=\\frac1m\\sum_{j=1}^mRSS(\\pmb\\theta_{sim},\\theta_{obs,j}) $$\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "<ol>\n",
    "    <li>Use the <a url=\"#minimising-entropy\">Minimising Entropy Algorithm</a> to find the set of summary statistics $S_{ME}$.</li>\n",
    "    <li>Of the $n_{acc}$ parameter sets accepted during the ABC-Rejection Sampling step for $S_{ME}$ identify the $n_{best}$ of these which produce the best fitting outputs. Denote this set as $\\{\\theta_{obs,1},\\dots,\\theta_{obs,n_{best}}\\}$.</li>\n",
    "    <li>For each subset $S'$ of $S$:</li>\n",
    "    <ol>\n",
    "        <li>Run ABC-Rejection Sampling (Best Samples approach) using $S'$ and return the accepted parameter values $\\{\\theta_{acc,i},\\dots,\\theta_{acc,n_{acc}}\\}$.</li>\n",
    "        <li>Calculate MRSS</li>\n",
    "        $$ MRSS=\\frac1{n_{best}}\\sum_{j=1}^{n_{best}}RSS(\\pmb\\theta_{acc},\\theta_{best,j}) $$\n",
    "    </ol>\n",
    "    <li>Return the set of statistics which produced the lowest MRSS value.</li>\n",
    "</ol>\n",
    "\n",
    "**Extensions**\n",
    "\n",
    "Again, the issue of the large number of possible subsets makes this algorithm inefficient. The same solutions as proposed in [Minimising-Entropy](#Minimising-Entropy) can be applied (a cap or pruning). An additional approach is to consider, which is in step 3. to only consider the subsets which performed \"well\" in step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_grad = (lambda ys:[np.mean([ys[i+1][0]-ys[i][0] for i in range(len(ys)-1)])])\n",
    "rand=(lambda ys:[stats.uniform(0,6).rvs(1)[0]]) # variance set st similar number of samples accepted as mean_grad\n",
    "rand_grad = (lambda ys:[mean_grad(ys)[0]*stats.uniform(0,2).rvs(1)[0]])\n",
    "summary_stats=[mean_grad,rand,rand_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_priors_intersect_known=[stats.uniform(1,0),stats.uniform(8,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 [(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
      "\n",
      "Results\n",
      "([0],25)\n",
      "Wall time: 1h 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats,_=ABC.two_step_minimum_entropy(summary_stats=summary_stats,n_obs=10,y_obs=lm.observe(),fitting_model=lm.copy([1,1]),priors=lm_priors_intersect_known,printing=False)\n",
    "    returned_stats.append(best_stats)\n",
    "\n",
    "print(\"\\n\\nResults\")\n",
    "print_results(returned_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Semi-) Automatic ABC\n",
    "\n",
    "In *Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation* Fearnhead and Prangle suggest an approach they call Semi-Automatic ABC. This approach involves a pilot run of ABC which is used to construct summary statistics, which are then used to fit the model in the second run of ABC.\n",
    "\n",
    "This method is called \"Semi-Automatic\" as the specification of the created summary statistics does rely on simulated values, but is only \"Semi\" as we still \"arbitrarly\" specify parts of the creation process (e.g. $f(\\cdot)$ below and the priors & summary stats during the pilot ABC run).\n",
    "\n",
    "<a id=\"semi_auto_abc_create\"></a>\n",
    "**Creating Summary Statistics**\n",
    "\n",
    "Theoretical analysis shows that the posterior mean of each parameter is the optimal summary statistic. However, this value cannot  be calculated and in practice other techniques need to be used. Posterior means are the optimal statistics when trying to minimise quadratic loss, thus finding statistics which do this is an optimal strategy.\n",
    "\n",
    "Fearnhead and Pangle suggest that \"Lasso\" (Hastie et al. 2001) and \"Canonical Correlation Analysis\" (Mardia et al. 1979) are valid methods, but ultimately determine that least-squares linear regression is best as it performs just as well and it easy to implement.\n",
    "\n",
    "Linear regression is applied to each parameter independely and least-squares is used to find $\\hat\\beta_0^{(i)},\\hat{\\pmb\\beta}_1^{(i)}$ which best fit the following linear expression\n",
    "$$ \\theta_i=\\mathbb{E}[\\theta_i|D]+\\varepsilon_i=\\beta_0^{(i)}+\\pmb\\beta_1^{(i)}f(D)+\\varepsilon_i $$\n",
    "where $\\theta_i$ is the $i^{th}$ parameter, $D$ is output data from the model when $\\theta_i$ is used (We generate $M$ $(\\theta_i,D)$ pairs in the algorithm and $f(\\cdot)$ is a transformation (possibly non-linear).\n",
    "\n",
    "Note that $\\pmb\\beta_1^{(i)}$ is a vector of the same dimension of $f(D)$, meaning $\\pmb\\beta_1^{(i)}f(D)$ is a weighted sum.\n",
    "\n",
    "The simplest choice for transform $f(\\cdot)$ is the identity function $f(x)=x$ but in practice different transformations may be found to be more effective such as using logarithms to make an exponential model linear.\n",
    "\n",
    "The summary statistic for each parameter $\\theta_i$ is\n",
    "$$S_i(D)=\\hat{\\pmb\\beta}_1^{(i)}f(D)$$\n",
    "The constant term $\\hat\\beta_0$ is dropped as ABC only every compares the difference between summary statistic values.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "<ol>\n",
    "    <li>Perform a pilot run of ABC using arbitratily chosen summary statistics. Let $F$ be the posterior generated for the parameters.</li>\n",
    "    <li>Generate $M$ sets of parameters from $F$ and simulated model outputs for each set. This creates a set of $M$ parameter-data pairs $(\\theta_i,D_i)$.</li>\n",
    "    <li>Use the simulated data $\\{(\\theta_{sim,1},D_1),\\dots,(\\theta_{sim,M},D_M)\\}$ to create a set of summary statistics $S$. (See <a href=\"#semi_auto_abc_create\">Above</a> for possible methods). </li>\n",
    "    <li>Return $S$.</li>\n",
    "</ol>\n",
    "\n",
    "**Extensions**\n",
    "\n",
    "Step 1. is purely require to find a region in the parameter space with non-neglible posterior mass and thus can be skipped if our priors are already informative.\n",
    "\n",
    "When running ABC using the creates summary stats $S$, the priors used to be truncated such that they don't exceed the range of the posterior generated in step 1.\n",
    "\n",
    "**Limitation**\n",
    "\n",
    "See https://spiral.imperial.ac.uk/bitstream/10044/1/71280/2/1201.1314v1.pdf\n",
    "\n",
    "Assumes a linear relationship between each parameter and the outcomes.\n",
    "\n",
    "**Note**\n",
    "The generate summary statistics maps the data from $\\mathbb{R}^{N\\times M}$ to $\\mathbb{R}^{1\\times M}$. This is a very dimension reduction and is intuitively appealing as there is a single summary statistics for parameter. If each parameter is independent then this is an optimal reduction, however, in practice, parameters are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Projections\n",
    "\n",
    "Neural networks and deep learning techniques have been used to model the relationship between parameter values and summary statistics.\n",
    "\n",
    "In *Learning Summary Statistics For Approximate Bayesian Coputation Via Deep Neural Networks* Jiang, Wu, Zheng and Hong explore automating the summary statistic selection process by using trained DNNs as summary statistics rather than theoretically motivated summary statistics. One DNN is used per model parameter and the network is targetting the posterior mean. A large limitation is that of DNNs overfitting, but there are standard methods to combat this (have lots of training data relative to num params; regularisation).\n",
    "\n",
    "*Marin et. al ('16)* used random forests to combine summary statistics. The idea being to use several summary statistics to produce a good estimate of the posterior mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
