{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ABC\n",
    "import numpy as np\n",
    "from Models import LinearModel\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics and ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary statistic $s(\\cdot)$ is a projection of high-dimensional data to a lower dimension space.\n",
    "$$ s:\\mathbb{R}^m\\to\\mathbb{R}^p\\text{ with }m>p $$\n",
    "Sets of summary statistics $S=\\{s_1,\\dots,s_K\\}$ are often applied to a data set, with each one capturing a different piece of information.\n",
    "$$ S:\\mathbb{R}^m\\to\\mathbb{R}^{\\sum p_i}\\text{ where }s_i:\\mathbb{R}^m\\to\\mathbb{R}^{p_i}\\text{ with }m>\\sum p_i $$\n",
    "It is common for datasets to be better represented by a matrix $\\mathbb{R}^{n\\times m}$ as they involve many observations, and each observation is multi-dimensional. In this case we want our summary statistics to map to dimensions $\\{p_1,\\dots,p_K\\}$ st $n\\times m\\ll \\sum p_i$.\n",
    "\n",
    "Ideally, summary statistics are able to compress a dataset $D$ without compromising the information held within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    " * [Motivation](#Motivation)\n",
    "   * [Motivating Example](#motivating_example)\n",
    " * [Summary Statistics and ABC](#Summary-Statistics-in-ABC)\n",
    " * [Common Summary Statistics](#Common-Summary-Statistics)\n",
    " * [Properties of Summary Statistics](#Properties-of-Summary-Statistics)\n",
    "   * [Sufficiency](#Sufficiency)\n",
    "   * [Approximation Sufficiency](#Approximate-Sufficiency)\n",
    "       * [Demonstration](#joyce_marjoram_demonstation)\n",
    " * [Non-Linear Projections](#Non-Linear-Projections)\n",
    " * [Summary Statistic Selection](#Summary-Statistic-Selection)\n",
    " * [(Semi-) Automatic ABC](#(Semi-)-Automatic-ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In recent years there has been an explosion in the amount of data available. This is due to reduction in costs of computational power and storage, commercial interests, changes in societal opinion towards data collection and many other factors.\n",
    "\n",
    "Larger datasets mean more computational power is required to run algorithms on them. Thus, being able to reduce the size of a dataset whilst still retaining most (if not all) of the meaningful information means the same analysis can be performed but require less resources.\n",
    "\n",
    "Approximate Bayesian Computation (ABC) methods are a class of algorithms for inferring model parameters given some observed data. ABC methods are computationally inefficient as they rely on simulations, thus they often require the use of summary statistics to be computationally tractable.\n",
    "\n",
    "<a id=\"motivating_example\"></a>\n",
    "**Motivating Example**\n",
    "\n",
    "Suppose you are given a dataset for the results of flipping a coin $X=\\{H,H,T,T,H,T,T,T,T,T\\}$ and you are tasked to determine whether the coin is biased or not. You may consider using two summary statistics:\n",
    " * The proportion of times heads occurs. $s_1(X)=3/10$.\n",
    " * The proportion of times tails occurs. $s_2(X)=7/10$.\n",
    "\n",
    "Now you analysis only involves two data points $(s_1(X),s_2(X))$ rather than $|X|=10$. This approach also has the advantage the number of data points being analysed is fixed, whereas the size of the dataset $X$ could be variable.\n",
    "\n",
    "This is obviously a very trivial example but it is easy to see how it can be extended to larger $X$s or multiple trials.\n",
    "\n",
    "(TODO - use how RONA data is reported to demonstrate why different summary statistics are important for different problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics in ABC\n",
    "See [ABC_Notebook](ABC_Notebook.ipynb) for details on ABC methods and its implementation.\n",
    "\n",
    "Approximate Bayesian Computation (ABC) methods are a class of algorithms for inferring model parameters given some observed data. ABC methods are computationally inefficient as they rely on simulations, thus they often require the use of summary statistics to be computationally tractable.\n",
    "\n",
    "ABC methods infer model parameters by generating an approximation of the posterior for each parameter, given some observed data from the true model. The better the approximation, the better the resulting model fit should be. Summary statistics influence the quality of this approximation as reducing dimensionality of data will always worsen the approximation. Our task is to choose summary statistics such that this loss of information (or at least make worthwhile improvements to the computational effiency).\n",
    "\n",
    "ABC methods involve comparing simulated data to observed data and determining how close the two sets are to each other. When these datasets have high-dimensionality it is less likely that they coincide due to the *curse of dimensionality*. Thus transforming these datasets with summary statistics makes it easier to compare them and should reduce the number of simulations required (or improve the quality of simulations kept).\n",
    "\n",
    "Greater reduction in dimensionality means greater reduction in computation requirements but also greater level of approximation, likely meaning the produced posterior is less representative.\n",
    "\n",
    "In ABC there are two sources of approximation-error: only requiring simulated data to be \"similar to\" rather than \"equal to\" the observed data; and comparing summaries of the simulated $S(X)$ and observed data $S(X_{obs})$, rather than comparing the raw simulated $X$ and observed data $X_{obs}$. An effective summary statistic should balance the trade-off of these two sources of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Summary Statistics\n",
    "Most good summary statistics consider the following properties\n",
    " * Centre of the data (e.g. mean, median).\n",
    " * Spread of the data (e.g. variance).\n",
    " * Shape of the data (e.g. skew).\n",
    " * Interdependence of data-fields (e.g. correlation).\n",
    " * Quantiles of the data\n",
    "The reasons for this is explained below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Summary Statistics\n",
    "(Computational efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficiency\n",
    "A statistic is said to be \"sufficient\" for a model if no other statistic provides more information about the model's parameters. This idea can be extended to sets of statistics (i.e. summary statistics).\n",
    "\n",
    "Formally a set of statistics $S(\\cdot)=(s_1(\\cdot),\\dots,s_K(\\cdot))$ are said to be sufficient for the data $X\\sim f(\\cdot;\\theta)$ if\n",
    "\n",
    "$$ \\mathbb{P}(X|S(X))=\\mathbb{P}(X|S(X),\\theta)) $$\n",
    "\n",
    "This demonstrates that the summary statistics $S(X)$ capture all the information the data $X$ holds about the model parameters $\\theta$. The summary statistics $S(\\cdot)$ are a loss-less compresion of the data. (The likelihood of data $X$ given the summary statistic values $S(X)$ is independent of the model parameters $\\theta$).\n",
    "\n",
    "It is worth noting that if a set of statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ are sufficient for $X$ then any super-set $S\\supseteq S_{1:K-1}$ is also sufficient for $X$. This means that not only do we want to find a sufficient set of statistics, but also the smallest set (Minimum cardinality). Having a smaller set of statistics is desirable due to the computational efficiencies. (TODO proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Sufficiency\n",
    "In practice sufficient summary statistics do not always exist. Moreover, the Pitman–Koopman–Darmois Theorem states that sufficient statistics (whose dimension does not increase as sample size increase) only exist for models in the exponential family.\n",
    "\n",
    "<!-- The concept of approximate sufficiency is a generalisation of sufficiency which can be applied to any statistical model and involves finding sets of statistics which capture a large amount of information from the sample. -->\n",
    "\n",
    "Joyce & Marjoram introduce an approach to finding approximately sufficient statistics by building up a subset of statistics from a larger set. A statistic $S_K$ is added to the set $S_{1:K-1}=\\{S_1,\\dots,S_{K-1}\\}$  if it is deemed to significantly increase the information extracted from the data $X$.\n",
    "\n",
    "**Score $\\delta_{K}$**\n",
    "\n",
    "To determine the information gain from adding $S_K$ to the set $S$, first consider the following result from the product rule\n",
    "\n",
    "$$\\begin{array}{rrcl}\n",
    "&\\mathbb{P}(S_{1:K}(X)|\\theta)&=&\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\mathbb{P}(S_{1:K-1}|\\theta)\\\\\n",
    "\\implies&\\ln\\left\\{\\mathbb{P}(S_{1:K}(X)|\\theta)\\right\\}&=&\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}+\\ln\\left\\{\\mathbb{P}(S_{1:K-1}|\\theta)\\right\\}\\\\\n",
    "\\implies&\\ln\\left\\{\\mathbb{P}(S_{1:K}|\\theta)\\right\\}-\\ln\\left\\{\\mathbb{P}(S_{1:K-1}|\\theta)\\right\\}&=&\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}\n",
    "\\end{array}$$\n",
    "\n",
    "This shows that $\\ln\\left\\{\\mathbb{P}(S_K(X)|S_{1:K-1},\\theta)\\right\\}$ quantifies the information gained by adding $S_K$ to $S_{1:K-1}$. Thus, we want to find statistcs $S_K$ which maximise this quantity.\n",
    "\n",
    "Joyrce & Marjoram propose the *Score* metric $\\delta_{K}$, defined as\n",
    "$$ \\delta_{K}:=\\sup_\\theta\\left\\{\\ln\\left\\{\\mathbb{P}(S_K|S_{1:K-1},\\theta)\\right\\}\\right\\}-\\inf_\\theta\\left\\{\\ln\\left\\{\\mathbb{P}(S_K|S_{1:K-1},\\theta)\\right\\}\\right\\} $$\n",
    "\n",
    "**$\\varepsilon$-Approximate Sufficiency**\n",
    "\n",
    "A set of statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ is said to be $\\varepsilon$-Sufficient relative to a statistic $S_K$ if the score of $S_K$ relative to the set $S_{1:K-1}$ is no-greater than $\\varepsilon$.\n",
    "$$\\delta_K\\leq\\varepsilon$$\n",
    "Note that this is equivalent to the definition of sufficiency when $S_K=X$ (identity function) and $\\varepsilon=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistic Selection\n",
    "The most naive way to assess summary statistics is to visually inspect the plots when they are used.\n",
    "\n",
    "### Approximate Sufficiency\n",
    "Here are details on an approach, proposed by Joyce & Majoram, to selecting summary statistics which uses the idea of approximate sufficiency.\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "Consider a large set of statistics $S:=\\{S_1,\\dots,S_N\\}$ and fix some value for $\\varepsilon\\in\\mathbb{R}^{\\geq0}$. Here is the algorithm proposed by Joyce & Marjoram for finding an $\\varepsilon$-sufficient subset $S^*$ of $S$, relative to the data set $X$.\n",
    "<ol>\n",
    "    <li>Define $S'=\\emptyset$</li>\n",
    "    <li>Calculate the score for statistic in $S_i\\in (S\\setminus S')$. See <a href=\"#approximating_score\">below</a>.</li>\n",
    "    <li>Identify the greatest score $\\delta_{max}:=\\max_{S_i\\in S}\\text{Score}(S_i,S')$.</li>\n",
    "    <li>Identify a statistic which attains the greatest score $S_{max}\\in\\text{argmax}_{S_i\\in S}\\text{Score}(S_i,S')$.</li>\n",
    "    <li>If $\\delta_\\text{max}>\\varepsilon$ then add $S_{max}$ to $S'$ and return to step 2. Otherwise, stop the algorithm and return $S'$</li>\n",
    "    $$S'=S'\\cup S_{max}$$\n",
    "    <li>When adding a new statistic, you should consider removing each summary statistic which has already been accepted so as to achieve the smallest set which is accepted.</li>\n",
    "</ol>\n",
    "\n",
    "**Variations**\n",
    "\n",
    "This algorithm is deterministic when a deterministic method is used in step 4. However it runs in $O(N^K)$ time when $K$ summary statistics are chosen. There are variations on this algorithm which aim to improve the run-time, inc.\n",
    " * Calculate the score for each statistic in $S$ sequentially and if one below $\\varepsilon$ then it is added to $S'$. (Rather than calculate score for all and chosing the maximum).\n",
    " * Remove $\\varepsilon$ and instead find the best subset of size $K$.\n",
    "\n",
    "**Implementation**\n",
    "<a id=\"approximating_score\"></a>\n",
    "\n",
    "The main difficulty in implementing the algorithm above is in calculating the score $\\delta_K$ for each statistic. Due to the nature of the problems which we are finding summary statistics for, we can only ever approximate the score $\\delta_K$.\n",
    "\n",
    "Joyce & Majoram replace the task of empirically estimating the score with the equivalent task of determining whether the odds-ratio differs from 1 more than would be expected from random noise.\n",
    "\n",
    "Suppose you have a set of already accepted statistics $S_{1:K-1}:=\\{S_1,\\dots,S_{K-1}\\}$ and are considering adding statistic $S_K$. Suppose a large set of parameters $\\Theta$ have been generated and let $\\Theta_{1:K-1},\\ \\Theta_{1:K}$ be the set of parameters where accepted when using $S_{1:K-1}$ and $S_{1:K}$ respectively.\n",
    "\n",
    "Define $N_{K-1}:=|\\Theta_{1:K-1}|,N_K:=|\\Theta_{1:K}|$\n",
    "\n",
    "<ol>\n",
    "    <li>Discretise the sets of accepted parameter $\\Theta_{1:K-1},\\Theta_{1:K}$ into $M$ bins. Let $C_{1:K-1},C_{1:K}$ be these two sets of counts.</li>\n",
    "    <li>Calculate the expected count for each bin $E=(N_K/N_{K-1})\\cdot C_{1:K-1}$. </li>\n",
    "    <li>Calculate the standard-variance for each bin $$sd=\\sqrt{E\\cdot\\frac{N_{K-1}-C_{1:K-1}}{N_{K-1}}}$$. </li>\n",
    "    <li>If any values in the counts from the proposed set $C_{1:K}$ is more than four standard deviations away from the expected counts then return that the proposed distribution is significantly different from the current (ie accepted $S_k$). Otherwise, accept don't accept the change in summary stats.</li>\n",
    "</ol>\n",
    "\n",
    "A big limitation of this approach is you can only determine whether two sets of summary stats produce notable different posteriors, and not whether one posterior is better than the other. This symmetry is an issue as it means it is as likely to replace a bad choice with a good choice as it is to replace a good choice with a bad choice.\n",
    "\n",
    "Moreover, only being able to compare and not rank sets of statistics means the algorithm above needs to be largely reworked in practice.\n",
    "\n",
    "**Demonstration**<a id=\"joyce_marjoram_demonstation\"></a>\n",
    "\n",
    "I have implemented the algorithm above as `ABC.joyce_marjoram()`. Here I demonstrate its usage by applying it to choosing a set of approximately sufficient summary statistics for a linear model $f(X)=1+10X+\\varepsilon$ where $\\varepsilon\\sim\\text{Normal}(0,30)$ and the start point is known. The only parameter which needs to be learnt is the gradient of the model. The mean difference gradient between consecutive data-points is a good estimator of the gradient of the model.\n",
    "\n",
    "I supply the algorithm with the following three summary statistics\n",
    " * $S_0$ - Mean gradient.\n",
    " * $S_1$ - $U\\sim\\text{Uniform}[0,6]$. This range was chosen so the variance is similar to $S_0$.\n",
    " * $S_2$ - $S_1\\cdot X$ where $X\\sim\\text{Uniform}[0,2]$\n",
    "\n",
    "I ran the algorithm 25 times, each time generating 10,000 samples using a uniform acceptance-kernel with bandwidth 1 and discretising the posterior into 10 bins. The results can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define model\n",
    "lm=LinearModel(  # 1+10x\n",
    "    n_params=2,\n",
    "    params=[1,10],\n",
    "    n_vars=1,\n",
    "    n_obs=10,\n",
    "    x_obs=[[x] for x in range(10)],\n",
    "    noise=0\n",
    "    )\n",
    "lm.plot_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define summary statistics\n",
    "# Linear Model with known start point\n",
    "mean_grad = (lambda ys:[np.mean([ys[i+1][0]-ys[i][0] for i in range(len(ys)-1)])])\n",
    "rand=(lambda ys:[stats.uniform(0,6).rvs(1)[0]]) # variance set st similar number of samples accepted as mean_grad\n",
    "rand_grad = (lambda ys:[mean_grad(ys)[0]*stats.uniform(0,2).rvs(1)[0]])\n",
    "summary_stats=[mean_grad,rand,rand_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 18s, [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Choose summary statistics for Linear Model with known start point\n",
    "param_bounds=[(1,1),(8,14)]\n",
    "lm_priors_intersect_known=[stats.uniform(1,0),stats.uniform(8,6)]\n",
    "returned_stats=[]\n",
    "n_tests=25\n",
    "\n",
    "for i in range(n_tests):\n",
    "    print(\"{}/{} \".format(i,n_tests),end=\"\")\n",
    "    print(returned_stats,end=\"\\r\")\n",
    "    best_stats=ABC.joyce_marjoram(summary_stats,n_obs=10,y_obs=lm.observe(),fitting_model=lm.copy([1,1]),priors=lm_priors_intersect_known,param_bounds=param_bounds,n_samples=10000,n_bins=10,printing=False)\n",
    "    returned_stats.append(best_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0],20)\n",
      "([1],5)\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "used=[]\n",
    "for x in returned_stats:\n",
    "    if x not in used:\n",
    "        used.append(x)\n",
    "        print(\"([{}],{})\".format(\",\".join([str(y) for y in x]),returned_stats.count(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comments*\n",
    "\n",
    "This implementation settles on the set $\\{S_0\\}$ in most cases, which is the best set of statistics from those provided. It is dissapoint that it picks the completely random noise $S_1$ on five occassions but this is likely due to the setup of the acceptance kernel so could be reduced with tuning (or more samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Semi-) Automatic ABC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
