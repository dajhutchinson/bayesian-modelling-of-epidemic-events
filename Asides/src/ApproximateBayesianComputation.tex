\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm,graphicx,tikz}
\usetikzlibrary{automata,positioning}
\graphicspath{ {img/} }
\usepackage[section,nohyphen]{DomH}
\headertitle{Approximate Bayesian Computation}

\begin{document}

\setcounter{section}{1}

\title{Approximate Bayesian Computation}
\author{Dom Hutchinson}
\date{\today}
\maketitle

\section*{Intro to ABC}

  \begin{definition}{Approximate Bayesian Computation (ABC)}
    \textit{Approximate Bayesian Computation (ABC)} is a family of computational methods for estimating the posterior of model parameters for \textit{Generative Models}. \textit{Generative Models} are models which can be simulated from but we do not have an explicit definition for their posterior $f+\mathcal{G}(x|\theta)$ (eg most IRL systems).
  \end{definition}

  \begin{proposition}{Motivating Idea\cite{ABC_Annual_Review}}
    \par Consider a set of observations\footnote{From a \textit{Generative Model}} $\mathbf{y}:=(y_1,\dots,y_n)$ where each $y_i\in\reals^m$ is high dimensional. Let $s(\cdot):\reals^m\to\reals^p$ be a mapping (known as a \textit{Summary Statistic}) from the observed data to some lower dimension $p$.
    \par \textit{ABC} aims to infer the joint distribution of parameters $\pmb\theta$ and general summary statistics $\mathbf{s}$, given the observed summary statistics $\mathbf{s}_{obs}:=s(\mathbf{y})$
    \[ p_\epsilon(\pmb\theta,\mathbf{s}|\mathbf{s}_{obs})\propto\underbrace{\pi_0(\pmb\theta)}_\text{Prior}\underbrace{f(s|\pmb\theta)}_\text{Likelihood}K_\epsilon(\|\mathbf{s}-\mathbf{s}_{obs}\|) \]
    where $K_\epsilon(\cdot)$ is a kernel function with scaling parameter $\epsilon$ and $\|\cdot\|$ is a distance measure (e.g. Euclidean).\footnote{The likelihood $f(s|\theta)$ is the only one of these features which is not specified by the user, and thus what we need to ``learn'' it.}
    \par From this joint distribution the posterior for parameters $\pmb\theta$, given the observed summary statistics $\mathbf{s}_{obs}:=s(\mathbf{y})$, can be calculated as
    \[ p_\epsilon(\pmb\theta|\mathbf{s}_{obs})=\int p_\epsilon(\pmb\theta,\mathbf{s}|\mathbf{s}_{obs})d\mathbf{s} \]
    Monte-Carlo Algorithms can be used to sample from this posterior $p_\epsilon(\pmb\theta|\mathbf{s}_{obs})$ without having to explicitly state the likelihood $f(\mathbf{s}|\theta)$. \textit{Numerical Integration} methods can then be used to evaluate the integral\footnote{See \textit{Monte-Carlo Integration} methods: \textit{Uniform Sampling}, \textit{Importance Sampling}}.
  \end{proposition}

  \begin{proposition}{Setup of ABC}
    To perform \textit{ABC} we typically have/define the following features
    \begin{itemize}
      \item A set of observations from a \textit{Generative Model} $\mathbf{y}:=(y_1,\dots,y_n)$ where each $y_i$ is high-dimensional.
      \item A map $s(\cdot)$ which maps the high-dimensional observed data to a lower dimension.
      \item A theorised model with posterior pdf $f_\mathcal{T}(\cdot|\pmb\theta,\x)$ where $\pmb\theta$ are the parameters which we wish to fit to the \textit{Generative Model} using ABC.
      \item A prior $\pi_0(\cdot)$ for the parameters $\pmb\theta$.
      \item A kernel $K_\epsilon(\cdot)$ and a distance measure $\|\cdot\|$.
    \end{itemize}
  \end{proposition}

  \begin{proposition}{ABC Algorithm - Simple, Online}
    Consider the setup in \texttt{Proposition 1.2}. Here is a simple, online algorithm for ABC
    \begin{enumerate}
      \item Sample a set of parameters from the prior $\tilde{\pmb\theta}_t\sim\pi_0(\pmb\theta)$.
      \item Chose points $\x$ in the variable-space which you wish to sample from the \textit{Theorised Model} at.\footnote{Ideally at points near to ones where the generative model has been observed.}
      \item Sample from the theorised model at these points, using the sample parameters
      \[ \mathbf{y}_t\sim f_\mathcal{T}(\mathbf{y}|\tilde{\pmb\theta},\x) \]
      \item Calculate the summary statistic values for the sampled values $\mathbf{s}_t=\mathbf{s}(\mathbf{y}_t)$.
      \item Reject the sample summary statistic value $\mathbf{s}_t$ with probability $K_\epsilon(\|\mathbf{s}_t-\mathbf{s}_{obs}\|)$ where ${\mathbf{s}_{obs}=s(\mathbf{y})}$.
      \item Repeat steps i)-iii) until a total of $M$ simulated values have been accepted.
    \end{enumerate}
    Our final sample contains a set of $M$ summary statistics, along with the parameter values $\pmb\theta$ and variable-space points $\x$, which produced them. This data can be used to approximate the posterior for the parameter values.
  \end{proposition}

\section*{Decisions}

  \begin{remark}{Decisions}
    When implementing ABC there are several decisions to make, including:
    \begin{itemize}
      \item What theorised model $f(\cdot|\pmb\theta,\x)$ to use.
      \item What kernel $K_\epsilon(\cdot)$ to use.
      \item What summary statistics $s(\cdot)$ to use.
      \item Do we even need summary statistics?
      \item How long to sample for?
    \end{itemize}
  \end{remark}

  \begin{proposition}{Kernels $K_\epsilon(\cdot)$}
    A \textit{Kernel} is used to determine with what probability to accept a sample, given it is a certain distance away from observed data. Here are some common kernels
    \begin{itemize}
      \item \textit{Uniform Kernel} $K_\epsilon(\|\mathbf{s}-\mathbf{s}_{obs}\|):=\indexed\{\|\mathbf{s}-\mathbf{s}_{obs}\|\leq\epsilon\}$ which accepts simulated values if they are within $\epsilon$ of observed data.
      \item \textit{Epanechnikov Kernel} $K_\epsilon(\|\mathbf{s}-\mathbf{s}_{obs}\|):=\frac3{4\epsilon}\left(1-\left(\frac{\|\mathbf{s}-\mathbf{s}_{obs}\|}\epsilon\right)^2\right)$ for $\|\mathbf{s}-\mathbf{s}_{obs}\|\leq\epsilon$
      \item \textit{Gaussian Kernel} $K(\|\mathbf{s}-\mathbf{s}_{obs}\|):=\frac1{\sqrt{2\pi}}e^{-\frac12\|\mathbf{s}-\mathbf{s}_{obs}\|^2}$
    \end{itemize}
  \end{proposition}

  \begin{proposition}{Summary Statistics $s(\cdot)$}
    See \texttt{SummaryStatisticSelection.pdf}
  \end{proposition}

  \begin{proposition}{How long to sample for}
    The algorithm given in \texttt{Proposition 1.3} runs the algorithm until a sufficiently large sample has been produced. This is not ideal as the algorithm will run for an unknown period of time and is dependent upon the kernel $K_\epsilon(\cdot)$ which has been defined.
    \par Alternatively, all simulated values could be kept and then all but the best $M$\footnote{$M$ closest to $s_\text{obs}$.} are discarded.
  \end{proposition}

\section*{Semi-Automatic ABC}

  \begin{definition}{Semi-Automatic ABC\cite{Constructing_Summary_Statistics_For_ABC}}
    In \textit{Semi-Automatic ABC} summary statistics are learnt from simulation, but the user still has to make choices around what transformation $\mathbf{f}(\cdot)$ of simulated data $\mathbf{y}$.
    \par An application of \textit{Semi-Automatic ABC} should perform better in a general setting than traditional \textit{ABC}.
  \end{definition}

  \begin{proposition}{Semi-Automatic ABC - Algorithm}
    \begin{enumerate}
      \item Perform a pilot run of ABC\footnote{We need to define some arbitrary summary-statistics for this.} to determine a training-region of non-negligible posterior mass.
      \item for $t\in[1,M]$:
      \begin{enumerate}
        \item Simulate parameters $\pmb\theta_t$ from our prior $\pi_0(\pmb\theta)$, with the prior truncated to the training-region determined in (i).
        \item Simulate results $\mathbf{y}_t\sim f(\mathbf{y}|\pmb\theta_t)$ using these parameters.
      \end{enumerate}
      \item Use simulated data and parameter values to estimate summary statistics.\footnote{Potential methods inc. linear-regression, lasso analysis, cross-correlation analysis.}
      \item Run ABC with these estimated-summary statistics.
    \end{enumerate}
  \end{proposition}

  \begin{remark}{Step iii)}
    In step iii) we have simulated data $\mathcal{D}:=\{(\pmb\theta_1,\mathbf{y}_1),\dots,(\pmb\theta_M,\mathbf{y}_M)\}$ where $\pmb\theta_t\in\reals^m,\ \mathbf{y}_t\in\reals_t^n\ \forall\ t\in[1,M]$. We want to learn a transformation $f(\mathbf{y}_t)$ of the simulated data $\mathbf{y}_t$ st the parameters $\pmb\theta_t$ can be learnt from the transformation.
    \par A \textit{Liner Regression} approach is to learn the pick a function $\mathbf{f}:\reals^n\to\reals^m$\footnote{This transformation can actually map to any dimension but we prefer for it to be a lower dimension that the simulated data $\mathbf{y}$.} which maps the simulated data $\x_t$ to the same dimension as the simulated parameters (ie to $m$ dimensions) and then the parameters $\pmb\beta_0,\pmb\theta_1$ which give the least total-error across the whole data set $\mathcal{D}$\footnote{Generally least-square-error.} for the following
    \[\begin{array}{rcl}
      \pmb\theta_t&=&\pmb\beta_0+\pmb\beta_1\cdot \mathbf{f}(\mathbf{y}_t)+\varepsilon_t\\
      \Leftrightarrow[\pmb\theta_t]_i&=&[\pmb\beta_0]_i+[\pmb\beta_1]_i\cdot[\mathbf{f}(\mathbf{y}_t)]_i+[\varepsilon_t]_i\\
    \end{array}\]
    Our estimate for the model parameters $\pmb\theta$, given some data $\mathbf{y}$, is thus the fitted value
    \[ \hat{\pmb\theta}=\expect[\pmb\theta|\mathbf{y}]=\hat{\pmb\beta}_0+\hat{\pmb\beta}_1 \mathbf{f}(\pmb{y}) \]
    The constant terms $\hat{\pmb\beta}_0$ can be ignored as ABC only uses the distance between summary statistics (not their absolute value). This means our $m$ summary statistics are the different dimensions of $\hat{\pmb\beta}_1\mathbf{f}(\cdot)$
  \end{remark}

  \begin{remark}{Choosing transformation $\mathbf{f}(\cdot)$}
    In \texttt{Remark 1.2}  the user has to define how to transform the simulated results (ie define $\mathbf{f}(\cdot)$) and this choice will affect the set of summary statistics generated. It is easy to run this stage multiple times, using different transformations on the same data $\mathcal{D}$ and then using standard model comparison procedures\footnote{e.g. BIC, sufficiency} to determine which of the generate summary statistics are sufficient.
  \end{remark}

\newpage
\bibliographystyle{unsrt}
\bibliography{aside_bib}

\end{document}
