\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage{amsmath,amsfonts,fancyhdr,bbm,graphicx,tikz,url}
\usetikzlibrary{automata,positioning}
\graphicspath{ {img/} }
\usepackage[section,nohyphen]{DomH}
\headertitle{Summary Statistic Selection}

\begin{document}

\title{Summary Statistic Selection}
\author{Dom Hutchinson}
\date{\today}
\maketitle

\tableofcontents

\section{Summary Statistics}

  \begin{definition}{Summary Statistics $s(\cdot)$}
    \textit{Summary Statistics} are a projection of high-dimensional data to a lower dimensional space.
    \[ s(\cdot):\reals^m\to\reals^n\quad\text{where }m>n \]
    This is aimed to be done in such a way that as much information is retained as possible (ie the summary is as accurate as possible). The lower dimensional projection is important to make ABC computationally tractable.
    \par The trade-off here is between computational requirements and data retention. As data is lost posterior accuracy and stability decreases.
  \end{definition}

  \begin{remark}{Summary Statistics for ABC}
    In ABC we want each \textit{Summary Statistic} to map from all observations of the model to a lower dimensional space. As each observation can be multi-dimensional, this means \textit{Summary Statistics} for ABC typically map from a matrix to a vector
    \[ s(\cdot):\reals^{n\times m}\to\reals^p \]
    where $n$ is the number of observations, $m$ is the dimension of each observation and $p$ is the dimension being mapped to. Ideally $p\ll n\times m$. Moreover, $\sum p_i\ll n\times m$ where $\{p_1,\dots,p_k\}$ are the dimensions the $k$ summary stats map to.
  \end{remark}

  \begin{proposition}{Common Summary Statistics}
    Typically summary statistics describe the following
    \begin{itemize}
      \item centre of the data (e.g. mean).
      \item spread of the data (e.g. variance).
      \item shape of the data (e.g. pearson's skew).
      \item dependence of different data fields (e.g correlation).
      \item start or end points.
    \end{itemize}
  \end{proposition}

  \begin{remark}{Reduced Dimension}
    Suppose we have $N$ samples of each of $M$ dimension, so all the data is represented by a matrix $\X\in\reals^{N\times M}$\footnote{ie $N$ observations, each with $M$ properties.}. If we were to just use the: mean, variance and pearson's skew to summarise each property, then the summarised data would only require a $\reals^{3\times M}$ matrix\footnote{3 values per property} whose size is independent of the number of observations $N$ and, significantly smaller than $\reals^{N\times M}$.
    \par The question is whether these three properties are sufficient to make valid/inciteful inferences from.
  \end{remark}

\section{Sufficiency}

  \begin{proposition}{Approximate Sufficiency (AS)}
    \textit{Approximate Sufficiency} is the practice of finding the subset of summary statistics (from a larger set) which satisfy some optimality condition. This is done by identifying a large set of summary statistics $S$ and then finding a subset $S'\subset S$ which is approximately as good as the superset $S$. There are several measures of sufficiency.
  \end{proposition}

  \begin{proposition}{Approach to AS}
    A typical approach to AS is to do the following\footnote{This approach requires the underlying model to be known, as we specify the model we are trying to fit, this is ok."}
    \begin{enumerate}
      \item Choose a measure of sufficient $M(\cdot)$.
      \item Start with an empty set $S'=\emptyset$.
      \item Keep adding summary statistics $s\in S/S'$ to $S'$ until $M(S')$ is no longer satisfied.
    \end{enumerate}
    This approach has limitations since the final subset $S'$ depends on the order in which summary statistics are added. Finding a way to order the elements of $S$ would help this.
  \end{proposition}

\subsection{Sufficiency}

  \begin{remark}{Distinguishing Models}
    If two models have the same sufficient statistics it is impossible to distinguish between them.
  \end{remark}

  \begin{definition}{Classical Sufficiency}
    Let $X\sim f(\cdot;\theta)$\footnote{$X$ could be one or many observations.} and $s(\cdot)$ be a summary statistic. $s(X)$ is a \textit{sufficient statistic} for $\theta$ if
    \[ \prob(X|s(X),\theta)=\prob(X|s(X)) \]
    ie the conditional distribution for $X$, given the summary statistic $s(X)$, is independent of $\theta$.
    \par This can be read intuitively that $s(X)$ captures all the information the sample $X$ contains about the parameter $\theta$. (A lossless data-compression).
  \end{definition}

  \begin{example}{Sufficient Statistic - Bernoulli Distribution}
    Let $X\sim\text{Bern}(p)$ with $p\in[0,1]$ unknown and $\x$ be $n$ independent samples of $X$. Note that
    \[\begin{array}{rrl}
      f_X(x)&:=&p^x(1-p)^{1-x}\\
      \implies f_\X(\x)&=&\prod_{i=1}^nf_X(x_i)\text{ by independence of samples}\\
      &=&p^{\sum x_i}(1-p)^{n-\sum x_i}
    \end{array}\]
    Consider the summary statistic $T(\x):=\sum x_i$. Note that $T(\X)$ is only one dimensional (rather than $n$) and ${T(\X)\sim\text{Binomial}(n,p)}$. Thus
    \[\begin{array}{rcl}
      f_T(T(\x))&=&{T(\x)\choose n}p^{T(\x)}(1-p)^{n-T(\x)}\\
      f_{\X,T}(\x,T(\x))&=&p^{T(\x)}(1-p)^{n-T(\x)}
    \end{array}\]
    Now consider the conditional distribution of $\X$ given the summary statistic $T(\X)$.
    \[\begin{array}{rcl}
      f_{\X|T(\X)}(\x|T(\x))&=&\dfrac{f_{\X,T(\X)}(\x)}{f_{T(\X)}(\x)}\\
      &=&\dfrac{p^{T(\x)}(1-p)^{n-T(\x)}}{{T(\x)\choose n}p^{T(\X)}(1-p)^{n-T(\X)}}\\
      &=&\dfrac1{{T(\x)\choose n}}
    \end{array}\]
    The conditional distribution of $\X$ given $T(\X)$ is independent of $p$, thus $T(\X)$ is a sufficient statistic for $p$.
  \end{example}

  \begin{example}{Sufficient Statistic - Gaussian Distribution with Unknown Mean}
    Let $X\sim\text{Normal}(\mu,\sigma^2_0)$ where $\mu\in\reals$ is unknown and $\sigma^2_0\in\reals$ is known, and $\x$ be $n$ independent observations of $X$. Note that
    \[\begin{array}{rrl}
      f_X(x)&:=&\frac1{\sqrt{2\pi\sigma^2_0}}\exp\left\{-\frac1{2\sigma_0^2}(x-\mu)^2\right\}\\
      \implies f_\X(\x)&=&\prod_{i=1}^nf_X(x_i)\text{ by independence of samples}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-\mu)^2\right\}
    \end{array}\]
    Consider the following formulation of the distribution of $\X$ with an arbitrary term $t$ introduced
    \[\begin{array}{rcl}
      f_\X(\x,t)&=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i+t-t-\mu)^2\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum((x_i-t)-(\mu-t))^2\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum\left[(x_i-t)^2+(\mu-t)^2-2(\mu-t)(x_i-t)\right]\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum(\mu-t)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum-2(\mu-t)(x_i-t)\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-t)^2\right\}\cdot\exp\left\{-\frac{-2(\mu-t)}{2\sigma_0^2}\sum(x_i-t)\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-t)^2\right\}\cdot\exp\left\{-\frac{-2(\mu-t)}{2\sigma_0^2}\left[\left(\sum x_i\right)-nt\right])\right\}\\
    \end{array}\]
    The third exponential disappears if $t:=\frac1nx_i$ (the sample mean). Consider the summary statistic $T(\X):=\frac1n\sum X_i$ meaning $T(\X)\sim\text{Normal}(\mu,\frac1n\sigma_0^2)$ by the Central Limit Theorem.
    \par The following are the marginal distribution for $T(\X)$ and the joint distribution of $\X$ and $T(\X)$.
    \[\begin{array}{rcl}
      f_{T(\X)}(T(\x))&=&\sqrt{\frac{n}{2\pi\sigma_0^2}}e^{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2}=n^{1/2}(2\pi\sigma_0^2)^{-1/2}\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2\right\}\\
      f_{\X,T(\X)}(\x,T(\x))&=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-T(\x))^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2\right\}\\
    \end{array}\]
    Now consider the conditional distribution of $\X$ given the summary statistic $T(\X)$.
    \[\begin{array}{rcl}
      f_{\X|T(\X)}(\x|T(\x))&=&\dfrac{f_{\X,T(\X)}(\x,T(\x))}{f_{T(\X)}(T(\x))}\\
      &=&\dfrac{(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-T(\x))^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2\right\}}{n^{1/2}(2\pi\sigma_0^2)^{-1/2}\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2\right\}}\\
      &=&n^{-1/2}(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-T(\x))^2\right\}
    \end{array}\]
    The conditional distribution of $\X$ given $T(\X)$ is independent of $\mu$, thus $T(\X)$ is a sufficient statistic for $\mu$.
  \end{example}

  \begin{proof}{If $S_{1:k-1}$ are sufficient then $\prob(\theta|S_{1:k})=\prob(\theta|S_{1:k-1})$ for all $S_k$.}
    \everymath={\displaystyle}
    Let $S_{1:k-1}$ be a set of summary-statistics which are sufficient for parameters $\theta$, and $S_k$ be any other summary statistic. Then
    \[\begin{array}{rcl}
      \prob(\theta|S_{1:k})&=&\frac{\prob(\theta,S_{1:k})}{\prob(S_{1:k})}\\
      &=&\frac{\prob(\theta,S_{1:k})}{\prob(S_{1:k})}\cdot\frac{\prob(S_{1:k-1})}{\prob(S_{1:k-1})}\\
      &=&\frac{\prob(\theta,S_k|S_{1:k-1})}{\prob(S_k|S_{1:k-1})}\\
      &=&\frac{\prob(\theta,S_k|S_{1:k-1})}{\prob(S_k|S_{1:k-1},\theta)}\text{ as }S_{1:k-1}\text{ are sufficient for }\theta\\
      &=&\frac{\prob(\theta,S_{1:k})}{\prob(S_{1:k-1})}\cdot\frac{\prob(\theta,S_{1:k-1})}{\prob(\theta,S_{1:k})}\\
      &=&\frac{\prob(\theta,S_{1:k})}{\prob(\theta,S_{1:k})}\\
      &=&\prob(\theta|S_{1:k-1})
    \end{array}\]
  \end{proof}

\subsection{Minimal Sufficiency}

  \begin{definition}{Minimal Sufficiency}
    A sufficient statistic $s(X)$ is \textit{Minimally Sufficient} if it can be represented as a function of any other sufficient statistic $t(X)$.
    \[ \exists\ f\text{ st }s(X)=f(t(X)) \]
  \end{definition}

  \begin{example}{Minimal Sufficient Statistics}
    TODO
    % e.g. The sample sum $T(X):=\sum_{i=1}^nX_i$ is a \textit{Minimially Sufficient} version of the same mean $S(X):=\frac1n\sum_{i=1}^nX_i$ since $T(X)=f(S(X))$ where $f(x):=nx$.
  \end{example}

\subsection{Fisher-Neyman Factorisation Theorem}

  \begin{theorem}{Fisher-Neyman Factorisation Theorem}
    If $X\sim f(\cdot;\theta)$ then $s(\cdot)$ is sufficient for $\theta$ iff there exists non-negative functions $g(\cdot;\theta),h(\cdot)$ st
    \[ f(X;\theta)=h(X)g(s(X);\theta) \]
    This shows that the data $X$ only interacts with the parameter $\theta$ through the sufficient summary statistics $s(X)$.
  \end{theorem}

  \begin{proof}{Fisher-Neyman Factorisation Theorem}
    TODO
  \end{proof}

  \begin{remark}{Usefulness of Fisher-Neyman Factorisation Theorem}
    In \texttt{Example 1.1}  and \texttt{Example 1.2} we had to guess at a definition of $T(\X)$ which produced a sufficient statistic. The \textit{Fisher-Neyman Factorisation Theorem} removes a lot of that guesswork, in place of a more formulaic approach to finding sufficient statistics (as shown in \texttt{Example 1.4} \& \texttt{Example 1.5}).
  \end{remark}

  \begin{example}{Sufficient Statistic - Bernoulli Distribution {\textbackslash}w FNF Theorem}
    Let $X\sim\text{Bern}(p)$ with $p\in[0,1]$ unknown and $\x$ be $n$ independent samples of $X$. Note that
    \[\begin{array}{rrl}
      f_X(x)&:=&p^x(1-p)^{1-x}\\
      \implies f_\X(\x)&=&\prod_{i=1}^nf_X(x_i)\text{ by independence of samples}\\
      &=&p^{\sum x_i}(1-p)^{n-\sum x_i}
    \end{array}\]
    Note that we can factorise $f_\X(\x)$ as $f_\X(\x)=h(\X)g(\sum X_i|p)$ where
    \[\begin{array}{rcl}
      h(\X)&:=&1\\
      g(\sum X_i|p)&:=&p^{\sum X_i}(1-p)^{n-\sum X_i}\\
      \Leftrightarrow g(T(\X)|p)&=&p^{T(\X)}(1-p)^{n-T(\X)}\text{ where }T(\X):=\sum X_i
    \end{array}\]
    Notice that $h(\cdot)$ is independent of the unknown parameter $p$ and that $g(\cdot|p)$ only interacts with $p$ through the summary statistic $T(\X)$. Thus by the \textit{Fisher-Neyman Factorisation Theorem} $T(\X):=\sum X_i$ is a sufficient statistic for $p$.
  \end{example}

  \begin{example}{Sufficient Statistic - Gaussian Distribution {\textbackslash}w FNF Theorem}
    Let $X\sim\text{Normal}(\mu,\sigma^2_0)$ where $\mu\in\reals$ is unknown and $\sigma^2_0\in\reals$ is known, and $\x$ be $n$ independent observations of $X$. Note that
    \[\begin{array}{rrl}
      f_X(x)&:=&\frac1{\sqrt{2\pi\sigma^2_0}}\exp\left\{-\frac1{2\sigma_0^2}(x-\mu)^2\right\}\\
      \implies f_\X(\x)&=&\prod_{i=1}^nf_X(x_i)\text{ by independence of samples}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-\mu)^2\right\}
    \end{array}\]
    Consider the following formulation of the distribution of $\X$ with an arbitrary term $t$ introduced
    \[\begin{array}{rcl}
      f_\X(\x,t)&=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i+t-t-\mu)^2\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum((x_i-t)-(\mu-t))^2\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum\left[(x_i-t)^2+(\mu-t)^2-2(\mu-t)(x_i-t)\right]\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum(\mu-t)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum-2(\mu-t)(x_i-t)\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-t)^2\right\}\cdot\exp\left\{-\frac{-2(\mu-t)}{2\sigma_0^2}\sum(x_i-t)\right\}\\
      &=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-t)^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-t)^2\right\}\cdot\exp\left\{-\frac{-2(\mu-t)}{2\sigma_0^2}\left[\left(\sum x_i\right)-nt\right])\right\}\\
    \end{array}\]
    The third exponential disappears if $t:=\frac1nx_i$ (the sample mean). Define $T(\X):=\frac1n\sum X_i$, thus
    \[ f_\X(\x)=(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(x_i-T(\x))^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\x))^2\right\} \]
    Note that we can factorise this expression $f_\X(\x)$ as ${f_\X(\x)=h(\X)g(T(\X)|\mu)}$ where ${T(\X):=\frac1n\sum X_i}$ and
    \[\begin{array}{rcl}
      h(\X)&=&(2\pi\sigma_0^2)^{-n/2}\exp\left\{-\frac1{2\sigma_0^2}\sum(X_i-T(\X))^2\right\}\\
      g(T(\X)|\mu)&=&\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-T(\X))^2\right\}
    \end{array}\]
    Notice that $h(\cdot)$ is independent of the unknown parameter $\mu$ and that $g(\cdot|\mu)$ only interacts with $\mu$ through the summary statistic $T(\X)$. Thus by the \textit{Fisher-Neyman Factorisation Theorem} $T(\X):=\frac1n\sum X_i$ is a sufficient statistic for $\mu$.
  \end{example}

\subsection{Bayesian Sufficiency}

  \begin{definition}{Bayesian Sufficiency\cite{wiki:Sufficient_statistic}}
    In a Bayesian Setting, a summary statistic $s(X)$ of $X\sim f(\cdot;\theta)$ is sufficient if for (almost) all $x\in X$
    \[ \prob(\theta|X=x)=\prob(\theta|s(X)=s(x)) \]
    ie the posterior for $\theta$ given the true model $X$ is the same as the posterior for $\theta$ given the summary statistic $s(X)$ (for almost all $x\in X$).
  \end{definition}

\subsection{$\epsilon$-Approximate Sufficiency}

  \begin{remark}{Approximate Sufficiency}
    Approximate sufficiency concerns sets of summary statistics, rather than a single statistic. We are looking for a set of statistics which are approximate sufficient, while no individual statistic in the set is.
    \par This can be used when the distribution is not explicitly known (and can only be approximated)?
  \end{remark}

  \begin{remark}{Motivation for using Approximate Sufficiency}
    \everymath={\displaystyle}
    Suppose we have a set of summary statistics $\{s_1(\cdot),\dots,s_n(\cdot)\}$ for parameter $\theta$ and are consider adding a new statistic $s_{n+1}(\cdot)$. We only want to do this if$s_{n+1}(\cdot)$ offers a substantial amount of new information about $\theta$.
    \[ \prob(s_{1:n+1}(X)|\theta)\gg\prob(s_{1:n}(X)|\theta) \]
    Note that
    \[\begin{array}{rcl}
      \prob(s_{1:n+1}(X)|\theta)&=&\prob(s_{n+1}(X)|s_{1:n}(X),\theta)\prob(s_{1:n}|\theta)\\
      \implies\prob(\theta|s_{1:n+1}(X))&=&\frac{\prob(s_{n+1}(X)|s_{1:n}(X),\theta)\prob(s_{1:n}(X)|\theta)\prob(\theta)}{\prpb(s_{n+1}(X)|s_{1:n}(X))\prob(s_{1:n}(X))}\text{ by Bayes}\\
      &=&\frac{\prob(s_{1:n}(X)|\theta)\prob(\theta)}{\prob(s_{1:n}(X))}\text{ if }\prob(s_{n+1}(X)|s_{1:n}(X))=\prob(s_{n+1}|s_{1:n}(X),\theta)\footnotemark
    \end{array}\]\footnotetext{i.e. If $s_{1:n}(\cdot)$ are sufficient for $\theta$.}
    We don't want to add $s_{n+1}(\cdot)$ if $s_{1:n}(\cdot)$ are already sufficient for $\theta$.
    \par As it is unlikely for $s_{1:n}(\cdot)$ to be completely sufficient, especially in high-dimensional systems\footnote{and as we are using empirical observation + stocastic noise.} so we don't add $s_{n+1}$ if $s_{1:n}$ are \textit{Approximately Sufficient}.
  \end{remark}

  \begin{remark}{Motivation for Using Log-Likelihood}
    Consider the following
    \[\begin{array}{rcl}
      \prob(s_{1:n+1}(X)|\theta)&=&\prob(s_{n+1}(X)|s_{1:n}(X),\theta)\prob(s_{1:n}|\theta)\\
      \implies\ln\prob(s_{1:n+1}(X)|\theta)&=&\ln\prob(s_{n+1}(X)|s_{1:n}(X),\theta)+\ln\prob(s_{1:n}|\theta)
    \end{array}\]
    This means $\ln\prob(s_{n+1}(X)|s_{1:n}(X),\theta)$ is the only difference between $\ln\prob(s_{1:n}|\theta)$ and $\ln\prob(s_{1:n+1}|\theta)$. Thus, the smaller the value of $\ln\prob(s_{n+1}(X)|s_{1:n}(X),\theta)$ the closer the two likelihoods are, implying $s_{1:n}$ and $s_{1:n+1}$  provide very similar amounts of information\footnote{Meaning there is little to be gained by including $s_{n+1}(\cdot)$.}
  \end{remark}

  \begin{definition}{Score $\delta_{n+1}$\cite{ABC_Approximately_Sufficient_Statistics}}
    Let $\{T_1,\dots,T_n\}$ be a set of summary statistics for parameters $\theta$ and $T_{n+1}$ be a statistic which we consider adding to the set.
    \par The \textit{Score} $\delta_{n+1}$ of the new statistic $T_{n+1}$ relative to the set $\{T_1,\dots,T_n\}$ is defined as
    \[ \delta_{n+1}:=\sup_\theta\ln\big[\prob(T_{n+1}|T_1,\dots,T_n,\theta)\big]-\inf_\theta\ln\big[\prob(T_{n+1}|T_1,\dots,T_n,\theta)\big] \]
    \textit{Score} $\delta_{n+1}$ is a measure of how much new information $T_{n+1}$ introduces
  \end{definition}

  \begin{definition}{$\epsilon$-Approximate Sufficiency \cite{ABC_Approximately_Sufficient_Statistics}}
    Let $\{T_1,\dots,T_n\}$ be a set of summary statistics for parameter $\theta$ and $T_{n+1}$ be a statistic which we consider adding to the set.
    \par The set $\{T_1,\dots,T_n\}$ is \textit{$\epsilon$-Sufficient} to the new statistic $T_{n+1}$, if the score of $T_{n+1}$ relative to $\{T_1,\dots,T_n\}$ is no-greater than some $\epsilon$
    \[ \delta_{n+1}\leq\epsilon \]
    If $T_{n+1}=\X$, the whole data set and $\epsilon=0$ then this is the same as the definition for sufficiency.
  \end{definition}

  \begin{definition}{Odds-Ratio $R_n(\theta)$}
    Let $s_{1}(\cdot),\dots,s_n(\cdot)$ be a set of summary statistics and $\pi_0(\theta)$ be a prior for $\theta$. Then the \textit{Odds-Ratio} for $s_n$ to $s_{1:n}$ is defined as
    \[ R_n(\theta):=\frac{\prob(\theta|s_{1:n}(X))}{\prob(\theta|s_{1:n-1}(X))} \]
    We are able to estimate this quantity from simulation.
  \end{definition}

  \begin{theorem}{}
    Let $s_{1}(\cdot),\dots,s_n(\cdot)$ be a set of summary statistics, $\pi_0(\theta)$ be a prior for $\theta$ and $\delta_n$ be the score of $s_n(\cdot)$ relative to $s_{1:n-1}(\cdot)$. Then
    \[ e^{-\delta_n}\leq R_n(\theta)\leq e^{\delta_n} \]
    Thus, the closer the score $\delta_n$ is to $0$ the closer in value the two likelihoods $\prob(\theta|s_{1:n}(X))$ and  $\prob(\theta|s_{1:n-1}(X))$ are (the more sufficient $s_{1:n-1}$ are for $s_{1:n}$).
  \end{theorem}

  \begin{proof}{Theorem 2.2}
    \everymath={\displaystyle}
    Consider the numerator
    \[\begin{array}{rcl}
      \prob(\theta|S_{1:k})&=&\frac{\prob(\theta,S_{1:k})}{\prob(S_{1:k})}\\
      &=&\frac{\prob(\theta,S_{1:k}|\theta)\pi(\theta)}{\int\prob(S_{1:k}|\theta)\pi(\theta)d\theta}\\
      &=&\frac{\prob(S_k|S_{1:k-1},\theta)\prob(S_{1:k-1}|\theta)\pi(\theta)}{\int\prob(S_k|S_{1:k-1},\theta)\prob(S_{1:k-1}|\theta)\pi(\theta)d\theta}\\
      &\leq&\frac{\prob(S_{1:k-1}|\theta)\pi(\theta)}{\int\prob(S_{1:k-1}|\theta)\pi(\theta)d\theta}\cdot\frac{\sup_\theta\prob(S_k|S_{1:k-1},\theta)}{\inf_\theta\prob(S_k|S_{1:k-1},\theta)}\\
      &=&\frac{\prob(S_{1:k-1}|\theta)\pi(\theta)}{\prob(S_{1:k-1})}\cdot\exp\left\{\ln\left(\frac{\sup_\theta\prob(S_k|S_{1:k-1},\theta)}{\inf_\theta\prob(S_k|S_{1:k-1},\theta)}\right)\right\}\\
      &=&\underbrace{\prob(\theta|S_{1:k-1})}_\text{Bayes' Rule}\cdot\exp\left\{\sup_\theta\ln\left(\prob(S_k|S_{1:k-1},\theta)\right)-\inf_\theta\ln\left(\prob(S_k|S_{1:k-1},\theta)\right)\right\}\\
      &=&\prob(\theta|S_{1:k-1})\cdot\exp\left\{\delta_k\right\}\\
      \implies R_k(\theta)&\leq&\exp\{\delta_k\}
    \end{array}\]
    A symmetric argument is made for the other bound.
  \end{proof}

  \begin{remark}{Adding Statistics}
    Consider a large set of statistics $T:=\{T_1,\dots,T_n\}$. To find a sufficient subset $T'$ of $T$ the following process can be used
    \begin{enumerate}
      \item Define $T'=\emptyset$.
      \item Calculate the score of each
      \item Let $\delta_\text{max}=\max_{t\in T}\text{Score}(t,T')$.
      \item Let $T_\text{max}=\argmax_{t\in T}\text{Score}(t,T')$.
      \item If $(\delta_\text{max}>\epsilon)$:
      \begin{itemize}
        \item $T'=T'\cup T_\text{max}$.
      \end{itemize}
      \item Repeat ii)-v) until no statistics have a score greater than $\epsilon$.
    \end{enumerate}
    This approach is deterministic wrt which statistics end up in the final $T'$. Another, stochastic, approach is to uniformly at random at one of the statistics with a score greater than $\epsilon$ to $T'$ each iteration.
  \end{remark}

  \begin{proposition}{Algorithm for Choosing Summary Statistics}
    Here is an algorithm for finding a set of approximately-sufficient summary statistics $S'$ from a larger set of summary statistics $S$.
    \begin{itemize}
      \item \textit{Initialisation} - Define a set of summary statistics $S:=\{s_1(\cdot),\dots,s_n(\cdot)\}$ and observe $y_{obs}$ from the true model.
      \item Calculate observed values for each summary statistic
      \[ S_{obs}:=\{s_1(y_{obs}),\dots,s_n(y_{obs})\} \]
    \end{itemize}
  \end{proposition}

\section{Non-Linear Projection}

\section{Do we need Summary Statistics?}

\subsection{Minimum Distance ABC}

\newpage
\bibliographystyle{unsrt}
\bibliography{aside_bib}


\end{document}
