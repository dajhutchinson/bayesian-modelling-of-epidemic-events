\documentclass[11pt,a4paper]{article}

\usepackage[margin=2.5cm, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage[vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath,amsthm,amsfonts,bbm,environ,fancyhdr,graphicx,hyperref,natbib,tikz,mdframed}
\usepackage[hang,flushmargin]{footmisc}
\usepackage[nottoc]{tocbibind}
\usetikzlibrary{automata,positioning}
\graphicspath{ {img/} }
% \usepackage[section,nohyphen]{DomH}
% \headertitle{Bayesian Modelling of Epidemic Processes}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\setlength\parindent{5ex}
\allowdisplaybreaks

% space between paragraphs
\setlength{\parskip}{.3\baselineskip}

\setcitestyle{square}

\newcommand{\algorithmfootnote}[2][\footnotesize]{%
  \let\old@algocf@finish\@algocf@finish% Store algorithm finish macro
  \def\@algocf@finish{\old@algocf@finish% Update finish macro to insert "footnote"
    \leavevmode\rlap{\begin{minipage}{\linewidth}
    #1#2
    \end{minipage}}%
  }%
}

% Footnote numbering style
\renewcommand{\thefootnote}{[\arabic{footnote}]}
\newcommand*{\indexed}{\mathbbm{1}}
\newcommand*{\prob}{\mathbb{P}}
\newcommand*{\expect}{\mathbb{E}}
\newtheoremstyle{break}
  {\topsep}{\topsep}%
  {\itshape}{}%
  {\bfseries}{}%
  {\newline}{}%
\theoremstyle{break}
\newtheorem{theorem}{Theorem}

\newmdtheoremenv{box_algorithm}{Algorithm}[section]
\newmdtheoremenv{box_definition}{Definition}[section]
\newmdtheoremenv{box_example}{Example}[section]
\newmdtheoremenv{box_remark}{Remark}[section]
\newmdtheoremenv{box_theorem}{Theorem}[section]

\begin{document}

\title{Bayesian Modelling of Epidemic Processes}
\author{D. Hutchinson}
\date{\today}
\clearpage\maketitle
\thispagestyle{empty}

\newpage
\setcounter{page}{1}
\pagenumbering{Roman}

\section*{Dedication}\label{sec_dedication}
% Me, I'm taking this one. And Chris Lovasz whose streams kept me passively entertained during this project.
% \newpage

\section*{Accompanying Resources}\label{sec_accompanying_resources}
% % TODO link to github for code and notebooks
% \newpage

\newpage
\section*{Abstract}\label{sec_abstract}

\newpage
\tableofcontents

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Introduction}\label{sec_introduction}

  \par What is a model? A (simple) mathematical formulation of a process which incorporates parameters of interest and likely some stochastic processes. Models need to be computational tractable (i.e. fairly simple)
  \par ``All models are wrong, some are useful''.
  \par What to use models for? check intuition, explanation \& prediction.
  \par What is ``posterior estimation''?
  \par The problem - Posterior estimation when likelihood is intractable. ``Likelihood-free'' estimation. (Classical example of determining most recent common ancestor of two DNA strands. Likelihood is intractable due to number of branches growing factorially. (\cite[]{selecting_summary_stats_in_ABC_for_calibration})

\subsection*{Motivation}\label{sec_motivation}

  \par What is bayesian inference
  \par Bayes Rule? Describe each component \& why is likelihood intractable?
  \par Why now? More, better data. Greater computational power.
  \par What can posterior be used for?
  \par Generative models?

\subsection*{Motivating Examples}\label{sec_motivating_examples}

  \par DNA mutation (\cite[]{modern_computational_approaches_for_analysing_molecular_genetic_variation_data})

\subsection*{History}\label{sec_history}

  \par Traditional parameter estimation methods - ``Maximum Likelihood''.
  \par Neutrality testing - (Hypothesis testing), compare results against a null hypothesis for a parameter value.

\subsection*{Successful Applications of these Methods}\label{sec_successful_applications}

\newpage
\section{Bayesian Modelling}

  \par Bayes' Rule
  \par Bayes rule allows for explanation of relationships in data (rather than just inferences)
  \par Define Bayesian inference
  \par vs. Frequentist modelling
  \par Stochastic vs deterministic models
  \par Consistency
  \par In general, we never know if our calculated posterior is actually close to the true posterior.
  \par Typically only have access to one set of observations (e.g. time-series of covid cases)
  \par Prior encodes assumptions/knowledge so reduces variance but can introduce bias

  \begin{box_theorem}[Bayes' Rule]\label{the_bayes_rule}
    Consider two random variables $X$ and $Y$. Bayes' Rule provides a formulation for the conditional distribution of $A$ given $B$.
    \[ \prob(Y|X)=\frac{\prob(X|Y)\prob(Y)}{\prob(X)} \]
    where each component is known as
    \begin{itemize}
      \item $\prob(Y|X)$, the Posterior of $Y$ given variable $X$.
      \item $\prob(X|Y)$, the Likelihood of $Y$ given fixed event $X$.
      \item $\prob(X)$, the prior distribution of $X$.
      \item $\prob(Y)$, the evidence for fixed event $Y$.
    \end{itemize}
    \begin{proof}
      Bayes' rule is follows from the definition of conditional distributions and joint distributions
      \everymath={\displaystyle}
      \[\begin{array}{rcl}
        \prob(Y|X)&=&\frac{\prob(X,Y)}{\prob(Y)}\\
        &=&\frac{\prob(Y|X)\prob(X)}{\prob(Y)}
      \end{array}\]
    \end{proof}
  \end{box_theorem}

\subsubsection*{Priors}

\subsection{SIR Model}

\newpage
\section{Approximate Bayesian Computation}\label{sec_ABC}

    In this section I motivate and provide the mathematical background for Approximate Bayesian Computation (ABC) methods \textit{Section \ref{sec_ABC_background}}; Present the general approach of ABC methods \textit{Section \ref{sec_ABC_ABC_methods}} and discuss four flavours of ABC algorithm \textit{Section \ref{sec_abc_rejection_sampling}-\ref{sec_abc_smc}}; Provide a comparison of these four methods \textit{Section \ref{sec_abc_comparison}}; and, I close this section by exploring how ABC methods can be used for model choice \textit{Section \ref{sec_abc_model_choice}} and how regression adjustment can be used to improve the results of ABC methods \textit{Section \ref{sec_abc_regression_adjustment}}.

\subsection{Motivation and Background}\label{sec_ABC_background}

  \par Consider a model $X$ with parameters $\theta$. The centre-point of Bayesian inference is the posterior distribution $\prob(\theta|X)$ for the parameters $\theta$ given observations $X$. Using Bayes rule we have the following formulation for this posterior .

  \[ \prob(\theta|X)=\frac{\prob(X|\theta)\prob(\theta)}{\prob(X)} \]

  \par For Bayesian inference we are only concerned with the relative weight the posterior assigns to each parameter value $\theta$, so we can discard the evidence $\prob(X)$ as it is just a normalising constant with respect to $\theta$. Meaning we can simplify the expression for the posterior as being proportional to the product of the likelihood $\prob(X|\theta)$ and the prior $\prob(\theta)$.

  \[ \prob(\theta|X)\propto\prob(X|\theta)\prob(\theta) \]

  \par As the prior is defined by the user, the only remaining task is to deduce an expression for the likelihood. However, for most real-world processes an explicit expression of the likelihood is computationally intractable due to the complex nature of the systems which govern them and their high degrees of freedom. Moreover, there are often so many parameters that it is intractable to specify all of them and thus we generally theorise a simpler model $\hat{X}$ and seek to calibrate this model to the true model by fitting its parameters. This motivates the need for likelihood-free inference methods such as Approximate Bayesian Computation.

  \begin{center}
    \noindent\rule{.8\textwidth}{0.4pt}
  \end{center}

  \noindent Suppose you have a sequence of $n$ of observations $x_{obs}:=(x_{obs,1},\dots,x_{obs,n})$ from our model $X$ where each observation may be multi-dimensional, $x_{obs,i}\in\mathbb{R}^p$ for $p\in\mathbb{N}$. Let $K_\varepsilon(\cdot)$ denote a kernel density function with bandwidth $\varepsilon>0$ and $\|\cdot\|$ denote a distance measure between observations of model $X$. I discuss kernel density functions and distance measures in \textit{Section \ref{sec_ABC_ABC_methods}}. Note that as the bandwidth tends to zero the value of the kernel density function for the distance between two points $K_\varepsilon(\|x-x_{obs}\|)$ tends to the Dirac delta function $\delta_{x_{obs}}(x)$. This result is trivially from the definition of a kernel density function.

  \begin{equation}
    \lim_{\varepsilon\to0}K_{\varepsilon}(\|x-x_{obs}\|)=\delta_{x_{obs}}(x):=\begin{cases}1&\text{if }x_{obs}=x\\0&\text{otherwise}\end{cases} \label{eqn_limit_of_kernel_and_distance}
  \end{equation}

  \par This result can be used to restate the likelihood function in terms of a kernel density function and distance measure.

  \[
    \everymath={\displaystyle}
    \begin{array}{rcl}
      \prob(x_{obs}|\theta)&=&\int\delta_{x_{obs}}(x)\prob(x|\theta)dx\\
      &=&\lim_{\varepsilon\to0}\int K_\varepsilon(\|x-x_{obs}\|)\prob(x|\theta)dx
    \end{array}
  \]

  \par Consider the following definition $\pi_{ABC}$ and note that it tends to, within a normalising constant, of the true posterior.

  \[
  \everymath={\displaystyle}
  \begin{array}{rrrl}
    &\pi_{ABC}(\theta|x_{obs})&:=&\int K_\varepsilon(\|x-x_{obs}\|)\prob(x|\theta)\pi_0(\theta)dx\\
    \implies&\lim_{\varepsilon\to0}\pi_{ABC}(\theta|x_{obs})&=&\lim_{\varepsilon\to0}\int K_\varepsilon(\|x-x_{obs}\|)\prob(x|\theta)\pi_0(\theta)dx\\
    &&=&\int \delta_{x_{obs}}(x)\prob(x|\theta)dx\cdot \pi_0(\theta)\\
    &&=&\prob(x_{obs}|\theta)\pi_0(\theta)\\
    &&\propto&\prob(\theta|x_{obs})
  \end{array}\]

  \noindent This shows that $\pi_{ABC}$ is an approximation of the true posterior, with it being a good approximation when $\varepsilon$ is small.
  \par Typically, due to the observations $x$ being of high dimension, a summary statistic $s(\cdot)$ is applied to them first and then the quantities $s:=s(x),s_{obs}:=(x_{obs})$ are used in place of $x,x_{obs}$. The analysis of the above derivation is unchanged when using summary statistics as long as the summary statistics are sufficient, if the summary statistics are not sufficient then $\pi_{ABC}$ can only ever be an approximation of the true posterior regardless of the bandwidth used. \textit{Section \ref{sec_summary_stats}} is dedicated to the topic of how to approach choosing summary statistics, with sufficiency being discussed in \textit{Section \ref{sec_sufficiency}}.

  \[ \pi_{ABC}(\theta|s_{obs}):=\int K_\varepsilon(\|s-s_{obs}\|)\prob(s|\theta)\pi_0(\theta)ds \]

  This formulation for the ABC approximation of the posterior is the one found in the standard ABC framework (e.g. \cite[]{overview_of_abc,annual_review_of_statistics_ABC}).

  \par The utility of being able to use $\pi_{ABC}(\theta|s_{obs})$ to approximate the true posterior is apparent when you consider the implied joint distribution of parameters and summary statistics $\pi_{ABC}(\theta,s|s_{obs})$

  \[\everymath={\displaystyle}\begin{array}{rrrl}
    &\pi_{ABC}(\theta|s_{obs})&=&\int \pi_{ABC}(\theta,s|s_{obs})ds\\
    \text{where}&\pi_{ABC}(\theta,s|s_{obs})&:=&K_\varepsilon(\|s-s_{obs}\|)\prob(s|\theta)\pi_0(\theta)
  \end{array}\]

  We can define Monte Carlo algorithms which target sampling from this joint distribution without needing to specify the likelihood $\prob(s|\theta)$. These samples become samples from the posterior by simply ignoring the summary statistic values $s$ which are sampled.

\subsection{ABC Methods}\label{sec_ABC_ABC_methods}

  \par Approximate Bayesian Computation (ABC) methods are a family computational methods which can be used to approximate posteriors for the parameters of models where the likelihood is intractable. The first algorithm to use the concept which would later be known as ABC was presented in \cite[]{inferring_coalescence_times_from_dna_sequence_data}, although this algorithm does not include the use of summary statistics nor use distance measures and kernel density functions to determine whether to accept a simulation or not. The algorithm presented in \cite[]{population_growth_of_human_Y_chromosomes} is much more recognisable as ABC and consider by many as the first true ABC algorithm. This algorithm would later be generalised to becomes the rejection sampling approach to ABC. Both of these papers were studies of population genetics, a field in which ABC is still popular used.

  \par ABC methods require a set of observations from the true model; a theorised model for which parameters can be set and observations generated; and a set of priors for the parameters of the theorised model. ABC methods then perform many simulations of the theorised model and, by comparing  the summary statistic values of the simulated observations to those of the true observations, inferences are made about which parameter values are most likely to be closest to the true values. \textbf{Algorithm \ref{alg_generic_abc}} outlines this basic flow which ABC methods follow. The general idea being that the parameter sets which make the theorised model generate observations which are closest to true observations are more likely to be the true parameter values.

  \begin{box_algorithm}[Generic Approximate Bayesian Computation]\label{alg_generic_abc}
    \textbf{Require:} Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Acceptance Kernel $K_\varepsilon(\cdot)$; Distance Measure $\|\cdot\|$.
    \begin{enumerate}
      \item Calculate summary statistic values $s_{obs}=s(x_{obs})$.
      \item Until stopping condition reached:
      \begin{enumerate}
        \item Sample a set of parameters $\tilde\theta$.
        \item Run the theorised model with sampled parameter $\tilde{x}=f\tilde\theta(X|\tilde\theta)$.
        \item Calculate summary statistic values $\tilde{s}=s(\tilde{x})$.
        \item Accepted parameters $\tilde\theta$ with probability $K_\varepsilon(\|\tilde{s}-s_{obs}\|)$.
      \end{enumerate}
      \item Return all accepted parameter sets $\hat\Theta$.
    \end{enumerate}
  \end{box_algorithm}

  \par \textbf{Algorithm \ref{alg_generic_abc}} demonstrates the simplicity of the underlying algorithm for ABC methods. Most ABC methods are straightforward to implement as they follow this basic structure and then change how certain parts of performed in practice (Typically how new samples are drawn and how the acceptance criteria are defined). This allows for a high level of modularity which has motivated innovations in ABC methods.

  \par The ideal ABC methods are those which run efficiently and perform well with small bandwidths $\varepsilon$. Efficient methods are important as this means more simulations can be processed in a given time-period, making convergence of the estimated posterior more likely. A method being able to handle smaller bandwidths means the posterior it produces will be a better approximation of the true posterior (See Eq. \ref{eqn_limit_of_kernel_and_distance}). All ABC methods will run with any value of the bandwidth, however those that use an informed search method for generating samples will require fewer simulations to achieve good results (e.g. ABC-SMC).

  \par Monte Carlo methods form the basis of how ABC methods approach exploring the parameter space. Monte Carlo methods are a class of methods which seek to generate samples from a space in a way which mimics sampling from the true model. They do this by running many, many simulations and use some degree of randomness to determine how each simulation is generated and which are accepted.

  \par Here is an overview of classes of Monte Carlo methods which are commonly used in ABC methods:
  \begin{itemize}
    \item \textit{Rejection-Sampling methods} calculate a probability $p$ that a given set of simulated values came from the true model. A value $u\sim U[0,1]$ is sampled from standard uniform distribution and if the sampled value $u$ is less than the acceptance-probability $p$ then the simulation is accepted as a sample. This procedure is run on a large number of simulations with each simulation being generated and assessed independently.

    \item \textit{Importance-Sampling methods} extend rejection-sampling by, instead of only accepting a subset of simulated values, all simulations are accepted but each is assigned a weight which indicates the perceived probability that that simulation could be generated by the true model. Typically this weight is the same as the acceptance probability $p$ calculated in rejection-sampling.

    \item \textit{Markov Chain Monte Carlo (MCMC) methods} extend rejection-sampling by, instead of generating each simulation independently, the parameters of the last accepted simulation are slightly perturbed and then used to generate a new simulation. This creates a search process rather than random simulation due to the dependency between consecutive samples.

    \item \textit{Sequential Monte Carlo (SMC) methods}\footnote{Also known as Particle-Filter methods.} extend importance-sampling by repeatedly resampling from the set of samples, with the weights of each parameter determining the probability it is sampled, and each iteration tightening the acceptance criteria. This means the estimated posterior will become more refined each time and hopefully converge on the true posterior.
  \end{itemize}

  \par The use of Monte Carlo methods means that ABC methods are inherently computationally inefficient due to the need to perform many, many simulations to % TODO
  . Further, Monte Carlo methods introduce a high degree of randomness into ABC methods which further motivates the need to perform lots of simulations as the strong law of large number is required to obtain consistent results. This limitation is mitigated due to the simplicity of most ABC algorithms meaning they are capable of process millions of simulations an hour on modern computers.

  \par The set of accepted parameter sets $\hat\Theta$ returned by ABC can be used for Bayesian inference. Estimating properties of the distributions, such as mean, mode and quantiles, is straightforward. Producing a discretised estimate of the posterior for each parameter can be achieved by calculating a histogram of the accepted values for each parameter, again straightforward. Kernel density functions can be used to produce a continuous estimates of the posteriors (See \cite[]{review_of_kde}).

  \begin{box_remark}[Posterior Mean is Minimum Mean-Square Error Estimator]\label{rem_posterior_mean_is_mmsee}
    Let $\theta$ denote the quantity we wish to estimate, $A$ denote an arbitrary estimator of $\theta$ and suppose we have observed $x_{obs}$ from model $X$. Then
    \[\begin{array}{rrcl}
      &MSE_\theta(A)&=&\expect\left[(\theta-A)^2|X=x_{obs}\right]\\
      &&=&\expect\left[\theta^2-2A\theta+A^2|X=x_{obs}\right]\\
      &&=&\expect\left[\theta^2|X=x_{obs}\right]-2A\expect\left[\theta|X=x_{obs}\right]+A^2\\
      \implies&\frac{\partial}{\partial A}MSE_\theta(A)&=&-2\expect[\theta|X=x_{obs}]+2a\\
      \implies&a&=&\expect[\theta|X=x_{obs}]
    \end{array}\]
    This shows that mean-square error is minimised when the posterior mean of $\theta$ given $x_{obs}$ is used as an estimator.
  \end{box_remark}

  \par ABC methods are commonly used to calibrate models or to compare models. Typically calibration is done by setting parameter values to the estimated posterior mean as the posterior mean minimises mean-square error (see \textbf{Remark \ref{rem_posterior_mean_is_mmsee}}). ABC methods are used for model comparison as they can directly estimate Bayes factor, I discuss model comparison further in \textit{Section \ref{sec_abc_model_choice}}.

  \par The key advantage of ABC methods, over other approaches to Bayesian inference, is that it produces a distribution, rather than a point-estimate, for parameter values. This allows for analysis into uncertainty around the parameter values. Additionally, as the strictness of the acceptance criteria is a parameter of ABC methods, ABC methods can fit or compare a large range of theorised models by loosening the acceptance criteria. Being able to use simpler models has the advantage of reducing issues which occur due to curse-of-dimensionality.

  \par A limitation of using ABC methods is the large number of hyper-parameters they have (Distance measures, summary statistics, bandwidths, perturbance kernels, etc.) and that the choices the user makes for how these parameters are set can drastically affect the algorithms performance. It is trivial to realise that if an uninformative distance measure such as $\|x\|=0\ \forall\ x$ is used or an acceptance kernel which accepts all simulations is used then the returned set of parameters will resemble the set of priors, and no meaningful inferences can be drawn. Moreover, these hyper-parameters need to be tuned for each model these methods are applied, which is laborious. This has motivated the innovation of adaptable ABC algorithms which automate the process of setting some of these parameters.

  \par As stochastic processes determine whether a simulation is accepted, or not, ABC methods incur information loss. This can mean that promising areas of the parameter space are not explored. This issue is mitigate by running many simulations.

  % TODO trade-off of low v high bandwidth (time v accuracy basically) no obvious relationship between the two
  % TODO curse of dimensionality

\subsubsection*{Summary Statistics}

  \par See \textit{Section \ref{sec_summary_stats}}.

\subsubsection*{Kernel Density Functions}

  \begin{box_definition}[Kernel Density Functions $K_\varepsilon(\cdot)$, \cite{non_parameteric_estimation_of_a_multivariate_probability_density}]
    Kernel density functions are functions $K:\mathbb{R}\to\mathbb{R}$ with the following properties:
    \begin{enumerate}
      \item Non-negative
      \[ K_\varepsilon(x)\geq0\ \forall\ x\in\mathcal{X} \]
      where $\mathcal{X}$ is the range of values $x$ can take.
      \item Symmetric
      \[ K_\varepsilon(x)=K_\varepsilon(-x)\ \forall\ x\in\mathcal{X} \]
      \item Normalised
      \[ \int_{\mathcal{X}}K_\varepsilon(x)dx=1 \]
      \item $K_\varepsilon(x)=\frac1\varepsilon K_1(x/\varepsilon)$.
    \end{enumerate}
    \noindent Kernel density functions are typically extended to allow for a smoothing parameter $\varepsilon\geq0$ such that $K_\varepsilon(x)=\frac1\varepsilon K(x/\varepsilon)$.
  \end{box_definition}

  \par The choice of kernel density function does not play a notable role in the asymptotic behaviour of ABC methods, however the bandwidth chosen for them does. A high bandwidth means that the weight of the kernel is spread much more evenly across its support meaning there is less discrimination between values close to the mean and those further away.

  \par It is standard to define kernel density functions such that they have zero mean. Having this property means that that $\max_xK_\varepsilon(x)=K_\varepsilon(0)$, this follows immediately from the kernel being symmetric. This is a useful property in the context of ABC methods as we pass the distance between two points $\|x-x_{obs}\|$ to the kernel density function to determine the probability we accept a simulation and this property means that simulations $x$ closest to the observed values $x_{obs}$ are more likely to be accepted.

  \par In practice, when implementing ABC methods we typically scale up the values returned by the kernel such that $K_\varepsilon(0)=1$. This is straightforward to do for well-known kernels as it only requires the removal of the normalising term. As the relative weights given to each value are maintained this does not affect the asymptotic behaviour of the algorithms, but will increase the acceptance rate. This also has the desirable effect that every time an exact match is found it will definitely be accepted.

  \begin{table}
    \everymath={\displaystyle}
    \centering
    \begin{tabular}{|l|l|}
      \hline
      \textbf{Name}&\textbf{Formula}\\\hline
      Uniform Kernel&$K_\varepsilon(x)=\frac1{2\varepsilon}\mathbbm{1}\{x\leq\varepsilon\}$\\\hline
      Gaussian Kernel&$K_\varepsilon(x)=\frac\varepsilon{\sqrt{2\pi}}\exp\left\{-\frac12x^2\varepsilon^2\right\}$\\\hline
      Epanechnikov Kernel&$K_\varepsilon(x)=\frac34\left(1-x^2\varepsilon^2\right)\mathbbm{1}\left\{|x|\leq\varepsilon\right\}$\\
      \hline
    \end{tabular}
    \caption{Common kernel density functions for ABC methods.\protect\footnotemark}
    \label{tab_common_kernels}
  \end{table}
  \footnotetext{$\mathbbm{1}\{A\}:=\begin{cases}1&\text{if }A\\0&\text{otherwise}\end{cases}$}

  \par \textbf{Table \ref{tab_common_kernels}} provides a table of the most commonly used kernel density functions for ABC, as recommended by \cite[]{annual_review_of_statistics_ABC}. The Epanechnikov kernel is asymptotically optimal for kernel density estimation when seeking to minimise mean-square error (See \cite[]{non_parameteric_estimation_of_a_multivariate_probability_density}), although this theoretical result is disputed in \cite[]{introduction_to_nonparametric_estimation}.

  \par The uniform kernel is popular in ABC as it is equivalent to accepting all simulations whose distance from the true observation is no greater than $\varepsilon$. The Gaussian kernel is more commonly used as it has an infinite support which is useful in certain scenarios. When choosing which kernel to use for ABC methods it is intuitive that it should match the theorised distribution of the noise in the theorised model. This further motivates the use of a Gaussian kernel as many models assume gaussian noise.

\subsubsection*{Distance Measures}

  \begin{table}
    \everymath={\displaystyle}
    \centering
    \begin{tabular}{|l|l|}
      \hline
      \textbf{Name}&\textbf{Formula}\\\hline
      Manhattan Distance&$L_1(\mathbf{x}):=\sum_{i=1}^m|x_i|$\\\hline
      Euclidean Distance&$L_2(\mathbf{x}):=\sqrt{\sum_{i=1}^mx_i^2}$\\\hline
      $L_p$ Norm&$L_p(\cdot):=\left(\sum_{i=1}^mx_i^p\right)^{1/p}$\\\hline
      $L_\infty$ Norm&$L_\infty(\mathbf{x}):=\max\{x\in \mathbf{x}\}$\\\hline
    \end{tabular}
    \caption{Common distance measures for ABC methods.}
    \label{tab_common_distances}
  \end{table}

  \par Distance measures quantify how far apart two multi-dimensional vectors are from each other, with greater values indicating the vectors are further away. A value of zero means that the two vectors are identical under the given measure.

  \par The choice of distance measure is integral to the performance of an ABC method as it determines whether a set of simulated values are deemed to be representative of the true model, or not, by quantifying how similar these values are to true observations. \textbf{Table \ref{tab_common_distances}} provides a list of popular distance measures for ABC methods. The Euclidean distance is most commonly as minimising Euclidean distance is clearly related to minimise SSE of a model.

  \par It is important to note that the value passed to distances measures in ABC methods is the difference of two sets of summary statistics. Well chosen summary statistics will extract meaningful information and perform a certain level of pre-processing of this data such as standardisation and weighting different dimensions. This means we do not need to consider these problems during selection of distance measure.

  \par An issue which arises when specifying distance measures is the ``Curse of Dimensionality''\footnote{Term first coined in \cite[]{adaptive_control_processes} in reference to how many algorithms may work well when applied to low-dimensions, but are intractabled when for higher-dimensions.}. This is the phenomena that as the dimensionality of vectors being compared increases, it becomes harder to distinguish between different pairs. This is an issue to ABC methods as the success of the approach relies on being able to accurately identify which simulations are closest to observed values. Using summary statistics which introduce a high level of dimensionality reduction will help.

  \par Different demonstrations of this phemonema is required for different distance measures, but for Euclidean distance it is generally demonstrated by comparing of the volume of a hyper-sphere with radius $r$ and the volume of a hyper-cube with side length $2r$. The volume of the hyper-cube quickly dwarfs that of the hyper-sphere as the number of dimensions are increased. % TODO incude mathematical proof (https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)
  The ``Curse of Dimensionality'' is a big short coming of the Euclidean distance as it was only every conceived for real-world spaces (i.e. two or three dimensions). Which distance measure is best ultimately depends on the data being used. Using the $L_p$ norm has shown promise but adds the additional problem of what value of $p$ is optimal. \cite[]{choosing_the_metric_in_high_dimensional_spaces_based_on_hub_analysis} present an approach to choosing an optimal $p$ which assesses the ``hub-ness'' of a dataset.

  \par There is a wealth of literature on the ``Curse of Dimensionality'' in the machine learning space, particularly for nearest-neighbour problems which are very relevant to the problem being addressed by distance measures in ABC (See \cite[]{when_is_nearest_neighbour_meaningful,what_is_the_nearest_neigbour_in_high_dimensional_space,hubs_in_space})

\subsubsection{ABC-Rejection Sampling}\label{sec_abc_rejection_sampling}

  \begin{box_algorithm}[ABC-Rejection Sampling ``Fixed Sample'']
    \begin{algorithm}[H]
      \SetKwIF{With}{}{}{with probability}{}{}{}{}
      \SetKwInOut{Require}{require}
      \Require{Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Prior Distributions $\pi_0(\theta)$; Acceptance Kernel $K_\varepsilon(\cdot)$; Distance Measure $\|\cdot\|$; Target Number $M$.}
      $s_{obs}\leftarrow s(x_{obs})$.\\
      $\tilde\Theta\leftarrow\{\}$.\\
      $t\leftarrow0$.\\
      \While{$t<M$}{
        $\tilde\theta_t\leftarrow$ sample $\pi_0(\theta)$.\\
        $\tilde{x}\leftarrow f(X|\tilde\theta_t)$.\\
        $\tilde{s}\leftarrow s\left(\tilde{x}\right)$.\\
        \With{$K_\varepsilon(\|s_{obs}-\tilde{s}\|)$} {
          $\hat\theta^{(t)}\leftarrow\tilde\theta$.\\
          Add $\hat\theta^{t}$ to $\hat\theta$.\\
          $t\leftarrow t+1$
        }
      }
      \Return{$\tilde\Theta=\left\{\theta^{(1)},\dots,\theta^{(M)}\right\}$}
    \end{algorithm}
  \end{box_algorithm}

  \begin{box_algorithm}[ABC-Rejection Sampling ``Best Samples'']
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Prior Distributions $\pi_0(\theta)$ Distance Measure $\|\cdot\|$; Number of Simulations $M$; Simulations to Accepted $N$.}
      $s_{obs}\leftarrow s(x_{obs})$.\\
      $\tilde\Theta\leftarrow\{\}$.\\
      $t\leftarrow0$.\\
      \For{$i=0,\dots,M$}{
        $\tilde\theta^{(i)}\leftarrow$ sample $\pi_0(\theta)$.\\
        $\tilde{x}^{(i)}\leftarrow f(X|\tilde\theta^{(i)})$.\\
        $\tilde{s}^{(i)}\leftarrow s\left(\tilde{x}^{(i)}\right)$.\\
        Add $\tilde{s}^{(i)}$ to $\tilde\Theta$.
      }
      \Return{$N$ elements with smallest values of $\|s(\tilde{x}^{(i)})-s_{obs}\|$.}
    \end{algorithm}
  \end{box_algorithm}

\subsubsection{ABC-Importance Sampling}\label{sec_abc_importance_sampling}

  % NOTE https://bookdown.org/rdpeng/advstatcomp/importance-sampling.html

  \begin{box_algorithm}[ABC-Importance Sampling]
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Prior Distributions $\pi_0(\theta)$ Distance Measure $\|\cdot\|$; Number of Simulations $M$; Importance Kernel $g(\cdot)$.}
      $s_{obs}\leftarrow s(x_{obs})$.\\
      $\tilde\Theta\leftarrow\{\}$.\\
      $t\leftarrow0$.\\
      \For{$i=0,\dots,M$}{
        $\tilde\theta^{(i)}\leftarrow$ sample $g(\theta)$.\\
        $\tilde{x}^{(i)}\leftarrow f(X|\tilde\theta^{(i)})$.\\
        $\tilde{s}^{(i)}\leftarrow s\left(\tilde{x}^{(i)}\right)$.\\
        $\tilde{w}^{(i)}\leftarrow \frac{\pi_0\left(\theta^{(i)}\right)}{g\left(\theta^{(i)}\right)}K_\varepsilon\left(\left\|s^{(i)}-s_{obs}\right\|\right)$.\\
        Add $\tilde\theta^{(i)}$ to $\tilde\Theta$ with weight $\tilde{w}^{(i)}$.
      }
      \Return{$\tilde\Theta:=\left\{\left(\tilde\theta^{(1)},\tilde{w}^{(1)}\right),\dots,\left(\tilde\theta^{(M)},\tilde{w}^{(M)}\right)\right\}$}
    \end{algorithm}
  \end{box_algorithm}

\subsubsection{ABC-MCMC}\label{sec_abc_mcmc}

  \begin{box_algorithm}[ABC-MCMC]
    \begin{algorithm}[H]
      \SetKwIF{With}{}{Else}{with probability}{}{}{otherwise}{}
      \SetKwInOut{Require}{require}
      \Require{Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Prior Distributions $\pi_0(\theta)$ Distance Measure $\|\cdot\|$; Chain length $M$; Acceptance Kernel $K_\varepsilon(\cdot)$; Perturbance Kernel $K^*(\cdot)$.}
      $s_{obs}\leftarrow s(x_{obs})$.\\
      $\tilde\Theta\leftarrow\{\}$.\\
      \BlankLine
      \# Burn-In Step\\
      \While{$K_\varepsilon\left(\left\|\tilde{s}^{(0)}-s_{obs}\right\|\right)$ is not accepted\label{abc_mcmc_burn_in_start}}{
        $\tilde\theta_0\leftarrow$ sample $\pi_0(\theta)$.\\
        $\tilde{x}^{(0)}\leftarrow f(X|\tilde\theta^{(0)})$.\\
        $\tilde{s}^{(0)}\leftarrow s\left(\tilde{x}^{(0)}\right)$\label{abc_mcmc_burn_in_end}
      }
      \BlankLine
      \# MCMC Step\\
      \While{$t=1,\dots,M$\label{abc_mcmc_mcmc_step_start}}{
        $\theta^*\leftarrow K^*\left(\theta^{(t-1)}\right)$.\\
        $x^*\leftarrow f\left(X|\theta^*\right)$.\\
        $s^*\leftarrow s\left(x^*\right)$.\\
        \With{$K_\varepsilon(\|s^*-s_{obs}\|)$}{
          $\tilde\theta^{(t)}\leftarrow \theta^*$.
        }\Else{
          $\tilde\theta^{(t)}\leftarrow \theta^{(t-1)}$.
        }
        Add $\tilde\theta^{(t)}$ to $\tilde\Theta$.\label{abc_mcmc_mcmc_step_end}
      }
      \Return{$\tilde\Theta:=\left\{\tilde\theta^{(1)},\dots,\tilde\theta^{(M)}\right\}$}
    \end{algorithm}
  \end{box_algorithm}

  \par Lines \ref{abc_mcmc_burn_in_start}-\ref{abc_mcmc_burn_in_end} form the initial burn-in step of the algorithm. Lines \ref{abc_mcmc_mcmc_step_start}-\ref{abc_mcmc_mcmc_step_end} form the MCMC step of the algorithm.

  \par MCMC draw samples from posterior

\subsubsection{ABC-SMC}\label{sec_abc_smc}

  \begin{box_algorithm}[ABC-SMC]
    \begin{algorithm}[H]
      \everymath={\displaystyle}
      \SetKwIF{With}{}{Else}{with probability}{}{}{otherwise}{}
      \SetKwInOut{Require}{require}
      \Require{Observed values $x_{obs}$; Summary statistics $s(\cdot)$; Theorised model $f(X|\cdot)$; Prior Distributions $\pi_0(\theta)$ Distance Measure $\|\cdot\|$; Chain length $M$; Acceptance Kernel $K_\varepsilon(\cdot)$; Set of Bandwidths $\{\varepsilon_0,\dots,\varepsilon_T\}$; Number of Iterations $T$; Perturbance Kernel $K^*(\cdot)$; Sample Size $N$.}
      $s_{obs}\leftarrow s(x_{obs})$.\\
      \BlankLine
      \# Initial Sample Step\\
      $\tilde\Theta_0\leftarrow\{\}$.\\
      $i\leftarrow0$\\
      \While{$i<N$}{
        $\tilde\theta_0^{(i)}\leftarrow$ sample $\pi_0(\theta)$.\\
        $\tilde{x}_0^{(i)}\leftarrow f(X|\tilde\theta_t^{(i)})$.\\
        $\tilde{s}_0^{(i)}\leftarrow s\left(\tilde{x}_t^{(i)}\right)$.\\
        \With{$K_{\varepsilon_0}\left(\left\|s_{0}^{(i)},\dots,s_{obs}\right\|\right)$} {
          $w_0^{(i)}\leftarrow\frac1N$
          Add $\tilde\theta_0^{(i)}$ to $\tilde\Theta_0$ with weight $\tilde{w}_0^{(i)} $.\\
          $i+=1$
        }
      }
      \BlankLine
      \# Resampling Step\\
      \For{$T=1,\dots,T$}{
        $\tilde\Theta_t\leftarrow\{\}$.\\
        $i\leftarrow0$\\
        \While{$i<N$}{
          $\tilde\theta_t^{(i)}\leftarrow$ sample $\tilde\Theta_{t-1}$.\\
          $\theta^*\leftarrow K^*\left(\tilde\Theta_{t-1}\right)$.\\
          $\tilde{x}_t^{(i)}\leftarrow f(X|\theta^*)$.\\
          $\tilde{s}_t^{(i)}\leftarrow s\left(\tilde{x}_t^{(i)}\right)$.\\
          \With{$K_{\varepsilon_0}\left(\left\|s_{0}^{(i)}-s_{obs}\right\|\right)$} {
            $\tilde\theta_t^{(i)}\leftarrow\theta^*$.\\
            $\tilde{w}_t^{(i)}\leftarrow\frac{\pi_0\left(\theta_t^{(i)}\right)}{\sum_{j=1}^Nw_{t-1}^{(j)}\prob\left(K^*\left(\theta_{t-1}^{(j)}=\theta_{t}^{(j)}\right)\right)}$.\\
            Add $\tilde\theta_t^{(i)}$ to $\tilde\Theta_t$ with weight $\tilde{w}_t^{(i)}$.\\
            $i\leftarrow i+1$.
          }
        }
        \BlankLine
        \# Normalise Weights\\
        \For{$i=1,\dots,N$}{
          $w_t^{(i)}\leftarrow\frac{\tilde{w}_t^{(i)}}{\sum_{i=1}^N\tilde{w}_t^{(i)}}$.\\
          Update weight of $\tilde\theta_t^{(i)}$ in $\tilde\Theta_t$ to be $w_t^{(i)}$.
        }
      }
      \Return{$\tilde\Theta_T:=\left\{\left(\tilde\theta_T^{(1)},w_T^{(1)}\right),\dots,\left(\tilde\theta_T^{(N)},w_T^{(N)}\right)\right\}$}
    \end{algorithm}
  \end{box_algorithm}

\subsubsection{Comparison}\label{sec_abc_comparison}

  Which algorithm to use in different scenarios - complexity of model, amount of data available.

\subsection{ABC for Model Choice}\label{sec_abc_model_choice}

\subsection{Regression Adjustment in ABC}\label{sec_abc_regression_adjustment}

  \par Beaumont et al - Local Linear Regressions (LOCL)
  \par Blum and Francois' - Nonlinear Conditional heteroscedastic regressions (NCH). (Uses neural networks)

\newpage
\section{Summary Statistic Selection}\label{sec_summary_stats}

  % TODO posterior mean are optimal summary statistics

  In this section I motivate the research into summary statistic selection \textit{Section \ref{sec_summary_stats_motivation}} and discuss features to consider when selecting summary statistics \textit{Section \ref{sec_properties_of_summary_statistics}}. I then describe five methods for summary statistic selection methods: three which use hand-crafted summary statistics \textit{Sections \ref{sec_approximate_sufficient_subset}-\ref{sec_two_step_minimum_entropy}}; and two which automatically generate summary statistics \textit{Sections \ref{sec_semi_automatic_abc}-\ref{sec_non_linear_projection}}. These approaches are covered in the chronological order in which they were original proposed. To close the section I use a toy example of an SIR model to compare these methods \textit{Section \ref{sec_summary_statistics_toy}}.

\subsection{Motivation}\label{sec_summary_stats_motivation}

  % TODO - there would be little reason to use summary statistics if it wasn't for computational limitations (otherwise use whole dataset)

  \par The study of summary statistics has relevance beyond ABC methods, largely due to the recent ``Big-Data Revolution'' which has seen the rate at which data can be collected and stored significantly outpace improvements in computational power. This has motivated research into effective methods to reduce the size of datasets so that more computationally intensive algorithms can be used to analyse the data. % TODO give examples of papers from other fields

  \par A summary statistic $s$ is a statistic which reduces the dimensionality of some sampled data, in a deterministic fashion, whilst retaining as much information about the sampled data as possible. Reducing the dimensionality of data is desirable as it reduces the computational requirements to analyse the data. Ideally, a summary statistic would compress the sampled data without any information loss (A property known as ``sufficiency''). However, low-dimension sufficient summary statistics are rare in practice and we often have to trade-off information retention against dimensionality reduction.

  \[ s:\mathbb{R}^m\to\mathbb{R}^p\text{ with }m>p \]

  \par In most cases each dimension of the output of a summary statistic is the result of an independent calculation. As such, it is often conceptually easier to consider each dimension as an independent summary statistics when selecting summary statistics. This idea of each dimension of independence also makes it conceptually easy to combine summary statistics by appending the result of one statistic onto the end of the other, as new dimensions. % TODO reword or expand this (probs move)
  As long as the sum of the dimensions of the outputs from the summary statistics in the set is less than that of the sampled data, then using a set of summary statistics still produces effective dimensionality reduction.

  \[ m>\sum_{i=1}^kp_i\text{ where }s_i:\mathbb{R}^m\to\mathbb{R}^{p_i} \]

  The success of ABC methods depends mainly on three user choices: choice of summary statistic; choice of distance measure; and choice of acceptance kernel. Of these, summary statistic choice is arguably the most important as the other two mainly affect the rate at which the algorithm converges on the posterior mean. Whereas, choosing summary statistics which are uninformative can lead to the parameter posteriors returned by the algorithm being drastically different from the true parameter posteriors. This is trivial to realise if you consider a scenario where $s(x)=c$, for some constant $c\in\mathbb{R}$, is used as the sole summary statistic as this would result in all (or none) of the simulations being accepted as thus the returned posterior will be the same as the supplied prior.

  \par In practice, the quality of the posteriors returned from an ABC method is limited by the amount of computational time which is dedicated to running the algorithm. For some problems, such as ......... % TODO
  , it is reasonable to dedicate the majority of your computing time on summary statistic selection, rather than on model fitting, as it is clear that even the simplest ABC methods (e.g. ABC-Rejection Sampling) will be sufficient to fit the model, given a good choice of summary statistics.

  \subsection*{Traditional Thinking}\label{sec_summary_stats_traditional_thinking}

  \par Traditionally, summary statistics for ABC methods are chosen manually using expert, domain-specific knowledge. Utilising this expert knowledge is desirable as these statistics will incentivise exploring regions of the parameter space which have been scientifically shown to be relevant to the given problem and thus more likely to contain the true parameter values (Similarly, these statistics will disincentivise exploring regions which have been shown to not be of interest). % TODO re-write this sentence

  % TODO - Having a more mathematically rigourous approach is desirable

  \par However, relying on expert knowledge to choose summary statistics limits the scenarios where ABC methods can be applied to only those where there has already been significant research. And, leads to statistics being chosen due to their prevalence in a field rather than their suitability to computational methods. Moreover, the use of hand-crafted summary statistics means that any limitations in current understanding of a field will be encoded into the model fitting process, possibly leading to misspecification.

  \par When using a set of summary statistics, expert knowledge is generally not sufficient to determine how best to weight each summary statistic. Some of the methods I describe below can be used to automate the process of determining these weights by specifying multiple versions of the same summary statistic, with each version having a different weight. % TODO mention that this is not easy to implement due to easy pitfalls

\subsection{Properties of Summary Statistics}\label{sec_properties_of_summary_statistics}

  % Degree reduction (ie greater compression)
  \par When evaluating a summary statistic for use in ABC there are main properties, both practical and mathematical, to consider.

\subsection*{Practical Properties}

  \par The key reason for using summary statistics is for the computational efficiencies which result from their dimensionality reduction. Reducing the size of a dataset means less operations need to be performed to analyse it, meaning more simulations can be processed in the same time-period. This naturally means summary statistics which result in greater dimensionality reduction are preferable, but similarly means that a summary statistic which is computationally inefficient to calculate is less desirable.

  % Computational efficiency & overflow
  % TODO correlation is very inefficient as it considers many
  \par For a model which produces data of dimension $n\times m$ (i.e. $n$ readings, each with $m$ features) most standard summary statistics are calculated in $O(n\cdot m)$  time. However, this is only a theoretical result and in practice there are meaningful differences in the computational requirements of each summary statistics. Calculating the mean and maximum values for each feature takes $O(n\cdot m)$ time in theory but, since calculating the mean relies on arithmetic operations and the maximum on comparison operations, they will take different amounts of time in practice. Statistics which rely on search or sorting operations (most notably order statistics) are variable in the their time complexity for different data sets which will affect the reliability of models which use them. Integer overflow is a possible issue for some summary statistics, although this is often easy to avoid when actively being considered during the implementation of an algorithm. Moreover, for statistics with non-linear computational complexity (e.g. correlation between each pair of features), the size of the dataset being analysed needs to be considered when evaluating summary statistic choice.

  % weights and scaling
  \par ABC-methods rely on distance measures to determine whether a simulation is good, or not. This means that the range and scale of values a summary statistic will likely produce will have an affect on how influential that summary statistic is to the final model fit. In most cases it is reasonable to standardise all statistics to have the same mean and variance, effectively giving the same weighting to each statistic. This can be implemented to occur adaptively within the ABC-method. There may be cases % TODO are there any?
  where assigning different weights to different summary statistics makes sense, and produces a better model fit, but these are hard to justify from a theoretical approach. The selection methods I discuss which compare hand-crafted statistics (Sections \ref{sec_approximate_sufficient_subset}-\ref{sec_two_step_minimum_entropy}) can be used to compare possible weightings by including several versions of the same summary statistic, each with a different scaling, in the set of statistics being compared. This will however increase computation time due to the increase size of the set of statistics and may make the results harder to interpret\footnote{Multiple sets of weighted summary statistics will be equivalent due to having the same ratio of weights}.

  % Interpretability
  \par For real-world modelling problems, the interpretability of summary statistics used in the final model is a key factor in how useful the solution is. Senior stakeholders in a problem will want to use the final model to justify their future decisions, this is much easier to do when the factors the model is considering, and the weights it assigns to them, are readily understandable. Hand-crafted statistics are almost always the most readily understandable statistics, as such generated statistics are rarely used in commercial problems\footnote{The current popularity of using ``Neural Networks'' in commercial settings does buck this trend. I hope this fad will subside soon in favour of more interpretable alternatives. I believe it is worth noting that the new European Union payment services directive (PSD2) requires that certain models used by financial institutions be ``explainable'' in order to improve the customer experience and to ensure no one is discriminated against due to their protected characteristics.}. In cases where it is chosen to use automatically generated statistics; one can develop an intuition for their model by varying the inputs, or removing certain features, and observing how the output varies. This is naturally harder to

\subsubsection*{Sufficiency}\label{sec_sufficiency}
  % \subsubsection*{Theory}

  \begin{box_definition}[Sufficient Statistic \cite{sufficient_statistics}]\label{def_sufficient_statistic}
    Let $s:\mathbb{R}^m\to\mathbb{R}^n$ be a statistic and $X$ be a model with parameters $\theta$. The statistic $s$ is said to be sufficient for the parameters $\theta$ if the conditional distribution of the model $X$, given the value of the statistic $s(X)$, is independent of the model parameter.
    \[ \prob(X|s(X))=\prob(X|s(X),\theta) \]
  \end{box_definition}

  \par Verbosely, a statistic is sufficient for a model parameter(s) if it captures all the information which a sample of the model carries about said parameter(s). This means that knowing the value of a sufficient statistic is as informative as knowing the true model parameters. This is clearly a desirable property as in practice we can always calculate the value of the summary statistic using the sampled data, but cannot know the true parameter values (otherwise we would not be trying to predict them). Sufficient statistics exist for all models as, trivially, the identity function is a sufficient statistic for all models.

  \par It can be intuitively helpful to consider a sufficient statistic as a data reduction method. Moreover, a sufficient summary statistic provides a loss-less compression of sampled data as it reduces the dimensionality of the data but retains all relevant information.

  \begin{box_remark}[Supersets of Sufficient Statistics]\label{the_sufficiency_of_superset}
    Let $s_{1:k-1}(\cdot):=\{s_1(\cdot),\dots,s_{k-1}(\cdot)\}$ be a collection of $k-1$ summary statistics and suppose that $s_{1:k-1}$ is sufficient for the parameters $\theta$ of some model $X$. Then $s_{1:k-1}\cup\{s_k\}$ is also sufficient for the parameters $\theta$, for all summary statistics $s_k$.
    \begin{proof}
      \everymath={\displaystyle}
      Consider a model with parameters $\theta$ and let $s_1,\dots,s_k$ be summary statistics where the set $s_{1:k-1}:=\{s_1,\dots,s_{k-1}\}$ is sufficient for parameter $\theta$. Note that the likelihood of set $s_k:=s_{1:k-1}\cup\{s_k\}$ given the model parameters $\theta$ can be stated as
      \[ \prob(s_{1:k}(X)|\theta)=\prob(s_k(X)|s_{1:k-1}(X),\theta)\prob(s_{1:k-1}|\theta) \]
      Now consider the following decomposition of the posterior for the model parameters $\theta$ given summary statistics $s_{1:k}$
      \[\begin{array}{rcl}
        \prob(\theta|s_{1:k}(X))&=&\frac{\prob(s_{1:k}(X)|\theta)\prob(\theta)}{\prob(s_{1:k}(X))}\\
        &=&\frac{\prob(s_k(X)|s_{1:k-1}(X),\theta)\prob(s_{1:k-1}|\theta)\prob(\theta)}{\prob(s_k(X)|s_{1:k-1}(X))\prob(s_{1:k-1}(X))}\\
      \end{array}\]
      Since the set $s_{1:k-1}$ is sufficient for $\theta$ we have that
      \[ \prob(s_k(X)|s_{1:k-1}(X))=\prob(s_k(X)|s_{1:k-1}(X),\theta)\]
      Applying this result to the decomposition above, we deduce that the posterior for the model parameters $\theta$ given $s_{1:k}$ or $s_{1:k-1}$ are identical.
      \[\begin{array}{rcl}
        \prob(\theta|s_{1:k}(X))&=&\frac{\prob(s_k(X)|s_{1:k-1}(X),\theta)\prob(s_{1:k-1}|\theta)\prob(\theta)}{\prob(s_k(X)|s_{1:k-1}(X),\theta)\prob(s_{1:k-1}(X))}\\
        &=&\frac{\prob(s_{1:k-1}|\theta)\prob(\theta)}{\prob(s_{1:k-1}(X))}\\
        &=&\prob(\theta|s_{1:k-1}(X))
      \end{array}\]
      Thus the set $s_{1:k}$ is sufficient for model parameters $\theta$. Due to the arbitrary nature of $s_{1:k-1}$ and $s_k$, this result holds for all supersets of sufficient summary statistics.
    \end{proof}
  \end{box_remark}

  \par \textbf{Remark \ref{the_sufficiency_of_superset}} states that if we have a set of summary statistics which are sufficient for a set of parameters, then adding more summary statistics will never increase (or decrease) the amount of relevant information being extracted from the sampled data. This means there is an optimally minimal number of summary statistics required to achieve sufficiency.

  \par I demonstrate in \textbf{Example \ref{example_sufficient_stats_normal}} that the sample mean is a sufficient summary statistic for a normal distribution with unknown mean.

  \begin{box_example}[Sufficient Statistic for Normal Distribution with Unknown Mean]\label{example_sufficient_stats_normal}
    \everymath={\displaystyle}
    \par\par Let $X\sim\text{Normal}(\mu,\sigma^2_0)$, with $\mu\in\mathbb{R}$ unknown and $\sigma_0^2\in\mathbb{R}$ known, and $\mathbf{x}$ be $n$ independent observations of $X$.
    \par We have that
    \[
      f_{\mathbf{X}}(\mathbf{X})=\prod_{i=1}^nf_X(X_i)=\frac1{(2\pi\sigma_0^2)^{n/2}}\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i-\mu)^2\right\}
    \]
    Let $s=s(\mathbf{X})$ be an arbitrary statistic of $n$ observations from the model. We will build up the conditional distribution of $\mathbf{X}$ given $s(\mathbf{X})$, by first considering their joint distribution
    \[\begin{array}{rcl}
      f_{\mathbf{X},s(\mathbf{X})}(\mathbf{X},s)&=&\frac1{(2\pi\sigma_0^2)^{n/2}}\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i+s-s-\mu)^2\right\}\\
      &=&\frac1{(2\pi\sigma_0^2)^{n/2}}\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n((X_i+s)-(\mu-s))^2\right\}\\
      &=&\frac1{(2\pi\sigma_0^2)^{n/2}}\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n\left((X_i-s)^2+(\mu-s)^2-2(\mu-s)(X_i-s)\right)\right\}\\
      &=&\frac1{(2\pi\sigma_0^2)^{n/2}}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i-s)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(\mu-s)^2\right\}\\
      &&\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n-2(\mu-s)(X_i-s)\right\}\\
      &=&\frac1{(2\pi\sigma_0^2)^{n/2}}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i-s)^2\right\}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(\mu-s)^2\right\}\\
      &&\cdot\exp\left\{\frac{\mu-s}{\sigma_0^2}\left(\sum_{i=1}^n(X_i)-ns)\right)\right\}
    \end{array}\]
    If we define $s(\mathbf{X})=\frac1n\sum_{i=1}^nX_i$, the sample mean, then the third exponential disappears. Note that $s(\mathbf{X})\sim\text{Normal}\left(\mu,\frac1n\sigma_0^2\right)$.
    \par Now consider the conditional distribution of $\mathbf{X}$ given $s(\mathbf{X})$.
    \[\begin{array}{rcl}
      f_{\mathbf{X}|s(\mathbf{X})}(\mathbf{X}|s)&=&\frac{f_{\mathbf{X},s(\mathbf{X})}(\mathbf{X},s)}{f_{s(\mathbf{X})}(s(\mathbf{X}))}\\
      &=&\frac{\sqrt{\frac1{\left(2\pi\sigma_0^2\right)^n}}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i-s)^2\right\}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-s)^2\right\}}{\sqrt{\frac{n}{2\pi\sigma_0^2}}\cdot\exp\left\{-\frac{n}{2\sigma_0^2}(\mu-s)^2\right\}}\\
      &=&\sqrt{\frac{1}{n(2\pi\sigma_0^2)^{n-1}}}\cdot\exp\left\{-\frac1{2\sigma_0^2}\sum_{i=1}^n(X_i-s)^2\right\}
    \end{array}\]
    This shows that the conditional distribution of $\mathbf{X}$ given $s(\mathbf{X})$ is independent of $\mu$, the unknown parameter, and thus the sample mean is a sufficient statistic for a normal distribution with unknown mean
  \end{box_example}

  \par \textbf{Example \ref{example_sufficient_stats_normal}} shows that finding sufficient summary statistics can be a highly manually and did require us to ``guess'' at the possible formulation of a summary statistic, then verify that it was sufficient. The Fisher-Neyman factorisation criterion (\textbf{Theorem \ref{fisher_neyman_factorisation_criterion}}) \cite[]{fnf_fisher_part,fnf_neyman_part}, first recognised by Fisher in \cite[]{fnf_fisher_part}, specifies a property which all sufficient statistics have. This property is used as the basis of a more formulaic approach to finding sufficient statistics by separating the terms of the conditional probability of a model given the summary statistic value into those which depend on the summary statistic and those which do not.

  \begin{box_theorem}[Fisher-Neyman Factorisation Criterion \cite{sufficient_statistics}]\label{fisher_neyman_factorisation_criterion}
    \par\par Let $X\sim f(\cdot;\theta)$ be a model with parameters $\theta$ and $s(\cdot)$ be a statistic.
    \par $s(\cdot)$ is a sufficient statistic for the model parameters $\theta$ \underline{iff} there exist non-negative functions $g(\cdot;\theta)$ and $h(\theta)$ where $h(\cdot)$ is independent of the model parameters\footnotemark\footnotetext{i.e. $h(\cdot)$ only depends on the sampled data} and
    \[ f(X;\theta)=h(X)g(s(X);\theta) \]
    This formulation shows that the distribution of the model $X$ only depends on the parameter $\theta$ through the information extracted by the statistic $s$. A consequence of the sufficiency of $s$.
    \begin{proof} \cite[]{fnf_theorem_proof}
      \everymath={\displaystyle}
      \begin{itemize}
        \item[$\Longrightarrow$] First, consider the forwards direction of the theorem and suppose $s$ is a sufficient summary statistic. Define functions
        \[ h(x)=\prob(X=x|s(X)=s(x))\quad\text{and}\quad g(s(x);\theta)=\prob(s(X)=s(x);\theta)\]
        Note that $h(\cdot)$ is independent of the model parameter $\theta$ due to the sufficiency of $s$. Then
        \[\begin{array}{rcl}
          f_X(x)&=&\prob(X=x)\\
          &=&\prob(X=x,s(X)=s(x))\\
          &=&\prob(X=x|s(X)=s(x))\prob(s(X)=s(x))\\
          &=&h(X)g(s(X))
        \end{array}\]
        \item[$\Longleftarrow$] Now, consider the reverse direction of the theorem and suppose there exists some functions $h(\cdot),g(\cdot;\theta)$, with $h(\cdot)$ independent of model parameter $\theta$, such that
        \[ f(x;\theta)=h(x)g(s(x);\theta)\text{ for all }x\in\mathcal{X},\ \theta\in\Theta \]
        where $\mathcal{X}$ is the support of $X$ and $\Theta$ the set of possible parameters.
        \par Then, for an arbitrary $c\in\mathbb{R}$
        \[\begin{array}{rcl}
          \prob(X=x|s(X)=c)&=&\frac{\prob(X=x,s(X)=c)}{\prob(s(X)=c)}\\
          &=&\frac{\indexed\{s(x)=c\}f(x;\theta)}{\sum_{y\in\mathcal{X};s(y)=c}f(y;\theta)}\\
          &=&\frac{\indexed\{s(x)=c\}h(x)g(s(x);\theta)}{\sum_{y\in\mathcal{X};s(y)=c}h(y)g(s(y);\theta)}\\
          &=&\frac{h(x)g(c;\theta)}{\sum_{y\in\mathcal{X};s(y)=c}h(y)g(c;\theta)}\\
          &=&\frac{h(x)}{\sum_{y\in\mathcal{X};s(y)=c}h(y)}
        \end{array}\]
        This final expression is independent of the model parameter $\theta$.
      \end{itemize}
      The result holds in both directions.
    \end{proof}
  \end{box_theorem}

  \par \textbf{Example \ref{example_sufficient_stats_poisson}} below demonstrates how the Fisher-Neyman Factorisation Theorem can be used to find a sufficient summary statistic for a Poisson model where the mean $\lambda$ is unknown

  \begin{box_example}[Using Fisher-Neyman Factorisation Theorem to find sufficient statistics for a Poisson distribution with unknown mean]\label{example_sufficient_stats_poisson}
    \everymath={\displaystyle}
    \par\par Let $X\sim\text{Poisson}(\lambda)$, with $\lambda\in\mathbb{R}^{>}$ unknown, $\mathbf{x}$ be $n$ independent observations of $X$ and $\textstyle\bar{x}:=\frac1n\sum_{i=1}^nx_i$ be the sample mean of these $n$ observations.
    \par Consider the joint distribution of these $n$ observations
    \[\begin{array}{rcl}
      f_{\mathbf{X}}(\mathbf{x})&=&\prod_{i=1}^nf_X(x_i)\\
      &=&\prod_{i=1}^n\frac{\theta^{x_i}e^{-\theta}}{x_i!}\\
      &=&\frac{1}{\prod_{i=1}^nx_i!}\cdot\theta^{\sum_{i=1}^nx_i}e^{-n\theta}\\
      &=&\underbrace{\left\{\frac{1}{\prod_{i=1}^nx_i!}\right\}}_{(1)}\cdot\underbrace{\left\{\theta^{\sum_{i=1}^nx_i}e^{-n\theta}\right\}}_{(2)}
    \end{array}\]
    The last step shows how the terms can be collected into: (1), those which are independent of model parameter $\theta$; and, (2), those which are dependent on model parameter $\theta$. We can how derive the conditions of the Fisher-Neyman Factorisation theorem by inspecting the final expression.
    \par It is apparent that we should define the function $h(\mathbf{x})$ as
    \[ h(\mathbf{x})=\frac1{\prod_{i=1}^nx_i!} \]
    In order to define the function $g(s(\mathbf{x});\theta)$ we first need to define the summary statistic $s(\mathbf{x})$. This is straightforward as all the sampled data $\mathbf{x}$ only occurs in a sum in (2), so we define $\textstyle s(\mathbf{x})=\sum_{i=1}^n x_i$. Meaning we can define $g(\mathbf{x};\theta)$ as
    \[ g(\mathbf{x};\theta)=\theta^{s(\mathbf{x})}e^{-n\theta} \]
    With these definitions we fulfil the conditions of the Fisher-Neyman Factorisation theorem, meaning $s(\mathbf{X})=\sum_{i=1}^nX_i$ is a sufficient statistic for the mean for a Poisson distribution.
  \end{box_example}

  \par In most cases sufficient statistics for a parameter are not unique. Moreover, each sufficient statistic does not necessarily produce the same level of compression. Consider a normal distribution with unknown mean, here both the sample sum and identity function are both sufficient statistics, however the sample sum is a much more desirable statistic to use as it provides compression down to a single dimension. This lack of uniqueness motivates the concept of minimal sufficiency.

  \begin{box_definition}[Minimally Sufficient Statistic, \cite{dictionary_of_statistical_terms}]\label{def_minimally_sufficient_statistic}
    Let $s(\cdot)$ be a sufficient statistic for parameter $\theta$ of model $X$. $s(\cdot)$ is minimally sufficient if for any other sufficient statistic $t(\cdot)$ of parameter $\theta$ there exists a function $f$ which maps $t(x)\mapsto s(x)$.
    \[ s(X)=f(t(X)) \]
  \end{box_definition}

  \par Minimally sufficient statistics have lower (effective) dimensionality than their non-minimal counterparts. This makes minimally sufficient statistics desirable as they produce the greatest level of compression and, in doing so, maximally reduce the computational resources required to analyse the sampled data.

  \par As with identifying sufficient statistics, determining whether, or not, a sufficient statistic is minimally sufficient is not a trivial task. I demonstrate this in \textbf{Example \ref{example_minimally_sufficient_bernoulli}}.

  \begin{box_example}[Minimally Sufficient Statistic for IID Bernoulli Random Variables]\label{example_minimally_sufficient_bernoulli}
    Let $X_1,\dots,X_n$ are independent and identically distribution Bernoulli random variables. Note that the identity function $s_1(\mathbf{X})=\mathbf{X}$ and the sum function $s_2(\mathbf{X})=\sum_{i=1}^nX_i$ are both sufficient statistics.
    \par We can map from $s_1$ to $s_2$ as follows
    \[ s_2(\mathbf{X})=\sum_{i=1}^n [s_2(\mathbf{X})]_i \]
    However, there is no function which can map from $s_2$ to $s_1$ as it would have to map the value 1 to both $(1,0,\dots,0)$ and $(0,1,\dots,0)$. This proves that the identity function $s_1$ is not a minimally sufficient statistic, but does not prove that the sum function $s_2$ is a minimally sufficient statistic as we have not considered all possible sufficient statistics for this distribution.
  \end{box_example}

  \begin{box_theorem}[Condition for Minimal Sufficiency, \cite{minimal_sufficiency_lecture_notes}]\label{the_condition_for_minimal_sufficiency}
    % http://www.stat.cmu.edu/~siva/705/lec12.pdf
    Consider a model with parameters $\theta$. Let $\mathbf{x},\mathbf{y}$ be two samples from this model and $s(\cdot)$ be a statistic.
    \begin{quote}
      If $\frac{\prob(\mathbf{y};\theta)}{\prob(\mathbf{x};\theta)}$ is independent of $\theta$ iff $s(\mathbf{x})=s(\mathbf{y})$, then statistic $s$ is minimally sufficient.
    \end{quote}
    \begin{proof}
      Let $s(\cdot)$ be a statistic for model $X$ with parameters $\theta$ and assume that $\frac{\prob(\mathbf{y};\theta)}{\prob(\mathbf{x};\theta)}$ is independent of $\theta$ iff $s(\mathbf{y})=s(\mathbf{x})$. I first show that this $s$ is sufficient and then that it is minimally sufficient.
      \par Note that this statistic $s$ produces a partition of the sample space $A=\{A_c:\exists\ \mathbf{x}\in\mathcal{X},\ s(\mathbf{x})=c\}$. For each set $A_c$ of the partition $A$ fix a point $\mathbf{x}_c\in\mathcal{X}$ to represent it.
      \par Let $\mathbf{x}$ be a sample of $X$ and define $\mathbf{y}=\mathbf{x}_{s(\mathbf{x})}$. Note that sample $\mathbf{y}$ is a function of $s(\mathbf{x})$ only and $s(\mathbf{x})=s(\mathbf{y})$. Consider the joint distribution of $\mathbf{x}$
      \[\prob(\mathbf{x};\theta)=\prob(\mathbf{x};\theta)\frac{\prob(\mathbf{y};\theta)}{\prob(\mathbf{y};\theta)}=\prob(\mathbf{y};\theta)\frac{\prob(\mathbf{x};\theta)}{\prob(\mathbf{y};\theta)} \]
      By our assumptions of $s$, we have that $\frac{\prob(\mathbf{x};\theta)}{\prob(\mathbf{y};\theta)}$ is independent of $\theta$. Thus, we can produce the following decomposition
      \[\begin{array}{rcl}
        \prob(\mathbf{x};\theta)&=&h(\mathbf{x})g(s(\mathbf{x});\theta)\\
        \text{where}&\\
        h(\mathbf{x})&=&\frac{\prob(\mathbf{x};\theta)}{\prob(\mathbf{y};\theta)}\\
        g(s(\mathbf{x});\theta)&=&\prob(s(\mathbf{y});theta)
      \end{array}\]
      By the Fisher-Neyman factorisation criterion we can deduce that $s$ is sufficient.
      \par Now, let $t$ be another sufficient statistic for $\theta$ and let $\mathbf{x},\mathbf{y}\in\mathcal{X}$ st $t(\mathbf{x})=t(\mathbf{y})$. By the Fisher-Neyman factorisation criterion, we have
      \[\begin{array}{rcl}
        \prob(\mathbf{x};\theta)&=&h(\mathbf{x})g(t(\mathbf{x});\theta)\\
        &=&\frac{h(\mathbf{x})}{h(\mathbf{y})}h(\mathbf{y})g(t(\mathbf{y});\theta)\\
        &=&\frac{h(\mathbf{x})}{h(\mathbf{y})}\prob(\mathbf{y};\theta)\text{ by Fisher-Neyman factorisation}\\
        \implies\frac{\prob(\mathbf{x};\theta)}{\prob(\mathbf{y};\theta)}&=&\frac{h(\mathbf{x})}{h(\mathbf{y})}
      \end{array}\]
      This shows that $\frac{\prob(\mathbf{x};\theta)}{\prob(\mathbf{y};\theta)}$ is independent of $\theta$, meaning $s(\mathbf{x})=s(\mathbf{y})$ by our assumptions of $s$. This result means there exists a function $f$ st $s(\mathbf{x})=f(t(\mathbf{x}))\ \forall\ \mathbf{x}\in\mathcal{X}$. Moreover, due to the arbitrary definition of $t$, for each sufficient statistic of $\theta$ there exists a function which maps from it to our statistic $s$, fulfilling the definition of $s$ being minimally sufficient.
    \end{proof}
  \end{box_theorem}

  \textbf{Theorem \ref{the_condition_for_minimal_sufficiency}} states that if the ratio of the marginal distributions of two samples from a model are independent of the model parameters if, and only if, the samples map to the same value under some statistic $s$, then $s$ is minimally sufficient. This property can be used to identify minimally sufficient summary statistics, either by assisting in deduction or by verifying a proposed statistic.

  % Rao-Blackwell Theorem (http://www.stats.ox.ac.uk/~steffen/teaching/bs2siMT04/si3c.pdf)
  % Our intention is to use summary statistics to estimate parameter values, the Rao-Blackwell Theorem provides a method for producing unbiased estimators using sufficient statistics.
  \par Statistics carry information about sampled data, but in Bayesian modelling most problems centre around estimating parameter values. In some cases a sufficient statistic may be a good estimator of a model parameter too, in \textbf{Example \ref{example_sufficient_stats_normal}} it was shown that the sample mean is a sufficient statistic for the population mean of a normal distribution. This is not always the case, in \textbf{Example \ref{example_sufficient_stats_poisson}} it was shown that the sum of sampled values is a sufficient statistic for the mean of a Poisson distribution but this is not a good estimator.

  \begin{box_theorem}[Rao-Blackwell Theorem, \cite{rao_blackwell_rao_part,rao_blackwell_blackwell_part}]\label{the_rao_blackwell_theorem}
    Let $X$ be a model with parameters $\theta$, $U=u(X)$ be an unbiased estimator for function $g(\theta)$ and $s(X)$ is a sufficient statistic for $\theta$.
    \begin{quote}
      The statistic $v(X):=\expect[u|T=t(X)]$ is an unbiased estimator of $g(\theta)$ and $\text{Var}(v(X))\leq\text{Var}(u(X))$.
    \end{quote}
    The statistic $v(X)$ is known as the Rao-Blackwell Estimator.
    \begin{proof}
      The proof that $v(X)$ is unbiased is immediate from the Tower Law
      \[\begin{array}{rcl}
        \expect[v(X)]&=&\expect[\expect[u|T=t(X)]]\\
        &=&\expect[u]\\
        &=&g(\theta)
      \end{array}\]
      Now consider the variance of $v(X)$
      \[\begin{array}{rrl}
        \text{Var}(v(X))&=&\text{MSE}[v(X)]-\text{Bias}[v(X)]^2=\text{MSE}[v(X)]\\
        &=&\expect[(v(X)-g(\theta))^2]\\
        &=&\expect[(\expect[v|T=t(X)]-g(\theta))^2]\\
        &=&\expect[(\expect[v-g(\theta)|T=t(X)])^2]\\
        &\footnotemark\leq&\expect[(v-g(\theta))^2|T=t(X)]\\
        &=&\text{Var}(u(X))\\
        \implies\text{Var}(v(X))&\leq&\text{Var}(u(X))
      \end{array}\]
      \footnotetext{$\text{Var}(X)=\expect[X^2]-\expect[X]^2\implies\expect[X^2]\geq\expect[X]^2$}
    \end{proof}
  \end{box_theorem}

  \par The Rao-Blackwell theorem (\textbf{Theorem \ref{the_rao_blackwell_theorem}}) provides a general relationship between estimators and sufficient statistics by demonstrating a transformation of an unbiased estimator, using a sufficient statistic, which produces an unbiased estimator with decreased variance and thus reduced mean-squared error.
  This is desirable as it is often straight-forward to derive a crude estimator and then apply this transformation in order to improve its performance. % TODO reword this
  A Rao-Blackwell transformation is idempotent as applying it to an already transformed estimator returns the same estimator, the proof of this follows immediately from the Tower Law.

  \par The Lehmann-Scheffe theorem \cite[]{lehmann_scheffe_theorem} states that if the statistic used in a Rao-Blackwell transformation is both sufficient and complete, then the resulting estimator is in fact the unique minimum-variance unbiased-estimator. This result is independent of how good the initial estimator was. % https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem

  % TODO (MAYBE) likelihood and sufficiency (http://www.stat.cmu.edu/~siva/705/lec11.pdf)

  \subsubsection*{Sufficiency In Practice}

  % Sufficiency and ABC (If summary stats are not sufficient then the posterior will only ever be an approximation)
  \par In Bayesian modelling problems we want to deduce the posterior for some model parameters to as high a degree of accuracy as possible. Let $f^*(\theta|X(\theta)=x_{obs})$ be the true posterior for model parameters $\theta$ and $\hat{f}(\theta|s(X(\theta))=s(x_{obs}))$ be the estimated posterior produced by our modelling method, given $x_{obs}$ was observed from the true model and summary statistics $s(\cdot)$ were used. If the summary statistics $s(\cdot)$ are sufficient then the estimated posterior $\hat{f}$ will converge towards the true posterior $f^*$, given enough simulations, however, if $s(\cdot)$ are not sufficient then $\hat{f}$ can never (consistently) converge on the true posterior $f^*$, and rather will always be an approximation. Thus, finding sufficient statistics for our models is highly desirable in Bayesian modelling. %TODO make this more succinct

  \begin{box_theorem}[Pitman–Koopman–Darmois Theorem, \cite{Sufficiency_and_Exponential_Families_for_Discrete_Sample_Spaces}]\label{the_pitman_koopman_darmois}
    Among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families are there sufficient statistics whose dimension are bounded as the sample size increases.
    \begin{proof}
      See \cite[]{pkd_theorem_darmois_part,pkd_theorem_pitman_part,pkd_theorem_koopman_part} for the original proofs.
    \end{proof}
  \end{box_theorem}

  % good sufficient statistics are rare (Pitman–Koopman–Darmois theorem)
  \par However, although sufficient statistics do exist for all models, as the identity function is a sufficient statistic for all models, they are not necessarily the best choice of summary statistic when implementing computational methods as they may provide very little dimensionality reduction relative to other statistics which still manage to retain a large about of the relevant data from a sample. Moreover, the Pitman-Koopman-Darmois theorem \textbf{Theorem \ref{the_pitman_koopman_darmois}} states that sufficient summary statistics which provide a high level of dimensionality reduction only exist for probability distributions from exponential families.

  \par This lack of computationally efficient sufficient statistics, for most models, motivated the concept of ``approximate sufficiency'' in \cite[]{Approximately_sufficient_statistics_and_bayesian_computation} which aims to balance the number of summary statistics with the amount of information being retained from a sample. I discuss this concept more when I present the summary statistic selection algorithm from \cite[]{Approximately_sufficient_statistics_and_bayesian_computation} in \textbf{Section \ref{sec_approximate_sufficient_subset}}.

  \par It is demonstrated in \cite[]{on_model_selection_with_summary_statistics} that the using summary statistics which are sufficient for parameters produces unreliable results when performing model selection. This is due to it being impossible to distinguish between models which have the same sufficient statistics for their parameters. For example, the sum of sampled values is a sufficient statistics for the means of both geometric and Poisson distribution and so cannot be used to compare these two models. Rather, cross-model sufficient statistics would be required to distinguish between these models in practice, which is impossible in practice. % TODO say why this is bad (e.g. This limits the ability to compare even simple models, and sufficient statistics rarely exist for more complex models)

  % IRL example (The Ewens Sampling Formula)
  \par To close this section, I shall mention the Ewens' Sampling formula \cite{ewens_sampling_formula} which illustrates a real-world scenario where useable and useful sufficient statistics have been found. The Ewens' Sampling formula provides, under certain conditions, a parametric probability distribution for the frequencies of unique types of allele observed in a sample of gametes when using the Infinite Alleles model. The mutation rate is the only parameter of this distribution and it is notable that the total number of types is a sufficient statistic for the mutation rate \cite[]{partition_structures_and_sufficient_statistics}. This is especially appealing as ABC methods are used widely in population genetics research (See \cite[]{bayesian_inference_of_the_demographic_history_of_chimpanzees,ABC_in_population_genetics,modern_computational_approaches_for_analysing_molecular_genetic_variation_data} among many others).

\subsection{Methods for Summary Statistic Selection}\label{sec_summary_stats_methods}

  \par When thinking about summary statistic selection it is useful to consider the summary statistics themselves as a feature of your theorised model. This makes the process of selecting summary statistics analogous to model selection, with each combination of summary statistics being considered as a unique model. This is the motivation behind many summary statistic selection methods.

  % TODO some of these algorithms are worth running multiple times and then choosing the set of statistics which are returned most often, or creating a mixtures model

\subsubsection{Approximate Sufficient Subset}\label{sec_approximate_sufficient_subset}

  \par \cite[]{Approximately_sufficient_statistics_and_bayesian_computation} presents the first algorithm for automating the selection of summary statistic. The key idea of their approach is to find a subset of summary statistics, from a large set of hand-crafted statistics, such that ABC methods perform approximately as well when using the subset. This requires a method for empirically evaluating the information extracted by sets of summary statistics. The use of hand-crafted statistics, as discussed above, comes with its own advantages and limitations.

  \begin{box_remark}[Difference of Log-Likelihood]\label{rem_difference_of_log_likelihood}
    Let $s_1,\dots,s_k$ be summary statistics for a model $X$ with parameters $\theta$. Define sets $s_{1:k-1}:=\{s_1,\dots,s_{k-1}\},\ s_{1:k}:=\{s_1,\dots,s_k\}$ and consider the likelihood of the set $s_{1:k}$ with respect to the model parameters $\theta$
    \[\begin{array}{rrcl}
    &\prob(s_{1:k}(X)|\theta)&=&\prob(s_k(X)|s_{1:k-1}(X),\theta)\cdot\prob(s_{1:k-1}(X)|\theta)\\
    \implies&\ln\prob(s_{1:k}(X)|\theta)&=&\ln\prob(s_k(X)|s_{1:k-1}(X),\theta)+\ln\prob(s_{1:k-1}(X)|\theta)\\
    \implies&\ln\prob(s_{1:k}(X)|\theta)-\ln\prob(s_{1:k-1}(X)|\theta)&=&\ln\prob(s_k(X)|s_{1:k-1}(X),\theta)
    \end{array}\]
  \end{box_remark}

  \par For the theoretical basis of their algorithm, Joyce \& Marjoram first show that the difference in log-likelihood value between two sets of summary statistics can be directly quantified as $\ln\prob(s_k(X)|s_{1:k-1}(X),\theta)$ (\textbf{Remark \ref{rem_difference_of_log_likelihood}}). It is worth noting that if the set $s_{1:k-1}$ is sufficient for model parameter $\theta$ then the quantity $\ln\prob(s_k(X)|s_{1:k-1}(X),\theta)$ would be independent of $\theta$ and thus mean $s_k$ does not contribute to inferences about $\theta$. This result reduces the problem of comparing sets of statistics to calculating or estimating a single value and motivates Joyce \& Marjoram use of log-likelihood in their definition of score. Score quantifies how much extra information is extracted when a single extra statistic is added to a set with greater score values meaning more extra information is extracted. Thus we want to find the statistics with the greatest scores. Moreover, if the score of a statistic differs significantly from 0 then it should be accepted.

  \begin{box_definition}[Score $\delta_k$, \cite{Approximately_sufficient_statistics_and_bayesian_computation}]\label{def_score}
    Let $s_1,\dots,s_k$ be $k$ summary statistics. The score of $s_k$ relative to the set $s_{1:k-1}:=\{s_1,\dots,s_{k-1}\}$ is defined as
    \[ \delta_k:=\sup_\theta\left\{\ln\prob(s_k|s_{1:k-1})\right\}-\inf_\theta\left\{\ln\prob(s_k|s_{1:k-1})\right\} \]
  \end{box_definition}

  \begin{box_definition}[$\varepsilon$-Approximate Sufficiency, \cite{Approximately_sufficient_statistics_and_bayesian_computation}]\label{def_approximate_suffiency}
    Let $s_1,\dots,s_k$ be $k$ summary statistics. The set $s_{1:k-1}:=\{s_1,\dots,s_{k-1}\}$ $\varepsilon$-sufficient for statistic $s_k$ if the score of $s_k$ relative to $s_{1:k-1}$ is no greater that $\varepsilon$.
    \[ \delta_k\leq\varepsilon \]
  \end{box_definition}

  \par ABC methods are applied in scenarios where likelihoods are intractable. This means that the score of a statistic is intractable too. Thus, Joyce \& Marjoram only use the score to motivate their algorithm and in practice use different approaches to compare statistics. I discuss this in more detail later when I explore the practicalities of their algorithm. % TODO is this true? or do their methods estimate score

  \begin{box_algorithm}[Approximately Sufficient Subset of Summary Statistics, \cite{Approximately_sufficient_statistics_and_bayesian_computation}]\label{alg_approximately_sufficient_subsets}
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Set of summary statistics $S$; Score threshold $\varepsilon$}
      $S'\leftarrow\emptyset$\\
      \While{true}{
        Calculate the score for each statistic in $S$ wrt $S'$\label{alg_line_calculate_score}\\
        $\delta_{max}\leftarrow\max_{s\in S}\text{Score}(s;S')$\label{alg_approximately_sufficient_subsets_max_score}\\
        $s_{max}\leftarrow\text{argmax}_{s\in S}\text{Score}(s;S')$\label{alg_approximately_sufficient_subsets_max_statistic}\\
        \lIf{$\delta_{max}>\varepsilon$}{\label{alg_approximately_sufficient_subsets_if_statement}
          $S'\leftarrow S'\cup\{s\}$
        } \lElse {
          \Return{S'}
        }
      }
    \end{algorithm}
  \end{box_algorithm}

  \par Joyce \& Marjorams' algorithm (\textbf{Algorithm \ref{alg_approximately_sufficient_subsets}}) starts with an empty set and proceeds to, each iteration, add the summary statistic with the greatest score wrt the set of already selected statistics, until it believes that the none of the remaining unselected summary statistics extracts a significant amount of extra information about the model parameters. They define the concept of $\varepsilon$-approximate sufficient sets to formalise this stopping condition, with the algorithm running until the set of accepted summary statistics $S'$ is $\varepsilon$-approximate sufficient for each unchosen summary statistic, individually. This makes $\varepsilon$ is a parameter of the algorithm, with smaller values likely leading to more summary statistics being accepted as the threshold for the amount of extra information extracted by each new statistic is lower. Alternatively, we could fix or cap the number of summary statistics we want to be accepted from the superset.

  \par As mentioned, in practice the score cannot be calculated. Joyce \& Marjoram instead determined that a proposed statistic introduces significant extra information if the posterior of parameters accepted under its usage was significantly different from the posterior when it was not used. This approach, set out in \textbf{Algorithm \ref{alg_evaluate_proposed_statistic}}, consists of estimating the expected value and standard deviation for the number of occurrences of each parameter value; and then accepting the proposed statistic if any of the observed number of occurrences is more than four standard deviations away from its expected value\footnote{In \cite[]{Approximately_sufficient_statistics_and_bayesian_computation} it is recommended to use a value of between one and four standard deviations}. For this approach to be computationally tractable the posterior space is discretised into $M$ bins whose counts can be compared. When this approach is applied the stopping condition of the main algorithm is changed to be \textit{``Stop if no proposed statistics were accepted in the last cycle''}. There are alternative stopping conditions which could be used, it is reasonable to place a cap on the number of statistics allowed to be accepted\footnote{A leave-one-out cross-validation could be used to determine the optimal number of statistics to use.}.

  \begin{box_algorithm}[Evaluate Proposed Statistic]\label{alg_evaluate_proposed_statistic}
    \begin{algorithm}[H]
      \footnotetext{The expected values $E$ (Line \ref{alg_expected_value}), the standard deviations $sd$ (Line \ref{alg_standard_deviation}) and the condition of the if statement (Line \ref{alg_line_condition}) are each evaluated piece-wise.}
      \SetKwInOut{Require}{require}
      \Require{Sets of accepted parameters $\Theta_{1:k-1},\Theta_{1:k}$; Number of bins $M$}
      $N_{1:k}\leftarrow\left|\Theta_{1:k}\right|$\\
      $N_{1:k-1}\leftarrow\left|\Theta_{1:k-1}\right|$\\
      $C_{1:k-1}\leftarrow\Theta_{1:k-1}$ discretised into $M$ bins\\
      $C_{1:k}\leftarrow\Theta_{1:k}$ discretised into $M$ bins\\
      $E\leftarrow\displaystyle\frac{C_{1:k-1}\cdot N_K}{N_{K-1}}$\tcp*{Expected value of each bin}\label{alg_expected_value}
      $sd\leftarrow\displaystyle\sqrt{\frac{E(N_{K-1}-C_{1:k-1})}{N_{K-1}}}$\tcp*{Standard deviation of each bin}\label{alg_standard_deviation}
      \lIf{Any $\left|C_{1:k}-E\right|>4sd$} { \label{alg_line_condition}
      \Return{Accept proposed statistic}
      } \lElse { \Return{Reject proposed statistic}}
    \end{algorithm}
  \end{box_algorithm}

  \par \textbf{Algorithm \ref{alg_evaluate_proposed_statistic}} requires sets of parameters which were accepted under each set of summary statistics in order to compare posteriors. These sets are acquired by generating a large number of simulations of the theorised model, using parameters sampled from the model priors, and then running ABC-Rejection Sampling to determine which parameters would be accepted under each set of summary statistics\footnote{Considerations need to be made for how the bandwidth of the kernel scale with the number of parameters. The simplest solution is for it to scale linearly.}. This approach has the desirable property that we only need to generate simulations once, and can then use the same set of samples each time we run \textbf{Algorithm \ref{alg_evaluate_proposed_statistic}}. This property allows us to justify generating a very large number of simulations which will make the posterior estimates more accurate. Using this approach means the approximation factor $\varepsilon$ is no longer a parameter of the algorithm, but the distance measure, acceptance kernel and bandwidth used in the ABC-Rejection Sampling step are now parameters, as well as the number of bins $M$ and number of model simulations. Implement caching to avoid having to run ABC-Rejection Sampling multiple times for the same set of statistics will dramatically improve the computational efficiency of this approach, especially when a large super-set of statistics is being used.

  \par A limitation of \textbf{Algorithm \ref{alg_evaluate_proposed_statistic}} is that it does not produce a numerical value which can be used to rank each proposed statistic\footnote{You could compare each possible subset but this would highly inefficient as it potentially requires $\binom{K!}2$ executions of Algorithms \ref{alg_evaluate_proposed_statistic}, where $K$ is the number of statistics being considered, and there is no guarantee this would produce a definitive best set, due to the complex relationships between statistics.}, as the theoretical score would. This means we cannot choose to keep adding the highest scoring statistic, as in \textbf{Algorithm \ref{alg_approximately_sufficient_subsets}}, and instead have to consider statistics in a somewhat arbitrary order. This means that the order in which statistics are considered will affect the result of the algorithm. An imperfect solution to this is to consider statistics in a random order and whenever a statistic is accepted, consider removing each statistic which has already been chosen. Implementing this is not trivial as considerations need to be made to avoid infinite loops where the same statistics keep getting added and removed.

  \par \textbf{Algorithm \ref{alg_evaluate_proposed_statistic}} performs poorly when the supplied set of statistics include uninformative statistics. This can be seen by noticing that a summary statistic which maps to a constant will almost always produce a posterior which is significantly different from an informative set of statistics and therefore be accepted as a statistic despite.

  % TODO \par Alternative methods for determining whether one distribution is significantly different from the other are estimating and comapring entropy, KL divergence

  % \begin{box_algorithm}[Practical Version of \textbf{Algorithm \ref{alg_approximately_sufficient_subsets}}]
  %     \begin{algorithm}[H]
  %       \SetKwInOut{Require}{require}
  %       \Require{Set of summary statistics $S$; Score threshold $\varepsilon$}
  %       $S'\leftarrow\emptyset$\\
  %       $\Theta,Y_{obs}\leftarrow$simulate $N$ sets of parameters and observations\\
  %       \While{$S_{check}$}{
  %         $s_{prop}\leftarrow$Random($S_{check}$)\\
  %         $S_{check}\leftarrow S_{check}\setminus\{s_{prop}\}$\\
  %         $\Theta_{S'}\leftarrow$Parameters in $\Theta$ accepted when using $S'$\\
  %         $\Theta_{S'\cup\{s_{prop}\}}\leftarrow$Parameters in $\Theta$ accepted when using $S'\cup\{s_{prop}\}$\\
  %         \lIf{\textbf{Algorithm \ref{alg_evaluate_proposed_statistic}}$(\Theta_{S'},\Theta_{s'\cup\{s_prop}\},M)$}{
  %           $S'\leftarrow S'\cup\{s\}$\\
  %
  %         } \lElse {
  %           \Return{S'}
  %         }
  %       }
  %       \Return{S'}
  %     \end{algorithm}
  %   \end{box_algorithm}

  % TODO conclusion?
  % NOTE main failing is that score cannot be estimated directly due to likelihood being unavailable

% TODO \subsubsection{Partial Least Squares}

  % TODO if need pages
  % NOTE if I add this need to change parts where I take about what I do in this section

  % A partial least squares transformation is applied to proposed summary statistics \cite[]{efficient_ABC_coupled_with_MCMC_wo_likelihood}

\subsubsection{Minimising Entropy}\label{sec_minimsing_entropy}

  % TODO why choose entropy over variance

  \par \cite[]{on_optimal_selection_of_summary_stats_for_ABC} explores using the set of summary statistics which minimise the entropy of the approximate posterior distribution returned by an ABC-method. In the paper Nunes \& Balding propose two algorithms: the first I discus in this section; and the second, a two-step approach, I discuss in section \ref{sec_two_step_minimum_entropy}. Both methods consider sets of handcrafted statistics.

  \begin{box_definition}[Entropy $H$, \cite{mathematical_theory_of_communication}]
    \everymath={\displaystyle}
    The entropy $H(X)$ of a probability distribution $X$ is a measure of the information and uncertainty in distribution.
    \[\begin{array}{rrcl}
      \text{Discrete}&H(X)&:=&-\sum_{x\in\mathcal{X}}\prob(X=x)\cdot\ln\prob(X=x)\\
      \text{Continuous}&H(X)&:=&-\int_{\mathcal{X}}f_X(x)\cdot\ln f_X(x)dx
    \end{array}\]
    where $\mathcal{X}$ is the support of distribution $X$.\\
    The joint-entropy of probability distributions $X_1,\dots,X_n$ is defined as
    \[\begin{array}{rrcl}
      \text{Discrete}&H(X_1,\dots,X_n)&:=&-\sum_{x_1\in\mathcal{X}_1}\dots\sum_{x_n\in\mathcal{X}_n}\prob(x_1,\dots,x_n)\cdot\ln\prob(x_1,\dots,x_n)\\
      \text{Continuous}&H(X_1,\dots,X_n)&:=&-\int f_{X_1,\dots,X_n}(x_1,\dots,x_n)\cdot\ln f_{X_1,\dots,X_n}(x_1,\dots,x_n)dx\dots dx_n
    \end{array}\]
    where $\mathcal{X}_i$ is the support of distribution $X_i$
  \end{box_definition}

  \par A greater entropy value indicates a lower amount of information in the distribution, and visa-versa. This motivates approaches which seek to minimise entropy as they will in turn maximise information. Nunes \& Balding's usage of entropy is equivalent to Joyce \& Marjoram's usage of score, the advantage of entropy is that there are well-studied methods for estimating its value. Entropy may appear to be an equivalent measure to variance, but this is only true for unimodal distributions. Entropy measures the spread of probability mass whereas variance measures the spread of the data values. The difference can be seen by considering how the values of entropy and variance change for a bimodal distribution if the distance between the two peaks is increased; entropy will not change, whilst variance will increase.

  % NOTE add something about which distributions get high or low entropy
  % TODO talk about multi-variate more?
  % \par Degenerate distributions\footnote{Distributions where the outcome is always the same known event (e.g. flipping a two-headed coin).} have the lowest entropy value, 0, whilst uniform distributions have the highest 1.

  \begin{box_definition}[$k^{th}$-Nearest Neighbour Estimator of Entropy, \cite{nearest_neighbour_estimates_of_entropy}]\label{def_knn_neighbour}
    Consider a distribution $X$ with $\rho$ different parameters and a set of parameter values $\Theta$ which were accepted during some ABC-method, with $n=|\Theta|$. \cite{nearest_neighbour_estimates_of_entropy} define the $k^{th}$-nearest neighbour estimator of entropy as
    \[ \hat{H}=\ln\left(\frac{\pi^{\rho/2}}{\Gamma\left(1+\frac\rho2\right)}\right)-\frac{\Gamma'(k)}{\Gamma(k)}+\ln(n)+\frac\rho{n}\sum_{i=1}^n\ln D_{i,k} \]
    where $D_{i,k}$ is the Euclidean distance between the $i^{th}$ accepted parameter set and its $k^{th}$ nearest neighbour and $\Gamma(\cdot)$ is the gamma function.
  \end{box_definition}

  \par In the context of summary statistic selection we want to calculate the entropy of the posterior distribution of model parameters given summary statistic values. We only ever have an approximation of this distribution and thus can only estimate its entropy. For computational efficiency it is common to discretise the approximated distribution. There are many techniques for estimating the entropy of a distribution from samples, see \cite[]{non_parameteric_entropy_estimation} for an overview. Due to most models of interest in Bayesian modelling having multiple parameters and thus the posterior being multivariate, Nunes \& Balding suggest using the asymptoticly $k^{th}$-Nearest Neighbour estimator of entropy \cite{nearest_neighbour_estimates_of_entropy} (\textbf{Definition \ref{def_knn_neighbour}}).

  \par When implementing \textbf{Definition \ref{def_knn_neighbour}} determining the $k^{th}$ nearest neighbour in an efficient manner is not trivial. A truncated insertion sort is a straightforward approach but has time complexity $O(kn)$ so does not scale efficiently for large values of $k$. \cite[]{nearest_neighbour_estimates_of_entropy} recommend using $k=4$ as their experiments found that greater values of $k$ did not decrease the root-mean square error RMSE significantly, and so were not worth the increased computational complexity.

  \par Using the ``Best Samples'' version of the ABC-Rejection Sampling algorithm to acquire the approximate posterior used in \textbf{Definition \ref{def_knn_neighbour}} is advisable as it does not require the specification of an acceptance kernel and thus the same configuration can be used for all sets of summary statistics. Also, as the number we specify the number of simulations this step should have the same run-time each time it is called, regardless of the set of statistics being analysed, assuming that the summary statistics take trivial time to calculate.

  \begin{box_algorithm}[Minimum Entropy Summary Statistic Selection, \cite{on_optimal_selection_of_summary_stats_for_ABC}]\label{alg_minimum_entropy_summary_statistic_selection}
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Set of summary statistics $S$}
      \For{$S'\in 2^{S}$}{\label{alg_me_sss_for_loop}
        $\Theta\leftarrow$Parameter sets accepted from ABC-Rejection Sampling using $S'$\label{alg_me_rejection_sampling}\\
        $\hat{H}_{S'}\leftarrow\hat{H}(\Theta)$
      }
      $S_{ME}^*\leftarrow\text{argmin}_{S'\in 2^S}\hat{H}_{S'}$\\
      \Return{$S_{ME}^*$}
    \end{algorithm}
  \end{box_algorithm}

  \par The first algorithm proposed by Nunes \& Balding \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} is very straight-forward. It calculates the entropy for each subset of the supplied set of summary statistics $S$ and returns whichever set has the lowest entropy. A limitation of \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} is how its computational complexity scales wrt the size of the set of supplied summary statistic $S$. As the for-loop (line \ref{alg_me_sss_for_loop}) considers every subset, the computational complexity of the algorithm scales exponential with the size of $S$. The simplest mitigation of this is to only consider subsets whose size is in some specified range, this could be implemented adaptively. A more complex procedure would be to introduce a pruning algorithm which does evaluate sets whose subsets produce high entropy values.

  \par The estimated entropy value for a set of statistics will vary each time due to the random nature of the parameter set $\Theta$ returned by the ABC-Rejection Sampling step (Line \ref{alg_me_rejection_sampling}). This means the set of parameters returned by \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} will vary each time it is executed. Allowing more simulations to be performed in this step will reduce the varability in the entropy results. Alternatively, you could instead run the algorithm multiple times, keeping the number of simulations performed in line \ref{alg_me_rejection_sampling} relatively low, and use the results to generate a mixtures model.

  \par \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} only returns the best performing set, and no other information. It could be extended to instead return the best $m$ sets along with their entropy values so that a mixtures model could be generated.

  \par \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} only uses entropy to evaluate the sets of summary statistics. However, as justified above, having a smaller set of statistics is preferable. This preference can be encoded into the algorithm by inflating the entropy value of larger sets. How much the value should be inflated is not a trivial matter. % TODO explore this more

  \par As each subset is assessed independently, \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} can be readily implemented using parallelisation. This will dramatically improve run time for this algorithm and is not something which can be done with Joyce \& Marjorams' approximately sufficient subset approach.

  % NOTE could say there are further avenues to explore in information theory (Mutual information, KL divergence & cross entropy)
  % https://www.princeton.edu/~kulkarni/Papers/Journals/j068_2009_WangKulVer_TransIT.pdf

\subsubsection{Two-Step Minimum Entropy}\label{sec_two_step_minimum_entropy}

  \par The second algorithm in \cite[]{on_optimal_selection_of_summary_stats_for_ABC} is an extension of the first. It uses the set of statistics $S_{ME}^*$ returned by \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} to simulate parameter sets $\Theta_{acc}$ which are treated as if they were observed. Each subset of statistics is then reassessed using these parameter sets $\Theta_{acc}$, with the subset which optimises some error measure returned as the recommended set.

  \begin{box_definition}[Mean Residual Sum of Squares Error, \cite{on_optimal_selection_of_summary_stats_for_ABC}]\label{def_mrsse}
    Let $\mathbf{X}:=\{X_1,\dots,X_n\}$ be a set of observations and $X^*$ be a target value. Residual sum of squares error (RSSE) measures the difference between the observed values and the target value by calculating the mean of the square of the residuals. A smaller RSSE value indicates less error as the observed values do not deviate much from the target value.
    \[ \text{RSSE}(\mathbf{X},X^*):=\sqrt{\frac1n\sum_{i=1}^n\|X_i-X^*\|} \]
    where $\|\cdot\|$ is the Euclidean distance.
    \par Now define $\mathbf{X}^*:=\{X_1^*,\dots,X_m^*\}$ to be a set of target values. The mean residual sum of squares error (MRSSE) is the mean RSSE value for each target value wrt the observed data $X$.
    \[ \text{MRSSE}(\mathbf{X},\mathbf{X}^*):=\frac1m\sum_{i=1}^m\text{RSSE}(\mathbf{X},X_i^*) \]
  \end{box_definition}

  \par The accepted parameter sets $\Theta_{acc}$ are treated as if they are the true parameter space distribution, this means the reassessments now considers the error between a simulated distribution and $\Theta_{acc}$. There are various measures which could be used, including Kolmogorov–Smirnov statistic \cite[]{kolmogorov_smirnov_statistics} and cross-entropy. Nunes \& Balding choose to use the mean residual sum of squares error (MRSSE, \textbf{Definition \ref{def_mrsse}}) with the set of statistics which minimises MRSSE wrt $\Theta_{acc}$ is return as the recommended set of statistics.

  \par MRSSE is a desirable statistic to use in the context of Bayesian modelling as there are theoretical results which prove that minimising MRSSE is a good metric for estimating the mean of a distribution and that posterior means are optimal summary statistics. % TODO include this result (https://math.stackexchange.com/questions/633440/proving-that-the-estimate-of-a-mean-is-a-least-squares-estimator)
  MRSSE is straightforward to compute and can be applied to multivariate distributions is sensitive to outlier values. Note that the scale of parameter values will affect the MRSSE and thus parameter values should be standardised before computation. A limitation of MRSSE is its sensitivity of outlier values, which is not mitigated by the standardisation.

  \begin{box_algorithm}[Two-Step ME Summary Statistic Selection\cite{on_optimal_selection_of_summary_stats_for_ABC}]\label{alg_two_step_me}
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Observations from true model $x_{obs}$, Set of summary statistics $S$, Number of simulations to run $n_{run}$, Number of simulations to accept $n_{obs}$} % TODO x_{obs} is required
      $S_{ME}\leftarrow$\textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}}($S$)\\
      $\hat{\Theta}_{ME}\leftarrow$Parameter sets accepted from ``Best Samples'' ABC-RS($x_{obs},S',n_{run},n_{acc}$)\\
      Standardise $\hat\Theta_{ME}$\\
      \For{$S'\in 2^{S}$}{\label{alg_two_step_me_for_loop}
        $\Theta_{acc}\leftarrow$Parameter sets accepted from ``Best Samples'' ABC-RS($x_{obs},S',n_{run},n_{acc}$)\\
        Standardise $\Theta_{acc}$\\
        $\text{MRSSE}_{S'}\leftarrow\text{MRSSE}(\Theta_{acc},\hat\Theta_{ME,i})$
      }
      $S^*\leftarrow\text{argmin}_{S'\in 2^S}MRSSE_{S'}$\\
      \Return{$S^*$}
    \end{algorithm}
  \end{box_algorithm}

  \par \textbf{Algorithm \ref{alg_two_step_me}} inherits many of the limitations of the \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}}, namely those concerning how its performance scales with the size of $S$ and the use of minimum entropy. The mitigations for these are the same as discussed in Section \ref{sec_minimsing_entropy}. Additionally, to reduce the number of subsets being evaluated in the for-loop (line \ref{alg_two_step_me_for_loop}). As \textbf{Algorithm \ref{alg_two_step_me}} requires the running of \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}} it will always have greater computational complexity.

  %TODO The second step of \textbf{Algorithm \ref{alg_two_step_me}} relies on a good set of statistics being returned by \textbf{Algorithm \ref{alg_minimum_entropy_summary_statistic_selection}}

  %TODO be more specific on model parameters

\subsubsection{Semi-Automatic ABC}\label{sec_semi_automatic_abc}
  \cite[]{constructing_summary_statistics_for_approximate_bayesian_computation_semi_automatic_ABC} presents the first algorithm which constructs its own summary statistics for ABC, rather than choose from a set of hand-crafted ones. Their approach (\textbf{Algorithm \ref{alg_semi_auto_abc}}) uses a pilot run of an ABC-method to generate a na\"ive approximation of the parameter posterior which is used to generate summary statistics. The approximate posterior is used to generate a ``training set'' from which a regression model can be fitted. Model parameters are assumed to be independent and one summary statistic is generated per each model parameter. The generated summary statistics target the posterior mean, an optimal summary statistic, and should be used in a proper running of ABC to generate parameter posteriors. This approach is referred to as \underline{semi}-automatic as it requires the user to specify the summary statistics used in the pilot run of ABC however the identity function would be appropriate, although inefficient.

  \begin{box_algorithm}[Semi-Automatic ABC, \cite{constructing_summary_statistics_for_approximate_bayesian_computation_semi_automatic_ABC}]\label{alg_semi_auto_abc}
    \begin{algorithm}[H]
      \SetKwInOut{Require}{require}
      \Require{Observations from true model $x_{obs}$, Set of summary statistics $S$, Number of simulated parameter sets $m$, Theorised model $X$}
      $f_\theta\leftarrow$Posterior from pilot run of an ABC-method using $x_{obs}$ and $S$\label{alg_semi_auto_abc_pilot_run}\\
      $\hat\Theta\leftarrow$ $m$ simulations from $f_\theta$\\
      $X_{\hat\theta}\leftarrow$ $X\left(\hat\theta\right)$ for each $\hat\theta\in\hat\Theta$\\
      Generate summary statistics using $\hat\Theta$ and $\{X_{\hat\theta}\}_{\hat\theta\in\hat\Theta}$\label{alg_semi_auto_abc_estimate_ss}
    \end{algorithm}
  \end{box_algorithm}

  \par Regression methods are used in line \ref{alg_semi_auto_abc_estimate_ss} with the goal of creating mappings from the simulated response data $x_{\hat\theta}$ and the generated parameter values $\hat\Theta$. The best regression methods are those which target the expected value of the parameter as the posterior mean is an optimal summary statistic. There are several approaches which can be taken, I outline three here
  \begin{enumerate} % NOTE PCA, multiple regressions too?
    \everymath={\displaystyle}
    \item Linear regression \cite[]{constructing_summary_statistics_for_approximate_bayesian_computation_semi_automatic_ABC} assumes that the model can be expressed as $\mathbf{y}=\alpha+\pmb\beta^T X+\varepsilon$ where $X$ is the explanatory variables, $\mathbf{y}$ is the response variables\footnote{In Bayesian modelling context typically $X$ is set to the observed values $x_{obs}$ and $y$ are set to the model parameters $\theta$.}, $\alpha\in\mathbb{R},\beta\in\mathbb{R}^{|\theta|}$ are coefficients to be fitted and $\varepsilon$ is some zero-mean additive noise which can be modelled by a random variable. Linear regression seeks to find the values $\hat\alpha,\hat{\pmb\beta}$ which optimises some loss function
    \[\begin{array}{rcl}
      \everymath={\displaystyle}
      \hat\alpha,\hat{\pmb\beta}&=&\text{argmin}_{\alpha,\pmb\beta}\sum_i L\left(\expect[y|\mathbf{x}_i,\alpha,\pmb\beta]-y_i\right)\\
      &=&\text{argmin}_{\alpha,\pmb\beta}\sum_i L\left(\alpha+\pmb\beta^T\mathbf{x}_i-y_i\right)
    \end{array}\]
    Linear regression works well when each response variable is independent and  can easily be extended to projections of $X$ by replacing all $X$ terms with $f(X)$ where $f(\cdot)$ is a (potentially non-linear) function. This is useful in the context of ABC-methods as we can define $f(\cdot)$ to be our summary statistics.
    \par Linear regression is a well study problem and there any many tractable solutions with least-squares estimation being perhaps the most popular. In ordinary least-squares estimation the quadratic loss function $L_2$ is used meaning the problem is to find
    \[\begin{array}{rcl}
      \everymath={\displaystyle}
      \hat\alpha_{LSE},\hat{\pmb\beta}_{LSE}&=&\text{argmin}_{\alpha,\pmb\beta}\sum_i \left(\alpha+\pmb\beta^T\mathbf{x}_i-y_i\right)^2\\
      &=&\text{argmin}_{\alpha,\pmb\beta}\sum_i \left(\alpha+\pmb\beta\mathbf{x}_i-y_i\right)^T\left(\alpha+\pmb\beta^T\mathbf{x}_i-y_i\right)
    \end{array}\]
    A closed-form estimator for these quantities is known \cite[]{econometrics}.
    \[ (\hat\alpha_{LSE},\hat{\pmb\beta}_{LSE})=\left(\tilde{X}^T\tilde{X}\right)^{-1}\tilde{X}^T \mathbf{y} \]
    where $\tilde{X}$ is $X$ with a column of 1s at the start for the constant term. There are extensions of ordinary least-squares which allow for weighting of variables and for the model to be heteroscedasticity. These extensions are not relevant to the problems being covered in this project.

    \item Lasso regression \cite[]{elements_of_statistical_learning} seeks the vector $\hat\beta$ which satisfies the following expression
    \everymath={\displaystyle}
    \[\begin{array}{rcl}
      \hat\beta&=&\text{argmin}_\beta\sum_{i=1}^N\left(y_i-\beta_0-\sum_{j=1}^\rho x_{ij}\beta_j\right)^2\\
      \text{subject to}&&\sum_{j=1}^\rho|\beta_j|\leq t
    \end{array}\]
    where $X$ are the explanatory variable values, $\mathbf{y}$ are the response variable values, $\rho:=|X_i|$ is the number of model parameters and $t$ is a restriction on the size of regression coefficients.
    \par Lasso and Ridge regression have the same objective function, but ridge regression uses an $L_2$ penalty function rather than lasso's $L_1$ function. An $L_1$ penalty function is preferable for feature selection as it shrinks coefficient values to zero more aggressively than an $L_2$ function, this is useful if the coefficient for a feature is (near) zero then the feature can be dropped.

    \item Canonical correlation analysis (CCA) \cite[]{multivariate_analysis} splits variables into two sets $\mathbf{X},\mathbf{Y}$\footnote{For Bayesian modelling you typically set $\mathbf{X}$ to be the model parameters and $ \textbf{Y}$ to be observed values.} and basis vectors $\pmb\alpha,\pmb\beta$ are sought such that the linear combinations ${\psi:=\pmb\alpha^T\mathbf{X}}$, ${\phi:=\pmb\beta^T\mathbf{Y}}$ are as correlated as possible. % TODO include solution and proof? (https://www.cs.cmu.edu/~tom/10701_sp11/slides/CCA_tutorial.pdf)
    \[ \pmb\alpha,\pmb\beta=\text{argmax}_{\pmb\alpha,\pmb\beta}\text{Corr}(\pmb\alpha\mathbf{X},\pmb\beta\mathbf{Y}) \]
    Solutions to this are known and readily calculatable.
    \[\begin{array}{rcl}
      \pmb\alpha&=&\Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}\\
      \pmb\beta&=&\Sigma_{YY}^{-1}\Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY}
    \end{array}\]
    where $\Sigma_{UV}$ is the cross-covariance matrix of random vectors $U,V$. \textit{R} provides an inbuilt function \texttt{cancor}.
  \end{enumerate}

  \par As Lasso uses the $L_1$ penalty function, which is non-linear, there is no closed expression of Lasso regression. Meaning that computing a solution to Lasso has $O(N^2)$ time-complexity\footnote{}. Fearnhead \& Prangle recommend the use of linear regression as it is straight-forward to implement and does not perform notably worse than the other approaches in general.

  \par For their specific implementation of linear least-squares regression they treat each model parameter $\theta_i$ completely separately and allow for mappings $f(\cdot)$ of the response data. This means they are fitting $\rho=|\theta|$ different models
  \[ \theta_i=\alpha^{(i)}+(\pmb\beta^{(i)})^Tf(\mathbf{x})+\varepsilon_i \]
  \par As ABC-methods only consider the distance between summary statistic values, the constant terms $\alpha^{(i)}$ can be neglect from our generate summary statistics. This means the summary statistic $s_i$ for the $i^{th}$ model parameter is defined as
  \[ s_i(\mathbf{x})=\hat\beta^{(i)}f(\mathbf{x}) \]

  \par The mapping $f(\cdot)$ is a parameter of this algorithm and should be used to encode likely relationships between observations and parameters, however it can just be set to the identity function for simplicity. As the mapping is part of the generated summary statistic $s_i$ it is important for it to be computationally efficient, it order for the summary statistic to be efficient.

  \begin{box_algorithm}[Semi-Automatic ABC - Least Squares]\label{alg_semi_auto_abc_ls}
    \begin{algorithm}[H]
      \footnotetext{$\rho:=|\theta|$, the number of model parameters.}
      \SetKwInOut{Require}{require}
      \Require{Observations from true model $x_{obs}$, Set of summary statistics $S$, Number of simulated parameter sets $m$, Theorised model $X$, Mapping $f(\cdot)$}
      $f_\theta\leftarrow$Posterior from pilot run of an ABC-method using $x_{obs}$ and $S$\label{alg_semi_auto_abc_ls_pilot_run}\\
      $\hat\Theta\leftarrow$ $m$ simulations from $f_\theta$\label{alg_semi_auto_abc_ls_generate_1}\\
      $X_{\hat\theta}\leftarrow$ $X\left(\hat\theta\right)$ for each $\hat\theta\in\hat\Theta$\label{alg_semi_auto_abc_ls_generate_2}\\
      $\hat{X}\leftarrow\{X_{\hat\theta_1},\dots,X_{\hat\theta_m}\}$\\
      $F\leftarrow f(\hat{X})$\\
      $\tilde{F}\leftarrow F$ with a preceding column of 1s\\
      \For{$i=1,\dots,\rho$}{
        $A_i\leftarrow i^{th}$ element of each set in $\hat\Theta$\\
        $(\alpha^{(i)},\pmb\beta^{(i)})\leftarrow(\tilde{F}^T\tilde{F}^{-1})\tilde{F}^TA_i$\\
        $s_i(\mathbf{x}):=\pmb\beta^{(i)}\mathbf{x}$
      }
      \Return{$\{s_1,\dots,s_\rho\}$}
    \end{algorithm}
  \end{box_algorithm}

  \par \textbf{Algorithm \ref{alg_semi_auto_abc_ls}} is a restatement of the general algorithm \textbf{Algorithm \ref{alg_semi_auto_abc}} using linear least-squares regression. Any ABC-method can be used for the pilot run (Line \ref{alg_semi_auto_abc_ls_pilot_run}), using the ``Best Samples'' version of ABC-Rejection Sampling is it has the simplest acceptance criteria to define and the most predictable run-time. Further, any set of summary statistics $S$ can be used to. The pilot run is an opportunity for expert knowledge to be encoded into the model by hand-crafted statistics, but, as this algorithm will mainly be run when such statistics are not known, the identity function can be used for simplicity and guaranteed sufficiency. The closer the posterior produced by the pilot run, the more representative the generated values (lines \ref{alg_semi_auto_abc_ls_generate_1}-\ref{alg_semi_auto_abc_ls_generate_2}) will be and thus the more informative the regression fit will be, creating better summary statistics. The other time expert knowledge can be encoded is in the specification of map $f(\cdot)$.

  \par The least-squares approach used in \textbf{Algorithm \ref{alg_semi_auto_abc_ls}} treats each model parameter as fully independent. This may not be true and ignoring this may lead to missed insights. Different regression approaches can be used to maintain dependencies between parameters (e.g. CCA). The generated summary statistics offer little insight or interpretability, on their own, but can be viewed intuitively as posterior mean estimators due to how they generated. This approach generates one summary statistic for each model parameter, if it could incorporate dependencies between model parameters then the total number of summary statistics could be reduced, increasing the compression level.

  \par Using the generated summary statistics in ABC-methods is not straightforward as we lack the intuition required to defined acceptance criteria. The use of adaptable versions of the ABC-methods avoids this issue as you only have to specify what acceptance rate you wish to achieve.

  % TODO performance

\subsubsection{Non-Linear Projection}\label{sec_non_linear_projection}

  The semi-automatic approach of \cite[]{constructing_summary_statistics_for_approximate_bayesian_computation_semi_automatic_ABC} does allow for non-linear projections from the response data $x$ to the parameter values $\theta$ but the user needs to specify the non-linear functions. More specifically, \textbf{Algorithm \ref{alg_semi_auto_abc_ls}} produces non-linear projections if, and only if, the mapping $f(\cdot)$ is non-linear.

  \par Being able to generate non-linear projections is desirable as it is not guaranteed that an (accurate) linear projection from response variables to model parameters exists. \cite[]{learning_summary_statistics_for_abc_via_dnn} presents the first attempt at using a deep neural-network\footnote{A feedforward neural-network is presented too, but these cannot model non-linear relationships unless they use a non-linear activation function.} to construct summary statistics. The general approach to ABC is the same as \cite[]{constructing_summary_statistics_for_approximate_bayesian_computation_semi_automatic_ABC}: Perform a pilot run to generate training data; Train a neural network to fit response values to parameter values; And, then use the trained network to calculate summary statistic values for a proper run of ABC. Due to the flexibility of DNNs the number of outputs (i.e. the dimensions of the summary statistic) can be specified to any value, although more outputs require more training time and potentially a larger network.

  \par The network used to demonstrate this approach in \cite[]{learning_summary_statistics_for_abc_via_dnn} is fairly small with three hidden layers, with 5-5-3 nodes each, and takes all the observed data as an input. The model was trained to fit to parameter values, resulting in summary statistics which approximate the posterior mean. They demonstrate their method on an Ising model and moving-average model and show it to outperform the usage of hand-crafted summary statistics and semi-automatic ABC. The trade-off is that their DNN approach requires significantly more time than the other approaches, requiring twenty minutes when Fearnhead \& Prangle's semi-automatic approach required less than one.

  \par A natural extension to this approach is to apply a mapping to the observed data before it is passed to the network, as in semi-automatic ABC. This would allow for the encoding of expert knowledge into the network which would mean a smaller network is required, reducing training time.

  \par This use of a neural network is liable to the same issues as many other uses, with the most dangerous being overfitting. Overfitting occurs when a neural network models the training data too closely and therefore does not perform well with more general data. Early stopping and regularisation are standard practices to mitigating overfitting. Additionally, improving the training set can help too. The training set can be improved by increasing its size and its diversity so that it is more representative of the general space. In this particular context, as the training set is generated from a posterior from a pilot run of ABC, we can improve the quality of the training set by improving this posterior. Allowing the pilot run to complete more simulations is guaranteed to improve the posterior, especially when using the identity function as the only summary statistic (due to the sufficiency of the identity function). Alternatively, the use of less na\"ive statistic will help too but it can be hard to identify these in practice. Using neural networks offers no interpretability of what inferences are being made, without very intricate investigation of the network.

  % TODO more approaches (random forests) see the annual review

  % TODO \par Neural networks are notorious for being slow to train. In their experiments, Wong \textit{et al.} find the running of their DNN approach to take ~20 minutes compared to Fearnhead \& Prangles' semi-automatic approach taking less than a minute. This motivates that neural networks are best used in more complex scenarios, however they will perform well with simple models as the relationships are

\subsubsection{Toy Example}\label{sec_summary_statistics_toy}

  Using identity function may not be ideal as it seeks to minimise total loss and doesn'y prioritise any features of the response. For some problems we want to prioritise features of the outcome (e.g. peak infection date in SIR model).

\subsection{Model Selection}

  \par Theorems which state when a model is misspecified that bayesian inference will put mass on the distributions ``closest to the ground truth'' rely on strong regularity conditions. \cite[]{inconsistency_of_bayesian_inference_for_misspecified_linear_models_and_a_proposal_for_reparing_it}
  \par Introduce learning rate (SafeBayes) \cite[]{inconsistency_of_bayesian_inference_for_misspecified_linear_models_and_a_proposal_for_reparing_it}

\newpage
\section{ABC and Epidemic Events}\label{sec_epidemic_events}

\newpage
\section{Conclusion}

\subsection{Future Areas of Research}

  % NOTE alternative to ABC (e.g. indirect inference, Gourieroux and Ronchetti, 1993)

% bibliography
\newpage
\setcounter{page}{1}
\pagenumbering{roman}
\bibliographystyle{royal}
\bibliography{References}

\end{document}
